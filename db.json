{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/yilia/source/main.0cf68a.css","path":"main.0cf68a.css","modified":0,"renderable":1},{"_id":"themes/yilia/source/slider.e37972.js","path":"slider.e37972.js","modified":0,"renderable":1},{"_id":"themes/yilia/source/main.0cf68a.js","path":"main.0cf68a.js","modified":0,"renderable":1},{"_id":"themes/yilia/source/mobile.992cbe.js","path":"mobile.992cbe.js","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/default-skin.b257fa.svg","path":"fonts/default-skin.b257fa.svg","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.16acc2.ttf","path":"fonts/iconfont.16acc2.ttf","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.45d7ee.svg","path":"fonts/iconfont.45d7ee.svg","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.8c627f.woff","path":"fonts/iconfont.8c627f.woff","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.b322fa.eot","path":"fonts/iconfont.b322fa.eot","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/tooltip.4004ff.svg","path":"fonts/tooltip.4004ff.svg","modified":0,"renderable":1},{"_id":"themes/yilia/source/img/avatar.jpg","path":"img/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/yilia/source/img/default-skin.png","path":"img/default-skin.png","modified":0,"renderable":1},{"_id":"themes/yilia/source/img/preloader.gif","path":"img/preloader.gif","modified":0,"renderable":1},{"_id":"themes/yilia/source/img/scrollbar_arrow.png","path":"img/scrollbar_arrow.png","modified":0,"renderable":1}],"Cache":[{"_id":"themes/yilia/.babelrc","hash":"db600d40e93e6d8023737a65d58d3be7370e5e30","modified":1536285380747},{"_id":"themes/yilia/.editorconfig","hash":"daaa8757fac18f8735fadd0a37a42c06f421ca14","modified":1536285380747},{"_id":"themes/yilia/.eslintignore","hash":"ed9d8911ca08c3dd5072c48dd0be4d06f8897730","modified":1536285380748},{"_id":"themes/yilia/.eslintrc.js","hash":"303d25adf02ad65720e537a16a4a137d14bb755f","modified":1536285380748},{"_id":"themes/yilia/.gitattributes","hash":"758cfbecfa7919e99abddf3297f37cde7e3d8d4e","modified":1536285380749},{"_id":"themes/yilia/.gitignore","hash":"d5fc575329853ff620b50fc62ad4b18fa09a308a","modified":1536285380749},{"_id":"themes/yilia/README.md","hash":"1bf755806af9d8874bd22e1abbdaaa24328ef4dc","modified":1536285380749},{"_id":"themes/yilia/_config.yml","hash":"e1107dc7462e6c2a7ce925f512a97d2f830aae21","modified":1568814941181},{"_id":"themes/yilia/package.json","hash":"ee6aa61f1cb89fd549e3e087c0232207a9c9ee30","modified":1536285380765},{"_id":"themes/yilia/webpack.config.js","hash":"da7657347109ddb4ab8602b219778117254677fe","modified":1536285380793},{"_id":"source/_posts/Kafka及Spring-Kafka整合.md","hash":"fcbc3a35721ba73d39c4ba7893a6801c28a5bfe8","modified":1568812296919},{"_id":"source/_posts/URLEncode和URLDecoder.md","hash":"29509889571ec5897aaa69fd24ea2821e8608581","modified":1568812296909},{"_id":"source/_posts/crontab使用时间参数.md","hash":"6f91a9598fb0cd173698981ff9cb4330b812dd9f","modified":1568812296911},{"_id":"source/_posts/hadoop删库跑路？.md","hash":"d7802cfa3d455977bd305f8320aa04d0a885ef1c","modified":1568812620086},{"_id":"source/_posts/hive开发UDF及使用.md","hash":"76afa78b6463563108b933920b8d696f01f4e610","modified":1568812296913},{"_id":"source/_posts/hive开发udaf和udtf.md","hash":"6098492f3287ddf68b8b24a850c624e8fa7d897c","modified":1568812296914},{"_id":"source/_posts/hive日常使用的几个小技巧（长期维护中-）.md","hash":"abbf0b5ce11744e2bbcaf64bee7aacb882b83efd","modified":1568812296915},{"_id":"source/_posts/kafka一个服务配置到不同topic的不同group.md","hash":"8fc94259adbf143d89d56f2e902fde3b29e1d5ec","modified":1568814245974},{"_id":"source/_posts/kafka动态配置topic.md","hash":"b21ec5ab91f816b009d51ab356cff6613b27b39f","modified":1568812296916},{"_id":"source/_posts/log4j与hadoop的简单结合.md","hash":"c2c5880f18f6fcd0929a3b5ffd3f0b143b34d5a0","modified":1568812296917},{"_id":"source/_posts/shell中获取hdfs文件路径参数.md","hash":"a3c6af78407718aa463138db36b8c4745b6bdee9","modified":1568812296918},{"_id":"source/_posts/spring基础学习记录.md","hash":"7d3bb3908f6850b16cd9d1adfaeb519bd85ee02c","modified":1568814044063},{"_id":"source/contact/index.md","hash":"34139dd2ddbf7c9b302ce392b4f338624ac4bdbf","modified":1536312830222},{"_id":"themes/yilia/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1536285380732},{"_id":"themes/yilia/.git/config","hash":"316bc92d9936dc739c39161838a28fb4641cbea2","modified":1536285380738},{"_id":"themes/yilia/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1536285287671},{"_id":"themes/yilia/.git/index","hash":"481525eb5342d42468ec2af935ce040a0f5c7b23","modified":1568861262076},{"_id":"themes/yilia/.git/packed-refs","hash":"76b1aa9479dd3de329935b6d94a6624c5f716d23","modified":1536285380728},{"_id":"themes/yilia/languages/default.yml","hash":"f26a34a7983d4bc17c65c7f0f14da598e62ce66d","modified":1536285380750},{"_id":"themes/yilia/languages/fr.yml","hash":"b4be1c1592a72012e48df2b3ec41cc9685573e50","modified":1536285380751},{"_id":"themes/yilia/languages/nl.yml","hash":"3d82ec703d0b3287739d7cb4750a715ae83bfcb3","modified":1536285380751},{"_id":"themes/yilia/languages/no.yml","hash":"ddf2035e920a5ecb9076138c184257d9f51896a7","modified":1536285380751},{"_id":"themes/yilia/languages/ru.yml","hash":"2a476b4c6e04900914c81378941640ac5d58a1f0","modified":1536285380752},{"_id":"themes/yilia/languages/zh-CN.yml","hash":"b057f389c6713010f97d461e48ec959b0b6f3b44","modified":1536285380752},{"_id":"themes/yilia/languages/zh-tw.yml","hash":"f5f0ca88185da7a8457760d84bf221781473bd7c","modified":1536285380752},{"_id":"themes/yilia/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1536285380763},{"_id":"themes/yilia/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1536285380763},{"_id":"themes/yilia/layout/index.ejs","hash":"ec498c6c0606acde997ce195dad97b267418d980","modified":1536285380763},{"_id":"themes/yilia/layout/layout.ejs","hash":"b471ab706d48e0be3f783eab1c94bf5878ef5a94","modified":1536285380763},{"_id":"themes/yilia/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1536285380764},{"_id":"themes/yilia/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1536285380764},{"_id":"themes/yilia/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1536285380765},{"_id":"themes/yilia/source/main.0cf68a.css","hash":"ddf6e2c6b953c2c59a3c271e6070010a4cc81cf9","modified":1536285380790},{"_id":"themes/yilia/source/slider.e37972.js","hash":"ce5eac88301fe4f2fce0fb6203adfd58eb8313ac","modified":1536285380793},{"_id":"themes/yilia/source-src/css.ejs","hash":"cf7eab48d626433120d1ef9697f719a359817018","modified":1536285380765},{"_id":"themes/yilia/source-src/script.ejs","hash":"28abac2426761d7e715b38aadd86ce6549c8ae77","modified":1536285380785},{"_id":"themes/yilia/layout/_partial/toc.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1536285380761},{"_id":"themes/yilia/source/main.0cf68a.js","hash":"283ae27ea37ac3e0e45b2e05c2482a4c594b9c25","modified":1536285380790},{"_id":"themes/yilia/source/mobile.992cbe.js","hash":"1801ef448909ea23c0a48e9d63b80d0cfd5534ce","modified":1536285380792},{"_id":"themes/yilia/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1536285287672},{"_id":"themes/yilia/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1536285287672},{"_id":"themes/yilia/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1536285287673},{"_id":"themes/yilia/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1536285287674},{"_id":"themes/yilia/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1536285287675},{"_id":"themes/yilia/.git/hooks/pre-commit.sample","hash":"33729ad4ce51acda35094e581e4088f3167a0af8","modified":1536285287676},{"_id":"themes/yilia/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1536285287676},{"_id":"themes/yilia/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1536285287677},{"_id":"themes/yilia/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1536285287678},{"_id":"themes/yilia/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1536285287679},{"_id":"themes/yilia/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1536285287679},{"_id":"themes/yilia/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1536285287681},{"_id":"themes/yilia/.git/logs/HEAD","hash":"da81ddca9befb2898d004d400bf93bb021c6eb0e","modified":1536285380735},{"_id":"themes/yilia/layout/_partial/after-footer.ejs","hash":"2d7a95aa6b78d97957726e11d403849bf4f7807e","modified":1536330470552},{"_id":"themes/yilia/layout/_partial/archive-post.ejs","hash":"edc0154b30a4127acda10297bec6aacf754b4ac4","modified":1536285380753},{"_id":"themes/yilia/layout/_partial/archive.ejs","hash":"a4eacc2bc1278095a0ef99f904b0634c78f980eb","modified":1536285380754},{"_id":"themes/yilia/layout/_partial/article.ejs","hash":"630c6ec866d056657d3d91e34b4c64eb993c0654","modified":1536317970036},{"_id":"themes/yilia/layout/_partial/aside.ejs","hash":"751e5deab5365348be5243688b419c82d337ab9a","modified":1536285380755},{"_id":"themes/yilia/layout/_partial/baidu-analytics.ejs","hash":"155327c23607f69989b58845f24d842a54e504b8","modified":1536285380755},{"_id":"themes/yilia/layout/_partial/css.ejs","hash":"236f8a377b2e4e35754319c3029bcd4a4115431d","modified":1536285380755},{"_id":"themes/yilia/layout/_partial/footer.ejs","hash":"0f646cd2c3d7129bd84e7c45c0a8e9bcc464fc54","modified":1536330641600},{"_id":"themes/yilia/layout/_partial/google-analytics.ejs","hash":"1ccc627d7697e68fddc367c73ac09920457e5b35","modified":1536285380756},{"_id":"themes/yilia/layout/_partial/head.ejs","hash":"12ca7d8dba56bc767b9309dda9526dcbaffc1614","modified":1536285380756},{"_id":"themes/yilia/layout/_partial/header.ejs","hash":"b69855e07b65117769adc515cb64b803932068c9","modified":1536285380756},{"_id":"themes/yilia/layout/_partial/left-col.ejs","hash":"fb1b8457b9eb15b55da1bf7b133e12c375dd26f8","modified":1536285380757},{"_id":"themes/yilia/layout/_partial/mathjax.ejs","hash":"11550a418921d330e6553be0569a94ab5a217967","modified":1536285380757},{"_id":"themes/yilia/layout/_partial/mobile-nav.ejs","hash":"ccec1fc70f021cb50ac85b524e7949878ab93a18","modified":1536285380757},{"_id":"themes/yilia/layout/_partial/tools.ejs","hash":"0ffcb251b79e8a920c9b4cb6bb7a96a808816165","modified":1536285380762},{"_id":"themes/yilia/layout/_partial/viewer.ejs","hash":"cc1c39903aed0a0601d104238d2bbd13ad2a36f3","modified":1536285380762},{"_id":"themes/yilia/source/fonts/default-skin.b257fa.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1536285380786},{"_id":"themes/yilia/source/fonts/iconfont.16acc2.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1536285380786},{"_id":"themes/yilia/source/fonts/iconfont.45d7ee.svg","hash":"75767c904d483d9b93469afb6b92bb6bdface639","modified":1536285380787},{"_id":"themes/yilia/source/fonts/iconfont.8c627f.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1536285380787},{"_id":"themes/yilia/source/fonts/iconfont.b322fa.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1536285380788},{"_id":"themes/yilia/source/fonts/tooltip.4004ff.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1536285380788},{"_id":"themes/yilia/source/img/avatar.jpg","hash":"b47a8a6be42fc8fe290374d0113b0c46d67b2ae7","modified":1536055949174},{"_id":"themes/yilia/source/img/default-skin.png","hash":"ed95a8e40a2c3478c5915376acb8e5f33677f24d","modified":1536285380789},{"_id":"themes/yilia/source/img/preloader.gif","hash":"6342367c93c82da1b9c620e97c84a389cc43d96d","modified":1536285380789},{"_id":"themes/yilia/source/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1536285380789},{"_id":"themes/yilia/source-src/css/_core.scss","hash":"29ba600e98ed55f7af4ade8038272c84cba21188","modified":1536285380766},{"_id":"themes/yilia/source-src/css/_function.scss","hash":"ce227b6f5a9af194fd5d455200630f32c05e151f","modified":1536285380766},{"_id":"themes/yilia/source-src/css/archive.scss","hash":"d6a7dd88404b383b5b94e4c7ec675a410c41f3cc","modified":1536285380766},{"_id":"themes/yilia/source-src/css/article-inner.scss","hash":"f7388f5c11370ef462f7cb913d8f72edf24ecaf9","modified":1536285380767},{"_id":"themes/yilia/source-src/css/article-main.scss","hash":"1577a2336b3ad122f49f60dff2bc1a97d4e7b18b","modified":1536285380767},{"_id":"themes/yilia/source-src/css/article-nav.scss","hash":"8f82fe898ba1c1bd00c24a7d8270feddc7eba3bc","modified":1536285380767},{"_id":"themes/yilia/source-src/css/article.scss","hash":"55d082fec4c6bb341725567acaa29ce37d50320a","modified":1536285380768},{"_id":"themes/yilia/source-src/css/aside.scss","hash":"07244c188f58ecfb90bb7c047b8cde977f1dc4b4","modified":1536285380768},{"_id":"themes/yilia/source-src/css/comment.scss","hash":"b85f344f2c66d43d7094746e0a9ccb21d0534201","modified":1536285380768},{"_id":"themes/yilia/source-src/css/fonts.scss","hash":"96d7eb1d42c06fdcccb8ef969f6ecd30c3194903","modified":1536285380771},{"_id":"themes/yilia/source-src/css/footer.scss","hash":"7ca837a4cc34db1c35f01baec85eb10ccc64ea86","modified":1536285380773},{"_id":"themes/yilia/source-src/css/global.scss","hash":"b4cb4f45a55d4250cd9056f76dab2a3c0dabcec4","modified":1536285380774},{"_id":"themes/yilia/source-src/css/grid.scss","hash":"f53ea8270752b5919ec5d79224d22af91f2eda12","modified":1536285380774},{"_id":"themes/yilia/source-src/css/highlight.scss","hash":"40e5aa5056dc0b3b9f51c5b387370b612e265d4e","modified":1536285380774},{"_id":"themes/yilia/source-src/css/left.scss","hash":"80dac621e43581a254d0152d5df901e4d0b01c09","modified":1536285380776},{"_id":"themes/yilia/source-src/css/main.scss","hash":"9eba1fcf4805256697528fcf3b767cf6dd8d0591","modified":1536285380776},{"_id":"themes/yilia/source-src/css/mobile-slider.scss","hash":"19f10fd2f0c3377aa4b165b3c2291ecf86dd9351","modified":1536285380776},{"_id":"themes/yilia/source-src/css/mobile.scss","hash":"d995dcd483a250fe61b426158afb61bf8923a927","modified":1536285380777},{"_id":"themes/yilia/source-src/css/page.scss","hash":"244c4d75c375978ff9edb74acc68825e63c6b235","modified":1536285380777},{"_id":"themes/yilia/source-src/css/reward.scss","hash":"a557a9ed244c82b8b71e9da9de3339d92783499f","modified":1536285380777},{"_id":"themes/yilia/source-src/css/scroll.scss","hash":"2495f7e4e3b055735c531f944b5f40a118a351ec","modified":1536285380778},{"_id":"themes/yilia/source-src/css/share.scss","hash":"9d6f6884f40c191882e56a1e1e1192400944a515","modified":1536285380778},{"_id":"themes/yilia/source-src/css/social.scss","hash":"a10a038a1dac8953cb4ffc7e04272eff9fac54e4","modified":1536285380778},{"_id":"themes/yilia/source-src/css/tags-cloud.scss","hash":"399744e98e7c67939ed9b23c2670d8baad044eda","modified":1536285380778},{"_id":"themes/yilia/source-src/css/tags.scss","hash":"915c93edd67c5326695cc7dc84b14c5f154dbcc8","modified":1536285380779},{"_id":"themes/yilia/source-src/css/tools.scss","hash":"2924fb6f77c4a9973cd928c2c7db0acb848ed483","modified":1536285380779},{"_id":"themes/yilia/source-src/css/tooltip.scss","hash":"b81cedbe31accca82e597801186911a7b5e6841c","modified":1536285380780},{"_id":"themes/yilia/source-src/js/Q.js","hash":"e56d9710afa79b31ca6b9fbd845f6d1895f5214b","modified":1536285380781},{"_id":"themes/yilia/source-src/js/anm.js","hash":"d18f6276a352b871390a4112d479b9e58b8cdbbe","modified":1536285380781},{"_id":"themes/yilia/source-src/js/aside.js","hash":"5e4c3c3d61f1e1ce2f09688d3aff25fadc851fff","modified":1536285380781},{"_id":"themes/yilia/source-src/js/browser.js","hash":"4dc04845cf27f350922b63f1813a9c82e6e33b05","modified":1536285380782},{"_id":"themes/yilia/source-src/js/fix.js","hash":"67b8819abb886c9d066fb3b0624ca15e06f63fe0","modified":1536285380782},{"_id":"themes/yilia/source-src/js/main.js","hash":"fe98bf90ce61658fe16ae057f8b6a512a845af3b","modified":1536285380782},{"_id":"themes/yilia/source-src/js/mobile.js","hash":"461c08ffcbc724d74ec7e0ff38e171eefe0f89fd","modified":1536285380783},{"_id":"themes/yilia/source-src/js/report.js","hash":"57680f9a23bd0a1eaafd64ae08cc33e20627ab15","modified":1536285380783},{"_id":"themes/yilia/source-src/js/share.js","hash":"d4ccff8266c37363b3904226f5d035b7db882c61","modified":1536285380784},{"_id":"themes/yilia/source-src/js/slider.js","hash":"0beaa112657ad57c723d9e773d5b79de60c1dd74","modified":1536285380784},{"_id":"themes/yilia/source-src/js/util.js","hash":"3bcdeb95072b85600874424e6929e3e22cfddaa0","modified":1536285380784},{"_id":"themes/yilia/source-src/js/viewer.js","hash":"c699cf3c89409ec8f044258e0715a470861b5d5d","modified":1536285380785},{"_id":"themes/yilia/layout/_partial/script.ejs","hash":"e98ec0b3b56f14d1d79af99ceb42727719a584f3","modified":1536285380761},{"_id":"themes/yilia/.git/objects/pack/pack-94fd5335e0c86113d48f9590b58865f2a4cabc40.idx","hash":"64c09d84703543b931c5b77db07c379da367a055","modified":1536285380555},{"_id":"themes/yilia/.git/refs/heads/master","hash":"4ed77da1a2617db0e77c3e3e190a1c79c16dfb9a","modified":1536285380734},{"_id":"themes/yilia/layout/_partial/post/category.ejs","hash":"e777cbf959b11c4dfda649c562799899b90ab4a3","modified":1536285380758},{"_id":"themes/yilia/layout/_partial/post/changyan.ejs","hash":"086c8a88fd3bcae7ec13258df58e25d6354af2fa","modified":1536285380758},{"_id":"themes/yilia/layout/_partial/post/date.ejs","hash":"aae96de18d48cd3b9b7bf6fed0100e15b53cca97","modified":1536285380758},{"_id":"themes/yilia/layout/_partial/post/duoshuo.ejs","hash":"f6b4c4eaafb5ac386273354b5f64a26139b7a3b0","modified":1536285380758},{"_id":"themes/yilia/layout/_partial/post/gitment.ejs","hash":"89f3f54420df08f38fe49f09487fd3732202d9e6","modified":1568870614109},{"_id":"themes/yilia/layout/_partial/post/nav.ejs","hash":"b6a97043f9ec37e571aacacfedcda1d4d75e3c7c","modified":1536285380759},{"_id":"themes/yilia/layout/_partial/post/share.ejs","hash":"f43e8251609676a7e51b58ff0f4aa629f25db02e","modified":1536325797325},{"_id":"themes/yilia/layout/_partial/post/tag.ejs","hash":"2c4e4ca36c9bb4318506c38aca7127f1f44d827f","modified":1536285380760},{"_id":"themes/yilia/layout/_partial/post/title.ejs","hash":"d4a460a35e2112d0c7414fd5e19b3a16093f1caf","modified":1536285380760},{"_id":"themes/yilia/layout/_partial/post/wangyiyun.ejs","hash":"fb022502c741b4a26bad6b2ad37245c10ede3f1a","modified":1536285380760},{"_id":"themes/yilia/source-src/css/core/_animation.scss","hash":"1834c3ed8560716e63bb3a50be94cac87fbbeaf3","modified":1536285380769},{"_id":"themes/yilia/source-src/css/core/_media-queries.scss","hash":"262ffcd88775080b7f511db37f58d2bcb1b2bfc7","modified":1536285380769},{"_id":"themes/yilia/source-src/css/core/_mixin.scss","hash":"91db061c9c17628291a005e5bd4936cf9d35a6c4","modified":1536285380770},{"_id":"themes/yilia/source-src/css/core/_reset.scss","hash":"398a49913b4a47d928103562b1ce94520be4026a","modified":1536285380770},{"_id":"themes/yilia/source-src/css/core/_variables.scss","hash":"6e75bdaa46de83094ba0873099c6e7d656a22453","modified":1536285380771},{"_id":"themes/yilia/source-src/css/fonts/iconfont.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1536285380772},{"_id":"themes/yilia/source-src/css/fonts/iconfont.svg","hash":"75767c904d483d9b93469afb6b92bb6bdface639","modified":1536285380772},{"_id":"themes/yilia/source-src/css/img/checkered-pattern.png","hash":"049262fa0886989d750637b264bed34ab51c23c8","modified":1536285380775},{"_id":"themes/yilia/source-src/css/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1536285380775},{"_id":"themes/yilia/source-src/css/img/tooltip.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1536285380775},{"_id":"themes/yilia/source-src/css/fonts/iconfont.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1536285380773},{"_id":"themes/yilia/source-src/css/fonts/iconfont.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1536285380773},{"_id":"themes/yilia/.git/logs/refs/heads/master","hash":"da81ddca9befb2898d004d400bf93bb021c6eb0e","modified":1536285380736},{"_id":"themes/yilia/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1536285380731},{"_id":"themes/yilia/.git/logs/refs/remotes/origin/HEAD","hash":"da81ddca9befb2898d004d400bf93bb021c6eb0e","modified":1536285380731},{"_id":"themes/yilia/.git/objects/pack/pack-94fd5335e0c86113d48f9590b58865f2a4cabc40.pack","hash":"625a436c2db1e61db441a04739a7ecffbf1fac36","modified":1536285380622},{"_id":"public/atom.xml","hash":"96b89d93a3ae2e28dd379b6ac2e35c62741ae36b","modified":1568870673880},{"_id":"public/content.json","hash":"f4b723ee876d167ceebf34dbcb1588dd2d8c1892","modified":1568870674082},{"_id":"public/contact/index.html","hash":"c18d01abb5c00ff91200c688bf2ebff838a6312b","modified":1568870674123},{"_id":"public/post/ad3ef435.html","hash":"d83521a308645ae541d1779b08f75f2b47d84317","modified":1568870674123},{"_id":"public/post/d4f0dd5b.html","hash":"52c59a41160ff22f43776570fd675cb45dcc100c","modified":1568870674123},{"_id":"public/post/37d63f90.html","hash":"4fcab7bea8f754b88fa2c8cb3cf568827d414fbd","modified":1568870674123},{"_id":"public/post/b033fd85.html","hash":"4f0606bee57812bc5c85efd5142d5c81778e3fed","modified":1568870674124},{"_id":"public/post/bf5e9970.html","hash":"0aa25db6316bf5321603bb5f8bdf04b75e8b7c71","modified":1568870674124},{"_id":"public/post/bc878bda.html","hash":"0f88913a7632a24bdeed8ab942c03acd5c62ab48","modified":1568870674124},{"_id":"public/post/78cb4de.html","hash":"6b3ad9c55d7181bca240344c0ba6ff0740f15c98","modified":1568870674124},{"_id":"public/post/b3d37a88.html","hash":"162f3cbe49c681e16395cdf77e7f0741dbd120d3","modified":1568870674125},{"_id":"public/post/96910ad9.html","hash":"81a7f7d5e1cdc2eceeb2145ee5ae3633368821fc","modified":1568870674125},{"_id":"public/post/686dbd19.html","hash":"adda178ab44d5ea0aba01b76c847827f10d3cd9e","modified":1568870674125},{"_id":"public/post/85675a3e.html","hash":"23f9f9ce5d7744449efb4b0de17c6a624ad635e3","modified":1568870674125},{"_id":"public/post/db474090.html","hash":"e6cbfa4b92381037e1cbac2f5d049b88f6a06e14","modified":1568870674126},{"_id":"public/archives/index.html","hash":"3fd8f8107e96cf11d52ecc8dbe2e4d7891b8b264","modified":1568870674126},{"_id":"public/archives/page/2/index.html","hash":"6cc486fd68820f8286f0dad2e04eb6f0b29c00c9","modified":1568870674126},{"_id":"public/archives/2018/index.html","hash":"b5253c347658f31d9fb4978836af871540a4bbe3","modified":1568870674127},{"_id":"public/archives/2018/09/index.html","hash":"b6bace5f7880384dbcd36cc4802bf306be7a1b0a","modified":1568870674127},{"_id":"public/archives/2018/10/index.html","hash":"3e39f6a03bf452786bc481aca85d82a9f3369808","modified":1568870674127},{"_id":"public/archives/2018/12/index.html","hash":"7d570ceda534e0c753be2ee67240c287f20f9156","modified":1568870674127},{"_id":"public/archives/2019/index.html","hash":"ccde0e78e44f0fc6148d28eddf673995dbc1be3a","modified":1568870674128},{"_id":"public/archives/2019/01/index.html","hash":"746818883ed77a5761932f5ee5ebf663c9d390c7","modified":1568870674128},{"_id":"public/archives/2019/02/index.html","hash":"ba43f3525bc9beffe8c869781e26b2499865b5fc","modified":1568870674128},{"_id":"public/archives/2019/03/index.html","hash":"7a515521f3e2a73dc5b352368f61371f1cef2cc2","modified":1568870674129},{"_id":"public/page/2/index.html","hash":"68783c104a5f18578462046be503a97f67f33fa1","modified":1568870674129},{"_id":"public/tags/java/index.html","hash":"d1a5bbf0a975a45e068babdbe9dd9cd609fbfd41","modified":1568870674129},{"_id":"public/tags/shell/index.html","hash":"1d1e1ac6d66e18a3753412354ae6c8f451c25da4","modified":1568870674129},{"_id":"public/tags/linux/index.html","hash":"4f1f2ebec5e0deb8068c0f5a67a63d69ef3e0547","modified":1568870674129},{"_id":"public/index.html","hash":"2e70000803954400225ae7120cc93304a3e7b081","modified":1568870674129},{"_id":"public/tags/hadoop/index.html","hash":"c6ddf7f17e5345bfdb411258dc8780f3aedce286","modified":1568870674130},{"_id":"public/tags/udf/index.html","hash":"0d52858956191a7ab10f00e0b28c6e789cf9150b","modified":1568870674130},{"_id":"public/tags/kafka/index.html","hash":"b2568ae5189b87ebaab07b58dcf21f0a881dae44","modified":1568870674130},{"_id":"public/tags/hive/index.html","hash":"53d603c839adcdd84ad1151e563ab0f7d525a025","modified":1568870674130},{"_id":"public/tags/日志处理/index.html","hash":"b98d810b9864fd6ab12f00d8ecf67094340957a3","modified":1568870674130},{"_id":"public/tags/log4j/index.html","hash":"2be11185b70c5b83a2b7e84a74c25fb6dfd092b7","modified":1568870674130},{"_id":"public/tags/spring/index.html","hash":"9b43341567d12e67aac124ff449caf6d2221452f","modified":1568870674131},{"_id":"public/fonts/default-skin.b257fa.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1568870674131},{"_id":"public/fonts/iconfont.16acc2.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1568870674131},{"_id":"public/fonts/iconfont.45d7ee.svg","hash":"75767c904d483d9b93469afb6b92bb6bdface639","modified":1568870674131},{"_id":"public/fonts/iconfont.8c627f.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1568870674142},{"_id":"public/fonts/iconfont.b322fa.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1568870674142},{"_id":"public/fonts/tooltip.4004ff.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1568870674142},{"_id":"public/img/default-skin.png","hash":"ed95a8e40a2c3478c5915376acb8e5f33677f24d","modified":1568870674142},{"_id":"public/img/avatar.jpg","hash":"b47a8a6be42fc8fe290374d0113b0c46d67b2ae7","modified":1568870674142},{"_id":"public/img/preloader.gif","hash":"6342367c93c82da1b9c620e97c84a389cc43d96d","modified":1568870674144},{"_id":"public/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1568870674145},{"_id":"public/slider.e37972.js","hash":"6dec4e220c89049037eebc44404abd8455d22ad7","modified":1568870674218},{"_id":"public/main.0cf68a.css","hash":"ddf6e2c6b953c2c59a3c271e6070010a4cc81cf9","modified":1568870674227},{"_id":"public/main.0cf68a.js","hash":"993fadeb5f6d296e9d997a49ee20dc97333ceab7","modified":1568870674229},{"_id":"public/mobile.992cbe.js","hash":"01b35e71e37aa2849664eb5daf26daede2278398","modified":1568870674235}],"Category":[],"Data":[],"Page":[{"title":"contact","date":"2018-09-07T09:32:59.000Z","_content":"#QQ\n    914606466\n#微信\n    WZY521478","source":"contact/index.md","raw":"---\ntitle: contact\ndate: 2018-09-07 17:32:59\n---\n#QQ\n    914606466\n#微信\n    WZY521478","updated":"2018-09-07T09:33:50.222Z","path":"contact/index.html","comments":1,"layout":"page","_id":"ck0q92d0000017guov3e2toro","content":"<p>#QQ<br>    914606466</p>\n<p>#微信<br>    WZY521478</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#QQ<br>    914606466</p>\n<p>#微信<br>    WZY521478</p>\n"}],"Post":[{"title":"URLEncode和URLDecoder","abbrlink":"686dbd19","date":"2018-10-17T08:22:49.000Z","_content":"## 一、背景\n&emsp;使用http请求的在服务之间传递消息时，会出现字符串乱码现象。  \n&emsp;使用POST方法提交时，会对其中的有些字符进行编码,数据内容的类型是 application/x-www-form-urlencoded\n<!--more-->\n```\n1.字符\"a\"-\"z\"，\"A\"-\"Z\"，\"0\"-\"9\"，\".\"，\"-\"，\"*\"，和\"_\" 都不会被编码;\n2.将空格转换为加号 (+) ;\n3.将非文本内容转换成\"%xy\"的形式,xy是两位16进制的数值;\n4.在每个 name=value 对之间放置 & 符号。\n```\n&emsp;URLDecoder 和 URLEncoder 用于完成普通字符串 和 application/x-www-form-urlencoded MIME 字符串之间的相互转换。\n## 二、使用\n```\n正常的字符串：破晓\nURL中的字符串：%e7%a0%b4%e6%99%93\n```\n### 1.URLEncode\n&emsp;URLEncode是对URL中的特殊字符部分进行编码\n```\nString url = \"破晓\";\ntry{\n    url = URLEncoder.encode(url,\"utf-8\");\n    System.out.println(\"-----\"+url);\n}catch (Exception e){\n    System.out.println(\"---\");\n}\n``` \n### 2.URLDecode\n&emsp;URLEncode是对URL中的特殊字符部分进行解码\n```\nString url = \"%e7%a0%b4%e6%99%93\";\ntry{\n    url = URLDecoder.decode(url);\n    System.out.println(\"-----\"+url);\n}catch (Exception e){\n    System.out.println(\"---\");\n}\n```\n## 三、应用\n&emsp;上次发现个问题，经过我们使用BASE64加密，通过http请求后，发现原来的“+”全部变成了“ ”。最后发现问题出在这里。  \n&emsp;上述方法即可解决http请求后出现乱码现象。","source":"_posts/URLEncode和URLDecoder.md","raw":"---\ntitle: URLEncode和URLDecoder\ntags:\n  - java\nabbrlink: 686dbd19\ndate: 2018-10-17 16:22:49\n---\n## 一、背景\n&emsp;使用http请求的在服务之间传递消息时，会出现字符串乱码现象。  \n&emsp;使用POST方法提交时，会对其中的有些字符进行编码,数据内容的类型是 application/x-www-form-urlencoded\n<!--more-->\n```\n1.字符\"a\"-\"z\"，\"A\"-\"Z\"，\"0\"-\"9\"，\".\"，\"-\"，\"*\"，和\"_\" 都不会被编码;\n2.将空格转换为加号 (+) ;\n3.将非文本内容转换成\"%xy\"的形式,xy是两位16进制的数值;\n4.在每个 name=value 对之间放置 & 符号。\n```\n&emsp;URLDecoder 和 URLEncoder 用于完成普通字符串 和 application/x-www-form-urlencoded MIME 字符串之间的相互转换。\n## 二、使用\n```\n正常的字符串：破晓\nURL中的字符串：%e7%a0%b4%e6%99%93\n```\n### 1.URLEncode\n&emsp;URLEncode是对URL中的特殊字符部分进行编码\n```\nString url = \"破晓\";\ntry{\n    url = URLEncoder.encode(url,\"utf-8\");\n    System.out.println(\"-----\"+url);\n}catch (Exception e){\n    System.out.println(\"---\");\n}\n``` \n### 2.URLDecode\n&emsp;URLEncode是对URL中的特殊字符部分进行解码\n```\nString url = \"%e7%a0%b4%e6%99%93\";\ntry{\n    url = URLDecoder.decode(url);\n    System.out.println(\"-----\"+url);\n}catch (Exception e){\n    System.out.println(\"---\");\n}\n```\n## 三、应用\n&emsp;上次发现个问题，经过我们使用BASE64加密，通过http请求后，发现原来的“+”全部变成了“ ”。最后发现问题出在这里。  \n&emsp;上述方法即可解决http请求后出现乱码现象。","slug":"URLEncode和URLDecoder","published":1,"updated":"2019-09-18T13:11:36.909Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0q92czr00007guoam084or3","content":"<h2 id=\"一、背景\"><a href=\"#一、背景\" class=\"headerlink\" title=\"一、背景\"></a>一、背景</h2><p>&emsp;使用http请求的在服务之间传递消息时，会出现字符串乱码现象。<br>&emsp;使用POST方法提交时，会对其中的有些字符进行编码,数据内容的类型是 application/x-www-form-urlencoded<br><a id=\"more\"></a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1.字符&quot;a&quot;-&quot;z&quot;，&quot;A&quot;-&quot;Z&quot;，&quot;0&quot;-&quot;9&quot;，&quot;.&quot;，&quot;-&quot;，&quot;*&quot;，和&quot;_&quot; 都不会被编码;</span><br><span class=\"line\">2.将空格转换为加号 (+) ;</span><br><span class=\"line\">3.将非文本内容转换成&quot;%xy&quot;的形式,xy是两位16进制的数值;</span><br><span class=\"line\">4.在每个 name=value 对之间放置 &amp; 符号。</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;URLDecoder 和 URLEncoder 用于完成普通字符串 和 application/x-www-form-urlencoded MIME 字符串之间的相互转换。</p>\n<h2 id=\"二、使用\"><a href=\"#二、使用\" class=\"headerlink\" title=\"二、使用\"></a>二、使用</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">正常的字符串：破晓</span><br><span class=\"line\">URL中的字符串：%e7%a0%b4%e6%99%93</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-URLEncode\"><a href=\"#1-URLEncode\" class=\"headerlink\" title=\"1.URLEncode\"></a>1.URLEncode</h3><p>&emsp;URLEncode是对URL中的特殊字符部分进行编码<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">String url = &quot;破晓&quot;;</span><br><span class=\"line\">try&#123;</span><br><span class=\"line\">    url = URLEncoder.encode(url,&quot;utf-8&quot;);</span><br><span class=\"line\">    System.out.println(&quot;-----&quot;+url);</span><br><span class=\"line\">&#125;catch (Exception e)&#123;</span><br><span class=\"line\">    System.out.println(&quot;---&quot;);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">``` </span><br><span class=\"line\">### 2.URLDecode</span><br><span class=\"line\">&amp;emsp;URLEncode是对URL中的特殊字符部分进行解码</span><br></pre></td></tr></table></figure></p>\n<p>String url = “%e7%a0%b4%e6%99%93”;<br>try{<br>    url = URLDecoder.decode(url);<br>    System.out.println(“—–”+url);<br>}catch (Exception e){<br>    System.out.println(“—“);<br>}<br><code>`</code></p>\n<h2 id=\"三、应用\"><a href=\"#三、应用\" class=\"headerlink\" title=\"三、应用\"></a>三、应用</h2><p>&emsp;上次发现个问题，经过我们使用BASE64加密，通过http请求后，发现原来的“+”全部变成了“ ”。最后发现问题出在这里。<br>&emsp;上述方法即可解决http请求后出现乱码现象。</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"一、背景\"><a href=\"#一、背景\" class=\"headerlink\" title=\"一、背景\"></a>一、背景</h2><p>&emsp;使用http请求的在服务之间传递消息时，会出现字符串乱码现象。<br>&emsp;使用POST方法提交时，会对其中的有些字符进行编码,数据内容的类型是 application/x-www-form-urlencoded<br>","more":"<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1.字符&quot;a&quot;-&quot;z&quot;，&quot;A&quot;-&quot;Z&quot;，&quot;0&quot;-&quot;9&quot;，&quot;.&quot;，&quot;-&quot;，&quot;*&quot;，和&quot;_&quot; 都不会被编码;</span><br><span class=\"line\">2.将空格转换为加号 (+) ;</span><br><span class=\"line\">3.将非文本内容转换成&quot;%xy&quot;的形式,xy是两位16进制的数值;</span><br><span class=\"line\">4.在每个 name=value 对之间放置 &amp; 符号。</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;URLDecoder 和 URLEncoder 用于完成普通字符串 和 application/x-www-form-urlencoded MIME 字符串之间的相互转换。</p>\n<h2 id=\"二、使用\"><a href=\"#二、使用\" class=\"headerlink\" title=\"二、使用\"></a>二、使用</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">正常的字符串：破晓</span><br><span class=\"line\">URL中的字符串：%e7%a0%b4%e6%99%93</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-URLEncode\"><a href=\"#1-URLEncode\" class=\"headerlink\" title=\"1.URLEncode\"></a>1.URLEncode</h3><p>&emsp;URLEncode是对URL中的特殊字符部分进行编码<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">String url = &quot;破晓&quot;;</span><br><span class=\"line\">try&#123;</span><br><span class=\"line\">    url = URLEncoder.encode(url,&quot;utf-8&quot;);</span><br><span class=\"line\">    System.out.println(&quot;-----&quot;+url);</span><br><span class=\"line\">&#125;catch (Exception e)&#123;</span><br><span class=\"line\">    System.out.println(&quot;---&quot;);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">``` </span><br><span class=\"line\">### 2.URLDecode</span><br><span class=\"line\">&amp;emsp;URLEncode是对URL中的特殊字符部分进行解码</span><br></pre></td></tr></table></figure></p>\n<p>String url = “%e7%a0%b4%e6%99%93”;<br>try{<br>    url = URLDecoder.decode(url);<br>    System.out.println(“—–”+url);<br>}catch (Exception e){<br>    System.out.println(“—“);<br>}<br><code>`</code></p>\n<h2 id=\"三、应用\"><a href=\"#三、应用\" class=\"headerlink\" title=\"三、应用\"></a>三、应用</h2><p>&emsp;上次发现个问题，经过我们使用BASE64加密，通过http请求后，发现原来的“+”全部变成了“ ”。最后发现问题出在这里。<br>&emsp;上述方法即可解决http请求后出现乱码现象。</p>"},{"title":"crontab使用时间参数","abbrlink":"78cb4de","date":"2018-12-25T08:02:00.000Z","_content":"&emsp;最近写了一个脚本，需要定时执行，决定使用Crontab。\n<!--more-->\n## 一、问题描述\n&emsp;由于脚本需要传入时间参数，传入时间时发生了不能执行的问题，如下。  \n刚开始写的调用脚本为\n```\n20 0 * * * source ~/.bash_profile;cd */shell;sh count.sh $(date +%Y%m%d)> count.log 2>&1\n```\n定时执行不能执行，查看系统日志后，发现错误位置在$(date。\n## 二、问题解决\n&emsp;看了网上发现问题所在。原因应该是，任务调度中，%是个特殊字符，表示特殊含义，有换行的意思。所以不能直接使用%，而需要添加反斜杠来进行转义。\n修改后的调用脚本为\n```\n20 0 * * * source ~/.bash_profile;cd */shell;sh count.sh $(date +\"\\%Y\\%m\\%d\")> count.log 2>&1^C\n```\n然后问题解决了。\n## 三、拓展\n&emsp;既然用到了crontab，就简单的学习一下吧。\n### 1.时间参数说明\n```\n20 0 * * *\n从左到右依次为：\n[分钟] [小时] [每月的某一天] [每年的某一月] [每周的某一天] [执行的命令]\n该参数的意义为：每天的0点20分执行脚本\n星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作\n逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”\n中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”\n正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10\n@reboot 系统重启时执行\n```\n### 2.添加/编辑 Crontab\n```\ncrontab [-u username] -e\n默认情况下，系统会编辑当前用户的crontab命令集合\n```\n### 3.查看Crontab\n```\ncrontab [-u username] -l\n```\n### 4.删除Crontab\n```\ncrontab [-u username] -r\n慎用。可以直接crontab -e 进行编辑\n```\n### 5.载入\n```\ncrontab [-u user] file\n将file做为crontab的任务列表文件并载入crontab\n如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。\n```\n### 6.Crontab服务\n```\nservice crond start    //启动服务\nservice crond stop     //关闭服务\nservice crond restart  //重启服务\nservice crond reload   //重新载入配置\nservice crond status   //查看服务状态\n```\n### 7.目录\n```\n/etc/cron.d/\n这个目录用来存放任何要执行的crontab文件或脚本。\n```\n### 8.注意事项\n```\n1)脚本中涉及文件路径时写全局路径\n2)脚本执行要用到java或其他环境变量时，通过source命令引入环境变量\n3)新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。\n```\n\n","source":"_posts/crontab使用时间参数.md","raw":"---\ntitle: crontab使用时间参数\ntags:\n  - linux\n  - shell\nabbrlink: 78cb4de\ndate: 2018-12-25 16:02:00\n---\n&emsp;最近写了一个脚本，需要定时执行，决定使用Crontab。\n<!--more-->\n## 一、问题描述\n&emsp;由于脚本需要传入时间参数，传入时间时发生了不能执行的问题，如下。  \n刚开始写的调用脚本为\n```\n20 0 * * * source ~/.bash_profile;cd */shell;sh count.sh $(date +%Y%m%d)> count.log 2>&1\n```\n定时执行不能执行，查看系统日志后，发现错误位置在$(date。\n## 二、问题解决\n&emsp;看了网上发现问题所在。原因应该是，任务调度中，%是个特殊字符，表示特殊含义，有换行的意思。所以不能直接使用%，而需要添加反斜杠来进行转义。\n修改后的调用脚本为\n```\n20 0 * * * source ~/.bash_profile;cd */shell;sh count.sh $(date +\"\\%Y\\%m\\%d\")> count.log 2>&1^C\n```\n然后问题解决了。\n## 三、拓展\n&emsp;既然用到了crontab，就简单的学习一下吧。\n### 1.时间参数说明\n```\n20 0 * * *\n从左到右依次为：\n[分钟] [小时] [每月的某一天] [每年的某一月] [每周的某一天] [执行的命令]\n该参数的意义为：每天的0点20分执行脚本\n星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作\n逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”\n中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”\n正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10\n@reboot 系统重启时执行\n```\n### 2.添加/编辑 Crontab\n```\ncrontab [-u username] -e\n默认情况下，系统会编辑当前用户的crontab命令集合\n```\n### 3.查看Crontab\n```\ncrontab [-u username] -l\n```\n### 4.删除Crontab\n```\ncrontab [-u username] -r\n慎用。可以直接crontab -e 进行编辑\n```\n### 5.载入\n```\ncrontab [-u user] file\n将file做为crontab的任务列表文件并载入crontab\n如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。\n```\n### 6.Crontab服务\n```\nservice crond start    //启动服务\nservice crond stop     //关闭服务\nservice crond restart  //重启服务\nservice crond reload   //重新载入配置\nservice crond status   //查看服务状态\n```\n### 7.目录\n```\n/etc/cron.d/\n这个目录用来存放任何要执行的crontab文件或脚本。\n```\n### 8.注意事项\n```\n1)脚本中涉及文件路径时写全局路径\n2)脚本执行要用到java或其他环境变量时，通过source命令引入环境变量\n3)新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。\n```\n\n","slug":"crontab使用时间参数","published":1,"updated":"2019-09-18T13:11:36.911Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0q92d0100027guoh24rlf7n","content":"<p>&emsp;最近写了一个脚本，需要定时执行，决定使用Crontab。<br><a id=\"more\"></a></p>\n<h2 id=\"一、问题描述\"><a href=\"#一、问题描述\" class=\"headerlink\" title=\"一、问题描述\"></a>一、问题描述</h2><p>&emsp;由于脚本需要传入时间参数，传入时间时发生了不能执行的问题，如下。<br>刚开始写的调用脚本为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">20 0 * * * source ~/.bash_profile;cd */shell;sh count.sh $(date +%Y%m%d)&gt; count.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure></p>\n<p>定时执行不能执行，查看系统日志后，发现错误位置在$(date。</p>\n<h2 id=\"二、问题解决\"><a href=\"#二、问题解决\" class=\"headerlink\" title=\"二、问题解决\"></a>二、问题解决</h2><p>&emsp;看了网上发现问题所在。原因应该是，任务调度中，%是个特殊字符，表示特殊含义，有换行的意思。所以不能直接使用%，而需要添加反斜杠来进行转义。<br>修改后的调用脚本为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">20 0 * * * source ~/.bash_profile;cd */shell;sh count.sh $(date +&quot;\\%Y\\%m\\%d&quot;)&gt; count.log 2&gt;&amp;1^C</span><br></pre></td></tr></table></figure></p>\n<p>然后问题解决了。</p>\n<h2 id=\"三、拓展\"><a href=\"#三、拓展\" class=\"headerlink\" title=\"三、拓展\"></a>三、拓展</h2><p>&emsp;既然用到了crontab，就简单的学习一下吧。</p>\n<h3 id=\"1-时间参数说明\"><a href=\"#1-时间参数说明\" class=\"headerlink\" title=\"1.时间参数说明\"></a>1.时间参数说明</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">20 0 * * *</span><br><span class=\"line\">从左到右依次为：</span><br><span class=\"line\">[分钟] [小时] [每月的某一天] [每年的某一月] [每周的某一天] [执行的命令]</span><br><span class=\"line\">该参数的意义为：每天的0点20分执行脚本</span><br><span class=\"line\">星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作</span><br><span class=\"line\">逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”</span><br><span class=\"line\">中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”</span><br><span class=\"line\">正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10</span><br><span class=\"line\">@reboot 系统重启时执行</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-添加-编辑-Crontab\"><a href=\"#2-添加-编辑-Crontab\" class=\"headerlink\" title=\"2.添加/编辑 Crontab\"></a>2.添加/编辑 Crontab</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">crontab [-u username] -e</span><br><span class=\"line\">默认情况下，系统会编辑当前用户的crontab命令集合</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-查看Crontab\"><a href=\"#3-查看Crontab\" class=\"headerlink\" title=\"3.查看Crontab\"></a>3.查看Crontab</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">crontab [-u username] -l</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-删除Crontab\"><a href=\"#4-删除Crontab\" class=\"headerlink\" title=\"4.删除Crontab\"></a>4.删除Crontab</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">crontab [-u username] -r</span><br><span class=\"line\">慎用。可以直接crontab -e 进行编辑</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-载入\"><a href=\"#5-载入\" class=\"headerlink\" title=\"5.载入\"></a>5.载入</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">crontab [-u user] file</span><br><span class=\"line\">将file做为crontab的任务列表文件并载入crontab</span><br><span class=\"line\">如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。</span><br></pre></td></tr></table></figure>\n<h3 id=\"6-Crontab服务\"><a href=\"#6-Crontab服务\" class=\"headerlink\" title=\"6.Crontab服务\"></a>6.Crontab服务</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">service crond start    //启动服务</span><br><span class=\"line\">service crond stop     //关闭服务</span><br><span class=\"line\">service crond restart  //重启服务</span><br><span class=\"line\">service crond reload   //重新载入配置</span><br><span class=\"line\">service crond status   //查看服务状态</span><br></pre></td></tr></table></figure>\n<h3 id=\"7-目录\"><a href=\"#7-目录\" class=\"headerlink\" title=\"7.目录\"></a>7.目录</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/etc/cron.d/</span><br><span class=\"line\">这个目录用来存放任何要执行的crontab文件或脚本。</span><br></pre></td></tr></table></figure>\n<h3 id=\"8-注意事项\"><a href=\"#8-注意事项\" class=\"headerlink\" title=\"8.注意事项\"></a>8.注意事项</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1)脚本中涉及文件路径时写全局路径</span><br><span class=\"line\">2)脚本执行要用到java或其他环境变量时，通过source命令引入环境变量</span><br><span class=\"line\">3)新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<p>&emsp;最近写了一个脚本，需要定时执行，决定使用Crontab。<br>","more":"</p>\n<h2 id=\"一、问题描述\"><a href=\"#一、问题描述\" class=\"headerlink\" title=\"一、问题描述\"></a>一、问题描述</h2><p>&emsp;由于脚本需要传入时间参数，传入时间时发生了不能执行的问题，如下。<br>刚开始写的调用脚本为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">20 0 * * * source ~/.bash_profile;cd */shell;sh count.sh $(date +%Y%m%d)&gt; count.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure></p>\n<p>定时执行不能执行，查看系统日志后，发现错误位置在$(date。</p>\n<h2 id=\"二、问题解决\"><a href=\"#二、问题解决\" class=\"headerlink\" title=\"二、问题解决\"></a>二、问题解决</h2><p>&emsp;看了网上发现问题所在。原因应该是，任务调度中，%是个特殊字符，表示特殊含义，有换行的意思。所以不能直接使用%，而需要添加反斜杠来进行转义。<br>修改后的调用脚本为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">20 0 * * * source ~/.bash_profile;cd */shell;sh count.sh $(date +&quot;\\%Y\\%m\\%d&quot;)&gt; count.log 2&gt;&amp;1^C</span><br></pre></td></tr></table></figure></p>\n<p>然后问题解决了。</p>\n<h2 id=\"三、拓展\"><a href=\"#三、拓展\" class=\"headerlink\" title=\"三、拓展\"></a>三、拓展</h2><p>&emsp;既然用到了crontab，就简单的学习一下吧。</p>\n<h3 id=\"1-时间参数说明\"><a href=\"#1-时间参数说明\" class=\"headerlink\" title=\"1.时间参数说明\"></a>1.时间参数说明</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">20 0 * * *</span><br><span class=\"line\">从左到右依次为：</span><br><span class=\"line\">[分钟] [小时] [每月的某一天] [每年的某一月] [每周的某一天] [执行的命令]</span><br><span class=\"line\">该参数的意义为：每天的0点20分执行脚本</span><br><span class=\"line\">星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作</span><br><span class=\"line\">逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”</span><br><span class=\"line\">中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”</span><br><span class=\"line\">正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10</span><br><span class=\"line\">@reboot 系统重启时执行</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-添加-编辑-Crontab\"><a href=\"#2-添加-编辑-Crontab\" class=\"headerlink\" title=\"2.添加/编辑 Crontab\"></a>2.添加/编辑 Crontab</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">crontab [-u username] -e</span><br><span class=\"line\">默认情况下，系统会编辑当前用户的crontab命令集合</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-查看Crontab\"><a href=\"#3-查看Crontab\" class=\"headerlink\" title=\"3.查看Crontab\"></a>3.查看Crontab</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">crontab [-u username] -l</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-删除Crontab\"><a href=\"#4-删除Crontab\" class=\"headerlink\" title=\"4.删除Crontab\"></a>4.删除Crontab</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">crontab [-u username] -r</span><br><span class=\"line\">慎用。可以直接crontab -e 进行编辑</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-载入\"><a href=\"#5-载入\" class=\"headerlink\" title=\"5.载入\"></a>5.载入</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">crontab [-u user] file</span><br><span class=\"line\">将file做为crontab的任务列表文件并载入crontab</span><br><span class=\"line\">如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。</span><br></pre></td></tr></table></figure>\n<h3 id=\"6-Crontab服务\"><a href=\"#6-Crontab服务\" class=\"headerlink\" title=\"6.Crontab服务\"></a>6.Crontab服务</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">service crond start    //启动服务</span><br><span class=\"line\">service crond stop     //关闭服务</span><br><span class=\"line\">service crond restart  //重启服务</span><br><span class=\"line\">service crond reload   //重新载入配置</span><br><span class=\"line\">service crond status   //查看服务状态</span><br></pre></td></tr></table></figure>\n<h3 id=\"7-目录\"><a href=\"#7-目录\" class=\"headerlink\" title=\"7.目录\"></a>7.目录</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/etc/cron.d/</span><br><span class=\"line\">这个目录用来存放任何要执行的crontab文件或脚本。</span><br></pre></td></tr></table></figure>\n<h3 id=\"8-注意事项\"><a href=\"#8-注意事项\" class=\"headerlink\" title=\"8.注意事项\"></a>8.注意事项</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1)脚本中涉及文件路径时写全局路径</span><br><span class=\"line\">2)脚本执行要用到java或其他环境变量时，通过source命令引入环境变量</span><br><span class=\"line\">3)新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。</span><br></pre></td></tr></table></figure>"},{"title":"hadoop删库跑路？","abbrlink":"bc878bda","date":"2019-01-03T13:00:01.000Z","_content":"&emsp;emmmm...昨天hadoop删库了。当时很慌，想半天好像记起来hadoop有个类似于回收站的东西，找了一下果然有，记录下，下次删库不要急着跑路。  \n<!--more-->\n## 一、“回收站”\nhadoop有个类似于回收站的机制，通常我们删除hdfs文件时\n```\nhadoop fs -rm -r olicity/tableName\n```\n执行命令后，并非将文件直接删除，而是将文件移动到设置的\".Trash\"目录下。  \n## 二、配置“回收站”\n默认情况下，.Trash为关闭状态，如果需要恢复误删文件，需要进行配置core-site.xml\n```\n<property>\n    <name>fs.trash.interval</name>\n    <value>100</value>\n</property>\n```\n说明：fs.trash.interval代表删除的文件保留的时间，时间单位为分钟，默认为0代表不保存删除的文件。我们只需要设置该时间即可打开.Trash。\n## 三、使用“回收站”\n配置完了，就试试吧。先删库。（一定要确保配置完了，要不然还是随便删点不重要的东西吧）  \n删除之后，会提示\n```\n19/01/04 16:24:10 INFO fs.TrashPolicyDefault: Moved: 'hdfs://路径' to trash at: hdfs://路径/.Trash/Current/路径\n```\n恢复文件时，只需要\n```\nhadoop fs -mv \n```\n嗯，移动出来就可以了。\n## 四、彻底删除\n如果说这东西，你真的确定不想要了&&还觉得他占空间，可以彻底删除。\n```\nhadoop fs -rm -r 路径/.Trash/路径\n```\n不过轻易不要这么干，万一出事就真的得跑路了。","source":"_posts/hadoop删库跑路？.md","raw":"---\ntitle: hadoop删库跑路？\nabbrlink: bc878bda\ndate: 2019-01-03 21:00:01\ntags:\n - hadoop\n---\n&emsp;emmmm...昨天hadoop删库了。当时很慌，想半天好像记起来hadoop有个类似于回收站的东西，找了一下果然有，记录下，下次删库不要急着跑路。  \n<!--more-->\n## 一、“回收站”\nhadoop有个类似于回收站的机制，通常我们删除hdfs文件时\n```\nhadoop fs -rm -r olicity/tableName\n```\n执行命令后，并非将文件直接删除，而是将文件移动到设置的\".Trash\"目录下。  \n## 二、配置“回收站”\n默认情况下，.Trash为关闭状态，如果需要恢复误删文件，需要进行配置core-site.xml\n```\n<property>\n    <name>fs.trash.interval</name>\n    <value>100</value>\n</property>\n```\n说明：fs.trash.interval代表删除的文件保留的时间，时间单位为分钟，默认为0代表不保存删除的文件。我们只需要设置该时间即可打开.Trash。\n## 三、使用“回收站”\n配置完了，就试试吧。先删库。（一定要确保配置完了，要不然还是随便删点不重要的东西吧）  \n删除之后，会提示\n```\n19/01/04 16:24:10 INFO fs.TrashPolicyDefault: Moved: 'hdfs://路径' to trash at: hdfs://路径/.Trash/Current/路径\n```\n恢复文件时，只需要\n```\nhadoop fs -mv \n```\n嗯，移动出来就可以了。\n## 四、彻底删除\n如果说这东西，你真的确定不想要了&&还觉得他占空间，可以彻底删除。\n```\nhadoop fs -rm -r 路径/.Trash/路径\n```\n不过轻易不要这么干，万一出事就真的得跑路了。","slug":"hadoop删库跑路？","published":1,"updated":"2019-09-18T13:17:00.086Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0q92d0800047guon9cq8bqh","content":"<p>&emsp;emmmm…昨天hadoop删库了。当时很慌，想半天好像记起来hadoop有个类似于回收站的东西，找了一下果然有，记录下，下次删库不要急着跑路。<br><a id=\"more\"></a></p>\n<h2 id=\"一、“回收站”\"><a href=\"#一、“回收站”\" class=\"headerlink\" title=\"一、“回收站”\"></a>一、“回收站”</h2><p>hadoop有个类似于回收站的机制，通常我们删除hdfs文件时<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop fs -rm -r olicity/tableName</span><br></pre></td></tr></table></figure></p>\n<p>执行命令后，并非将文件直接删除，而是将文件移动到设置的”.Trash”目录下。  </p>\n<h2 id=\"二、配置“回收站”\"><a href=\"#二、配置“回收站”\" class=\"headerlink\" title=\"二、配置“回收站”\"></a>二、配置“回收站”</h2><p>默认情况下，.Trash为关闭状态，如果需要恢复误删文件，需要进行配置core-site.xml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;100&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<p>说明：fs.trash.interval代表删除的文件保留的时间，时间单位为分钟，默认为0代表不保存删除的文件。我们只需要设置该时间即可打开.Trash。</p>\n<h2 id=\"三、使用“回收站”\"><a href=\"#三、使用“回收站”\" class=\"headerlink\" title=\"三、使用“回收站”\"></a>三、使用“回收站”</h2><p>配置完了，就试试吧。先删库。（一定要确保配置完了，要不然还是随便删点不重要的东西吧）<br>删除之后，会提示<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">19/01/04 16:24:10 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://路径&apos; to trash at: hdfs://路径/.Trash/Current/路径</span><br></pre></td></tr></table></figure></p>\n<p>恢复文件时，只需要<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop fs -mv</span><br></pre></td></tr></table></figure></p>\n<p>嗯，移动出来就可以了。</p>\n<h2 id=\"四、彻底删除\"><a href=\"#四、彻底删除\" class=\"headerlink\" title=\"四、彻底删除\"></a>四、彻底删除</h2><p>如果说这东西，你真的确定不想要了&amp;&amp;还觉得他占空间，可以彻底删除。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop fs -rm -r 路径/.Trash/路径</span><br></pre></td></tr></table></figure></p>\n<p>不过轻易不要这么干，万一出事就真的得跑路了。</p>\n","site":{"data":{}},"excerpt":"<p>&emsp;emmmm…昨天hadoop删库了。当时很慌，想半天好像记起来hadoop有个类似于回收站的东西，找了一下果然有，记录下，下次删库不要急着跑路。<br>","more":"</p>\n<h2 id=\"一、“回收站”\"><a href=\"#一、“回收站”\" class=\"headerlink\" title=\"一、“回收站”\"></a>一、“回收站”</h2><p>hadoop有个类似于回收站的机制，通常我们删除hdfs文件时<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop fs -rm -r olicity/tableName</span><br></pre></td></tr></table></figure></p>\n<p>执行命令后，并非将文件直接删除，而是将文件移动到设置的”.Trash”目录下。  </p>\n<h2 id=\"二、配置“回收站”\"><a href=\"#二、配置“回收站”\" class=\"headerlink\" title=\"二、配置“回收站”\"></a>二、配置“回收站”</h2><p>默认情况下，.Trash为关闭状态，如果需要恢复误删文件，需要进行配置core-site.xml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;100&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<p>说明：fs.trash.interval代表删除的文件保留的时间，时间单位为分钟，默认为0代表不保存删除的文件。我们只需要设置该时间即可打开.Trash。</p>\n<h2 id=\"三、使用“回收站”\"><a href=\"#三、使用“回收站”\" class=\"headerlink\" title=\"三、使用“回收站”\"></a>三、使用“回收站”</h2><p>配置完了，就试试吧。先删库。（一定要确保配置完了，要不然还是随便删点不重要的东西吧）<br>删除之后，会提示<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">19/01/04 16:24:10 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://路径&apos; to trash at: hdfs://路径/.Trash/Current/路径</span><br></pre></td></tr></table></figure></p>\n<p>恢复文件时，只需要<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop fs -mv</span><br></pre></td></tr></table></figure></p>\n<p>嗯，移动出来就可以了。</p>\n<h2 id=\"四、彻底删除\"><a href=\"#四、彻底删除\" class=\"headerlink\" title=\"四、彻底删除\"></a>四、彻底删除</h2><p>如果说这东西，你真的确定不想要了&amp;&amp;还觉得他占空间，可以彻底删除。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop fs -rm -r 路径/.Trash/路径</span><br></pre></td></tr></table></figure></p>\n<p>不过轻易不要这么干，万一出事就真的得跑路了。</p>"},{"title":"hive开发UDF及使用","abbrlink":"96910ad9","date":"2018-10-26T03:03:04.000Z","_content":"&emsp;最近有个数据挖掘的需求，要求统计所给经纬度附近n公里某些事物的数量。涉及到地球两点间的距离计算，需要写UDF进行计算。\n<!--more-->\n## 一、UDF编写\n&emsp;根据经纬度计算两点间的距离，网上有很多计算方法，试了几个，发现这篇[博客](https://blog.csdn.net/u011001084/article/details/52980834)的方法计算的精度差比较小，他的分析方法也很详细，最终采用此方法。\n```\nimport com.ai.hive.udf.topdomain.StringUtil;\nimport org.apache.hadoop.hive.ql.exec.UDF;\nimport org.apache.hadoop.io.Text;\nimport org.apache.log4j.Logger;\n\n/**\n * 功能：根据两地经纬度计算两点之间的距离\n * create temporary function LocDistanceCalUDF as 'com.ai.hive.udf.util.LocDistanceCalUDF';\n * @author olicity\n */\npublic class LocDistanceCalUDF extends UDF{\n    private static Logger log = Logger.getLogger(LocDistanceCalUDF.class);\n\n    private Text nullText = new Text(\"\");\n    /**\n    *根据经纬度计算地球两点间的距离\n    */\n    private static double distanceCal(double lng1, double lat1,double lng2,double lat2){\n        double dx = lng1 - lng2;// 经度差值\n        double dy= lat1 - lat2;// 纬度差值\n        double b = (lat1 + lat2) / 2.0;// 平均纬度\n        double Lx = Math.toRadians(dx)*6367000.0*Math.cos(Math.toRadians(b));// 东西距离\n        double Ly = 6367000.0*Math.toRadians(dy);// 南北距离\n        return Math.sqrt(Lx*Lx+Ly*Ly);// 用平面的矩形对角距离公式计算总距离(米)\n    }\n    /**\n    *重写evaluate方法\n    */\n    public Text evaluate(Text longitudeText1, Text latitudeText1,Text longitudeText2, Text latitudeText2){\n        try{\n            if(longitudeText1==null || latitudeText1==null || longitudeText2==null || latitudeText2==null){\n                return nullText;\n            }\n            if(StringUtil.isEmpty(longitudeText1.toString()) || StringUtil.isEmpty(latitudeText1.toString()) || StringUtil.isEmpty(longitudeText2.toString()) || StringUtil.isEmpty(latitudeText2.toString())){\n                return nullText;\n            }\n            double lng1 = Double.valueOf(longitudeText1.toString());\n            double lat1 = Double.valueOf(latitudeText1.toString());\n            double lng2 = Double.valueOf(longitudeText2.toString());\n            double lat2 = Double.valueOf(latitudeText2.toString());\n\n            double dis = distanceCal(lng1,lat1,lng2,lat2);\n            return new Text(String.valueOf(dis));\n        }catch (Exception e){\n            return nullText;\n        }\n\n    }\n    /**\n    *重写evaluate方法\n    */\n    public Text evaluate(Text locationA,Text locationB){\n        try{\n            if (locationA==null||locationB==null){\n                return nullText;\n            }\n            if(StringUtil.isEmpty(locationA.toString()) || StringUtil.isEmpty(locationB.toString())){\n                return nullText;\n            }\n            String locationA2String  = locationA.toString();\n            String locationB2String  = locationB.toString();\n            double lng1 = Double.valueOf(locationA2String.split(\",\")[0]);\n            double lat1 = Double.valueOf(locationA2String.split(\",\")[1]);\n            double lng2 = Double.valueOf(locationB2String.split(\",\")[0]);\n            double lat2 = Double.valueOf(locationB2String.split(\",\")[1]);\n\n            double dis = distanceCal(lng1,lat1,lng2,lat2);\n            return new Text(String.valueOf(dis));\n        }catch(Exception e){\n            return nullText;\n        }\n    }\n\n}\n```\n&emsp;UDF类要继承org.apache.hadoop.hive.ql.exec.UDF类，类中要实现evaluate。 当我们在hive中使用自定义的UDF的时候，hive会调用类中的evaluate方法来实现特定的功能。  \n\n## 二、UDF导入\n### 1.jar包上传\n&emsp;右键类名，Copy reference，复制此类全路径得到：com.ai.hive.udf.util.LocDistanceCalUDF。将写完的类打成jar包上传到服务器。路径如：/user/olicity/hive/UDF\n### 2.jar包引入classpath变量中\n进入hive，引入jar包，执行命令\n```\nadd jar /user/olicity/hive/UDF/udf.jar;\n```\n查看导入的jar包\n```\nlist jars;\n```\n### 3.创建函数\n创建一个名为LocDistanceCalUDF的临时函数，关联该jar包\n```\ncreate temporary function LocDistanceCalUDF as 'com.ai.hive.udf.util.LocDistanceCalUDF';\n```\n查看创建的函数\n```\nshow functions;\n```\n### 4.注意\n&emsp;上述方法仅限于当前会话生效，如需要添加一个永久的函数对应的永久的路径，则\n```\ncreate function locUDF.LocDistanceCalUDF \n  as 'com.ai.hive.udf.util.LocDistanceCalUDF' \n  using jar 'hdfs://hdfs路径/udf.jar';\nuse LocDistanceCalUDF;\n```\n需要将jar包放到hdfs上，然后创建函数关联路径即可。  \n另外还看到过另一种方法，配置hive-site.xml文件中的hive.aux.jars.path\n```\n配置参考如下：\n   <property>\n       <name>hive.aux.jars.path</name>\n       <value>file:///home/hdfs/fangjs/DefTextInputFormat.jar,file:///jarpath/test.jar</value>\n   </property>\n```\n\n## 三、UDF使用\n&emsp;准备工作已经就绪，可以开始查表了。emmmmm，就简单的俩表查吧，表结构和表数据就不展示了，示例表也就不建了，所给经纬度表叫A表，需要查询的表叫B表，临时中间表叫c表，经纬度的表中字段定义是loc，距离就算2公里吧。\n```\ncreate table C\nas\nselect B.* from B join A where (LocDistanceCalUDF(A.loc,B.loc)<=2000);\n```\nOK.\n## 四、总结\n&emsp;关于sql语句还是需要再多加练习，尤其是多表联查。Hadoop之路任重而道远。\n","source":"_posts/hive开发UDF及使用.md","raw":"---\ntitle: hive开发UDF及使用\ntags:\n  - hive\n  - udf\nabbrlink: 96910ad9\ndate: 2018-10-26 11:03:04\n---\n&emsp;最近有个数据挖掘的需求，要求统计所给经纬度附近n公里某些事物的数量。涉及到地球两点间的距离计算，需要写UDF进行计算。\n<!--more-->\n## 一、UDF编写\n&emsp;根据经纬度计算两点间的距离，网上有很多计算方法，试了几个，发现这篇[博客](https://blog.csdn.net/u011001084/article/details/52980834)的方法计算的精度差比较小，他的分析方法也很详细，最终采用此方法。\n```\nimport com.ai.hive.udf.topdomain.StringUtil;\nimport org.apache.hadoop.hive.ql.exec.UDF;\nimport org.apache.hadoop.io.Text;\nimport org.apache.log4j.Logger;\n\n/**\n * 功能：根据两地经纬度计算两点之间的距离\n * create temporary function LocDistanceCalUDF as 'com.ai.hive.udf.util.LocDistanceCalUDF';\n * @author olicity\n */\npublic class LocDistanceCalUDF extends UDF{\n    private static Logger log = Logger.getLogger(LocDistanceCalUDF.class);\n\n    private Text nullText = new Text(\"\");\n    /**\n    *根据经纬度计算地球两点间的距离\n    */\n    private static double distanceCal(double lng1, double lat1,double lng2,double lat2){\n        double dx = lng1 - lng2;// 经度差值\n        double dy= lat1 - lat2;// 纬度差值\n        double b = (lat1 + lat2) / 2.0;// 平均纬度\n        double Lx = Math.toRadians(dx)*6367000.0*Math.cos(Math.toRadians(b));// 东西距离\n        double Ly = 6367000.0*Math.toRadians(dy);// 南北距离\n        return Math.sqrt(Lx*Lx+Ly*Ly);// 用平面的矩形对角距离公式计算总距离(米)\n    }\n    /**\n    *重写evaluate方法\n    */\n    public Text evaluate(Text longitudeText1, Text latitudeText1,Text longitudeText2, Text latitudeText2){\n        try{\n            if(longitudeText1==null || latitudeText1==null || longitudeText2==null || latitudeText2==null){\n                return nullText;\n            }\n            if(StringUtil.isEmpty(longitudeText1.toString()) || StringUtil.isEmpty(latitudeText1.toString()) || StringUtil.isEmpty(longitudeText2.toString()) || StringUtil.isEmpty(latitudeText2.toString())){\n                return nullText;\n            }\n            double lng1 = Double.valueOf(longitudeText1.toString());\n            double lat1 = Double.valueOf(latitudeText1.toString());\n            double lng2 = Double.valueOf(longitudeText2.toString());\n            double lat2 = Double.valueOf(latitudeText2.toString());\n\n            double dis = distanceCal(lng1,lat1,lng2,lat2);\n            return new Text(String.valueOf(dis));\n        }catch (Exception e){\n            return nullText;\n        }\n\n    }\n    /**\n    *重写evaluate方法\n    */\n    public Text evaluate(Text locationA,Text locationB){\n        try{\n            if (locationA==null||locationB==null){\n                return nullText;\n            }\n            if(StringUtil.isEmpty(locationA.toString()) || StringUtil.isEmpty(locationB.toString())){\n                return nullText;\n            }\n            String locationA2String  = locationA.toString();\n            String locationB2String  = locationB.toString();\n            double lng1 = Double.valueOf(locationA2String.split(\",\")[0]);\n            double lat1 = Double.valueOf(locationA2String.split(\",\")[1]);\n            double lng2 = Double.valueOf(locationB2String.split(\",\")[0]);\n            double lat2 = Double.valueOf(locationB2String.split(\",\")[1]);\n\n            double dis = distanceCal(lng1,lat1,lng2,lat2);\n            return new Text(String.valueOf(dis));\n        }catch(Exception e){\n            return nullText;\n        }\n    }\n\n}\n```\n&emsp;UDF类要继承org.apache.hadoop.hive.ql.exec.UDF类，类中要实现evaluate。 当我们在hive中使用自定义的UDF的时候，hive会调用类中的evaluate方法来实现特定的功能。  \n\n## 二、UDF导入\n### 1.jar包上传\n&emsp;右键类名，Copy reference，复制此类全路径得到：com.ai.hive.udf.util.LocDistanceCalUDF。将写完的类打成jar包上传到服务器。路径如：/user/olicity/hive/UDF\n### 2.jar包引入classpath变量中\n进入hive，引入jar包，执行命令\n```\nadd jar /user/olicity/hive/UDF/udf.jar;\n```\n查看导入的jar包\n```\nlist jars;\n```\n### 3.创建函数\n创建一个名为LocDistanceCalUDF的临时函数，关联该jar包\n```\ncreate temporary function LocDistanceCalUDF as 'com.ai.hive.udf.util.LocDistanceCalUDF';\n```\n查看创建的函数\n```\nshow functions;\n```\n### 4.注意\n&emsp;上述方法仅限于当前会话生效，如需要添加一个永久的函数对应的永久的路径，则\n```\ncreate function locUDF.LocDistanceCalUDF \n  as 'com.ai.hive.udf.util.LocDistanceCalUDF' \n  using jar 'hdfs://hdfs路径/udf.jar';\nuse LocDistanceCalUDF;\n```\n需要将jar包放到hdfs上，然后创建函数关联路径即可。  \n另外还看到过另一种方法，配置hive-site.xml文件中的hive.aux.jars.path\n```\n配置参考如下：\n   <property>\n       <name>hive.aux.jars.path</name>\n       <value>file:///home/hdfs/fangjs/DefTextInputFormat.jar,file:///jarpath/test.jar</value>\n   </property>\n```\n\n## 三、UDF使用\n&emsp;准备工作已经就绪，可以开始查表了。emmmmm，就简单的俩表查吧，表结构和表数据就不展示了，示例表也就不建了，所给经纬度表叫A表，需要查询的表叫B表，临时中间表叫c表，经纬度的表中字段定义是loc，距离就算2公里吧。\n```\ncreate table C\nas\nselect B.* from B join A where (LocDistanceCalUDF(A.loc,B.loc)<=2000);\n```\nOK.\n## 四、总结\n&emsp;关于sql语句还是需要再多加练习，尤其是多表联查。Hadoop之路任重而道远。\n","slug":"hive开发UDF及使用","published":1,"updated":"2019-09-18T13:11:36.913Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0q92d0a00057guo3t6blyku","content":"<p>&emsp;最近有个数据挖掘的需求，要求统计所给经纬度附近n公里某些事物的数量。涉及到地球两点间的距离计算，需要写UDF进行计算。<br><a id=\"more\"></a></p>\n<h2 id=\"一、UDF编写\"><a href=\"#一、UDF编写\" class=\"headerlink\" title=\"一、UDF编写\"></a>一、UDF编写</h2><p>&emsp;根据经纬度计算两点间的距离，网上有很多计算方法，试了几个，发现这篇<a href=\"https://blog.csdn.net/u011001084/article/details/52980834\" target=\"_blank\" rel=\"noopener\">博客</a>的方法计算的精度差比较小，他的分析方法也很详细，最终采用此方法。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import com.ai.hive.udf.topdomain.StringUtil;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class=\"line\">import org.apache.hadoop.io.Text;</span><br><span class=\"line\">import org.apache.log4j.Logger;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * 功能：根据两地经纬度计算两点之间的距离</span><br><span class=\"line\"> * create temporary function LocDistanceCalUDF as &apos;com.ai.hive.udf.util.LocDistanceCalUDF&apos;;</span><br><span class=\"line\"> * @author olicity</span><br><span class=\"line\"> */</span><br><span class=\"line\">public class LocDistanceCalUDF extends UDF&#123;</span><br><span class=\"line\">    private static Logger log = Logger.getLogger(LocDistanceCalUDF.class);</span><br><span class=\"line\"></span><br><span class=\"line\">    private Text nullText = new Text(&quot;&quot;);</span><br><span class=\"line\">    /**</span><br><span class=\"line\">    *根据经纬度计算地球两点间的距离</span><br><span class=\"line\">    */</span><br><span class=\"line\">    private static double distanceCal(double lng1, double lat1,double lng2,double lat2)&#123;</span><br><span class=\"line\">        double dx = lng1 - lng2;// 经度差值</span><br><span class=\"line\">        double dy= lat1 - lat2;// 纬度差值</span><br><span class=\"line\">        double b = (lat1 + lat2) / 2.0;// 平均纬度</span><br><span class=\"line\">        double Lx = Math.toRadians(dx)*6367000.0*Math.cos(Math.toRadians(b));// 东西距离</span><br><span class=\"line\">        double Ly = 6367000.0*Math.toRadians(dy);// 南北距离</span><br><span class=\"line\">        return Math.sqrt(Lx*Lx+Ly*Ly);// 用平面的矩形对角距离公式计算总距离(米)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    /**</span><br><span class=\"line\">    *重写evaluate方法</span><br><span class=\"line\">    */</span><br><span class=\"line\">    public Text evaluate(Text longitudeText1, Text latitudeText1,Text longitudeText2, Text latitudeText2)&#123;</span><br><span class=\"line\">        try&#123;</span><br><span class=\"line\">            if(longitudeText1==null || latitudeText1==null || longitudeText2==null || latitudeText2==null)&#123;</span><br><span class=\"line\">                return nullText;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            if(StringUtil.isEmpty(longitudeText1.toString()) || StringUtil.isEmpty(latitudeText1.toString()) || StringUtil.isEmpty(longitudeText2.toString()) || StringUtil.isEmpty(latitudeText2.toString()))&#123;</span><br><span class=\"line\">                return nullText;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            double lng1 = Double.valueOf(longitudeText1.toString());</span><br><span class=\"line\">            double lat1 = Double.valueOf(latitudeText1.toString());</span><br><span class=\"line\">            double lng2 = Double.valueOf(longitudeText2.toString());</span><br><span class=\"line\">            double lat2 = Double.valueOf(latitudeText2.toString());</span><br><span class=\"line\"></span><br><span class=\"line\">            double dis = distanceCal(lng1,lat1,lng2,lat2);</span><br><span class=\"line\">            return new Text(String.valueOf(dis));</span><br><span class=\"line\">        &#125;catch (Exception e)&#123;</span><br><span class=\"line\">            return nullText;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    /**</span><br><span class=\"line\">    *重写evaluate方法</span><br><span class=\"line\">    */</span><br><span class=\"line\">    public Text evaluate(Text locationA,Text locationB)&#123;</span><br><span class=\"line\">        try&#123;</span><br><span class=\"line\">            if (locationA==null||locationB==null)&#123;</span><br><span class=\"line\">                return nullText;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            if(StringUtil.isEmpty(locationA.toString()) || StringUtil.isEmpty(locationB.toString()))&#123;</span><br><span class=\"line\">                return nullText;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            String locationA2String  = locationA.toString();</span><br><span class=\"line\">            String locationB2String  = locationB.toString();</span><br><span class=\"line\">            double lng1 = Double.valueOf(locationA2String.split(&quot;,&quot;)[0]);</span><br><span class=\"line\">            double lat1 = Double.valueOf(locationA2String.split(&quot;,&quot;)[1]);</span><br><span class=\"line\">            double lng2 = Double.valueOf(locationB2String.split(&quot;,&quot;)[0]);</span><br><span class=\"line\">            double lat2 = Double.valueOf(locationB2String.split(&quot;,&quot;)[1]);</span><br><span class=\"line\"></span><br><span class=\"line\">            double dis = distanceCal(lng1,lat1,lng2,lat2);</span><br><span class=\"line\">            return new Text(String.valueOf(dis));</span><br><span class=\"line\">        &#125;catch(Exception e)&#123;</span><br><span class=\"line\">            return nullText;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;UDF类要继承org.apache.hadoop.hive.ql.exec.UDF类，类中要实现evaluate。 当我们在hive中使用自定义的UDF的时候，hive会调用类中的evaluate方法来实现特定的功能。  </p>\n<h2 id=\"二、UDF导入\"><a href=\"#二、UDF导入\" class=\"headerlink\" title=\"二、UDF导入\"></a>二、UDF导入</h2><h3 id=\"1-jar包上传\"><a href=\"#1-jar包上传\" class=\"headerlink\" title=\"1.jar包上传\"></a>1.jar包上传</h3><p>&emsp;右键类名，Copy reference，复制此类全路径得到：com.ai.hive.udf.util.LocDistanceCalUDF。将写完的类打成jar包上传到服务器。路径如：/user/olicity/hive/UDF</p>\n<h3 id=\"2-jar包引入classpath变量中\"><a href=\"#2-jar包引入classpath变量中\" class=\"headerlink\" title=\"2.jar包引入classpath变量中\"></a>2.jar包引入classpath变量中</h3><p>进入hive，引入jar包，执行命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add jar /user/olicity/hive/UDF/udf.jar;</span><br></pre></td></tr></table></figure></p>\n<p>查看导入的jar包<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list jars;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-创建函数\"><a href=\"#3-创建函数\" class=\"headerlink\" title=\"3.创建函数\"></a>3.创建函数</h3><p>创建一个名为LocDistanceCalUDF的临时函数，关联该jar包<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create temporary function LocDistanceCalUDF as &apos;com.ai.hive.udf.util.LocDistanceCalUDF&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>查看创建的函数<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show functions;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-注意\"><a href=\"#4-注意\" class=\"headerlink\" title=\"4.注意\"></a>4.注意</h3><p>&emsp;上述方法仅限于当前会话生效，如需要添加一个永久的函数对应的永久的路径，则<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create function locUDF.LocDistanceCalUDF </span><br><span class=\"line\">  as &apos;com.ai.hive.udf.util.LocDistanceCalUDF&apos; </span><br><span class=\"line\">  using jar &apos;hdfs://hdfs路径/udf.jar&apos;;</span><br><span class=\"line\">use LocDistanceCalUDF;</span><br></pre></td></tr></table></figure></p>\n<p>需要将jar包放到hdfs上，然后创建函数关联路径即可。<br>另外还看到过另一种方法，配置hive-site.xml文件中的hive.aux.jars.path<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">配置参考如下：</span><br><span class=\"line\">   &lt;property&gt;</span><br><span class=\"line\">       &lt;name&gt;hive.aux.jars.path&lt;/name&gt;</span><br><span class=\"line\">       &lt;value&gt;file:///home/hdfs/fangjs/DefTextInputFormat.jar,file:///jarpath/test.jar&lt;/value&gt;</span><br><span class=\"line\">   &lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"三、UDF使用\"><a href=\"#三、UDF使用\" class=\"headerlink\" title=\"三、UDF使用\"></a>三、UDF使用</h2><p>&emsp;准备工作已经就绪，可以开始查表了。emmmmm，就简单的俩表查吧，表结构和表数据就不展示了，示例表也就不建了，所给经纬度表叫A表，需要查询的表叫B表，临时中间表叫c表，经纬度的表中字段定义是loc，距离就算2公里吧。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create table C</span><br><span class=\"line\">as</span><br><span class=\"line\">select B.* from B join A where (LocDistanceCalUDF(A.loc,B.loc)&lt;=2000);</span><br></pre></td></tr></table></figure></p>\n<p>OK.</p>\n<h2 id=\"四、总结\"><a href=\"#四、总结\" class=\"headerlink\" title=\"四、总结\"></a>四、总结</h2><p>&emsp;关于sql语句还是需要再多加练习，尤其是多表联查。Hadoop之路任重而道远。</p>\n","site":{"data":{}},"excerpt":"<p>&emsp;最近有个数据挖掘的需求，要求统计所给经纬度附近n公里某些事物的数量。涉及到地球两点间的距离计算，需要写UDF进行计算。<br>","more":"</p>\n<h2 id=\"一、UDF编写\"><a href=\"#一、UDF编写\" class=\"headerlink\" title=\"一、UDF编写\"></a>一、UDF编写</h2><p>&emsp;根据经纬度计算两点间的距离，网上有很多计算方法，试了几个，发现这篇<a href=\"https://blog.csdn.net/u011001084/article/details/52980834\" target=\"_blank\" rel=\"noopener\">博客</a>的方法计算的精度差比较小，他的分析方法也很详细，最终采用此方法。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import com.ai.hive.udf.topdomain.StringUtil;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class=\"line\">import org.apache.hadoop.io.Text;</span><br><span class=\"line\">import org.apache.log4j.Logger;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * 功能：根据两地经纬度计算两点之间的距离</span><br><span class=\"line\"> * create temporary function LocDistanceCalUDF as &apos;com.ai.hive.udf.util.LocDistanceCalUDF&apos;;</span><br><span class=\"line\"> * @author olicity</span><br><span class=\"line\"> */</span><br><span class=\"line\">public class LocDistanceCalUDF extends UDF&#123;</span><br><span class=\"line\">    private static Logger log = Logger.getLogger(LocDistanceCalUDF.class);</span><br><span class=\"line\"></span><br><span class=\"line\">    private Text nullText = new Text(&quot;&quot;);</span><br><span class=\"line\">    /**</span><br><span class=\"line\">    *根据经纬度计算地球两点间的距离</span><br><span class=\"line\">    */</span><br><span class=\"line\">    private static double distanceCal(double lng1, double lat1,double lng2,double lat2)&#123;</span><br><span class=\"line\">        double dx = lng1 - lng2;// 经度差值</span><br><span class=\"line\">        double dy= lat1 - lat2;// 纬度差值</span><br><span class=\"line\">        double b = (lat1 + lat2) / 2.0;// 平均纬度</span><br><span class=\"line\">        double Lx = Math.toRadians(dx)*6367000.0*Math.cos(Math.toRadians(b));// 东西距离</span><br><span class=\"line\">        double Ly = 6367000.0*Math.toRadians(dy);// 南北距离</span><br><span class=\"line\">        return Math.sqrt(Lx*Lx+Ly*Ly);// 用平面的矩形对角距离公式计算总距离(米)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    /**</span><br><span class=\"line\">    *重写evaluate方法</span><br><span class=\"line\">    */</span><br><span class=\"line\">    public Text evaluate(Text longitudeText1, Text latitudeText1,Text longitudeText2, Text latitudeText2)&#123;</span><br><span class=\"line\">        try&#123;</span><br><span class=\"line\">            if(longitudeText1==null || latitudeText1==null || longitudeText2==null || latitudeText2==null)&#123;</span><br><span class=\"line\">                return nullText;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            if(StringUtil.isEmpty(longitudeText1.toString()) || StringUtil.isEmpty(latitudeText1.toString()) || StringUtil.isEmpty(longitudeText2.toString()) || StringUtil.isEmpty(latitudeText2.toString()))&#123;</span><br><span class=\"line\">                return nullText;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            double lng1 = Double.valueOf(longitudeText1.toString());</span><br><span class=\"line\">            double lat1 = Double.valueOf(latitudeText1.toString());</span><br><span class=\"line\">            double lng2 = Double.valueOf(longitudeText2.toString());</span><br><span class=\"line\">            double lat2 = Double.valueOf(latitudeText2.toString());</span><br><span class=\"line\"></span><br><span class=\"line\">            double dis = distanceCal(lng1,lat1,lng2,lat2);</span><br><span class=\"line\">            return new Text(String.valueOf(dis));</span><br><span class=\"line\">        &#125;catch (Exception e)&#123;</span><br><span class=\"line\">            return nullText;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    /**</span><br><span class=\"line\">    *重写evaluate方法</span><br><span class=\"line\">    */</span><br><span class=\"line\">    public Text evaluate(Text locationA,Text locationB)&#123;</span><br><span class=\"line\">        try&#123;</span><br><span class=\"line\">            if (locationA==null||locationB==null)&#123;</span><br><span class=\"line\">                return nullText;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            if(StringUtil.isEmpty(locationA.toString()) || StringUtil.isEmpty(locationB.toString()))&#123;</span><br><span class=\"line\">                return nullText;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            String locationA2String  = locationA.toString();</span><br><span class=\"line\">            String locationB2String  = locationB.toString();</span><br><span class=\"line\">            double lng1 = Double.valueOf(locationA2String.split(&quot;,&quot;)[0]);</span><br><span class=\"line\">            double lat1 = Double.valueOf(locationA2String.split(&quot;,&quot;)[1]);</span><br><span class=\"line\">            double lng2 = Double.valueOf(locationB2String.split(&quot;,&quot;)[0]);</span><br><span class=\"line\">            double lat2 = Double.valueOf(locationB2String.split(&quot;,&quot;)[1]);</span><br><span class=\"line\"></span><br><span class=\"line\">            double dis = distanceCal(lng1,lat1,lng2,lat2);</span><br><span class=\"line\">            return new Text(String.valueOf(dis));</span><br><span class=\"line\">        &#125;catch(Exception e)&#123;</span><br><span class=\"line\">            return nullText;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;UDF类要继承org.apache.hadoop.hive.ql.exec.UDF类，类中要实现evaluate。 当我们在hive中使用自定义的UDF的时候，hive会调用类中的evaluate方法来实现特定的功能。  </p>\n<h2 id=\"二、UDF导入\"><a href=\"#二、UDF导入\" class=\"headerlink\" title=\"二、UDF导入\"></a>二、UDF导入</h2><h3 id=\"1-jar包上传\"><a href=\"#1-jar包上传\" class=\"headerlink\" title=\"1.jar包上传\"></a>1.jar包上传</h3><p>&emsp;右键类名，Copy reference，复制此类全路径得到：com.ai.hive.udf.util.LocDistanceCalUDF。将写完的类打成jar包上传到服务器。路径如：/user/olicity/hive/UDF</p>\n<h3 id=\"2-jar包引入classpath变量中\"><a href=\"#2-jar包引入classpath变量中\" class=\"headerlink\" title=\"2.jar包引入classpath变量中\"></a>2.jar包引入classpath变量中</h3><p>进入hive，引入jar包，执行命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add jar /user/olicity/hive/UDF/udf.jar;</span><br></pre></td></tr></table></figure></p>\n<p>查看导入的jar包<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list jars;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-创建函数\"><a href=\"#3-创建函数\" class=\"headerlink\" title=\"3.创建函数\"></a>3.创建函数</h3><p>创建一个名为LocDistanceCalUDF的临时函数，关联该jar包<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create temporary function LocDistanceCalUDF as &apos;com.ai.hive.udf.util.LocDistanceCalUDF&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>查看创建的函数<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show functions;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-注意\"><a href=\"#4-注意\" class=\"headerlink\" title=\"4.注意\"></a>4.注意</h3><p>&emsp;上述方法仅限于当前会话生效，如需要添加一个永久的函数对应的永久的路径，则<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create function locUDF.LocDistanceCalUDF </span><br><span class=\"line\">  as &apos;com.ai.hive.udf.util.LocDistanceCalUDF&apos; </span><br><span class=\"line\">  using jar &apos;hdfs://hdfs路径/udf.jar&apos;;</span><br><span class=\"line\">use LocDistanceCalUDF;</span><br></pre></td></tr></table></figure></p>\n<p>需要将jar包放到hdfs上，然后创建函数关联路径即可。<br>另外还看到过另一种方法，配置hive-site.xml文件中的hive.aux.jars.path<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">配置参考如下：</span><br><span class=\"line\">   &lt;property&gt;</span><br><span class=\"line\">       &lt;name&gt;hive.aux.jars.path&lt;/name&gt;</span><br><span class=\"line\">       &lt;value&gt;file:///home/hdfs/fangjs/DefTextInputFormat.jar,file:///jarpath/test.jar&lt;/value&gt;</span><br><span class=\"line\">   &lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"三、UDF使用\"><a href=\"#三、UDF使用\" class=\"headerlink\" title=\"三、UDF使用\"></a>三、UDF使用</h2><p>&emsp;准备工作已经就绪，可以开始查表了。emmmmm，就简单的俩表查吧，表结构和表数据就不展示了，示例表也就不建了，所给经纬度表叫A表，需要查询的表叫B表，临时中间表叫c表，经纬度的表中字段定义是loc，距离就算2公里吧。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create table C</span><br><span class=\"line\">as</span><br><span class=\"line\">select B.* from B join A where (LocDistanceCalUDF(A.loc,B.loc)&lt;=2000);</span><br></pre></td></tr></table></figure></p>\n<p>OK.</p>\n<h2 id=\"四、总结\"><a href=\"#四、总结\" class=\"headerlink\" title=\"四、总结\"></a>四、总结</h2><p>&emsp;关于sql语句还是需要再多加练习，尤其是多表联查。Hadoop之路任重而道远。</p>"},{"title":"hive开发udaf和udtf","abbrlink":"37d63f90","date":"2019-03-27T02:58:54.000Z","_content":"&emsp;之前开发过udf，但是udf只能处理一对一的情况，也就是一个输入对应一个输出。而日常开发中却会遇到多种情况，普通的udf不能满足，这时候就需要引入udtf和udaf了。\n<!--more-->\n## 一、简介\n### 1.1 UDAF\n&emsp;UDAF(User- Defined Aggregation Funcation)用户定义聚合函数，可对多行数据产生作用；等同与SQL中常用的SUM()，AVG()，也是聚合函数；  \n简单说就是多行输入一行输出。\n### 1.2 UDTF\n&emsp;UDTF(User-Defined Table-Generating Functions)用户定义表生成函数，用来解决输入一行输出多行；  \n简单说就是一行输入多行输出。\n## 二、UDAF编写例子\n### 2.1 说明\n```\n1.引入如下两下类\n    import org.apache.hadoop.hive.ql.exec.UDAF  \n    import org.apache.hadoop.hive.ql.exec.UDAFEvaluator  \n2.函数类需要继承UDAF类，计算类Evaluator实现UDAFEvaluator接口\n3.Evaluator需要实现UDAFEvaluator的init、iterate、terminatePartial、merge、terminate这几个函数。\n    a）init函数实现接口UDAFEvaluator的init函数。\n    b）iterate接收传入的参数，并进行内部的迭代。其返回类型为boolean。\n    c）terminatePartial无参数，其为iterate函数遍历结束后，返回遍历得到的数据，terminatePartial类似于 hadoop的Combiner。\n    d）merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean。\n    e）terminate返回最终的聚集函数结果。\n```\n### 2.2 实例\n```\npackage hive.udaf;  \n  \nimport org.apache.hadoop.hive.ql.exec.UDAF;  \nimport org.apache.hadoop.hive.ql.exec.UDAFEvaluator;  \n//create temporary function udaf_avg 'hive.udaf.Avg'\npublic class Avg extends UDAF {  \n    public static class AvgState {  \n        private long mCount;  \n        private double mSum;  \n  \n    }  \n  \n    public static class AvgEvaluator implements UDAFEvaluator {  \n        AvgState state;  \n  \n        public AvgEvaluator() {  \n            super();  \n            state = new AvgState();  \n            init();  \n        }  \n  \n        /** \n         * init函数类似于构造函数，用于UDAF的初始化 \n         */  \n        public void init() {  \n            state.mSum = 0;  \n            state.mCount = 0;  \n        }  \n  \n        /** \n         * iterate接收传入的参数，并进行内部的轮转。其返回类型为boolean * * @param o * @return \n         */  \n  \n        public boolean iterate(Double o) {  \n            if (o != null) {  \n                state.mSum += o;  \n                state.mCount++;  \n            }  \n            return true;  \n        }  \n  \n        /** \n         * terminatePartial无参数，其为iterate函数遍历结束后，返回轮转数据， * terminatePartial类似于hadoop的Combiner * * @return \n         */  \n  \n        public AvgState terminatePartial() {  \n            // combiner  \n            return state.mCount == 0 ? null : state;  \n        }  \n  \n        /** \n         * merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean * * @param o * @return \n         */  \n  \n        public boolean merge(AvgState avgState) {  \n            if (avgState != null) {  \n                state.mCount += avgState.mCount;  \n                state.mSum += avgState.mSum;  \n            }  \n            return true;  \n        }  \n  \n        /** \n         * terminate返回最终的聚集函数结果 * * @return \n         */  \n        public Double terminate() {  \n            return state.mCount == 0 ? null : Double.valueOf(state.mSum / state.mCount);  \n        }  \n    }  \n}\n```\n## 三、UDTF编写例子\n### 3.1 说明\n```\n1.引入import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;\n2.继承GenericUDTF类\n3.重写initialize（返回输出行信息：列个数，类型）, process, close三方法\n```\n### 3.2 实例\n```\n//create temporary function DaylistUDTF as 'com.ai.hive.udf.uc.DaylistUDTF';\npublic class DaylistUDTF extends GenericUDTF{\n\n    @Override\n    public void close() throws HiveException {\n        // TODO Auto-generated method stub\n\n    }\n    @Override\n    public void process(Object[] args) throws HiveException {\n        int argsLength = args.length;\n        int resultLength = argsLength+1;\n        String daylist = args[4].toString();\n        String dayId = args[11].toString();\n        String[] result = new String[resultLength];\n        for(int j = 0; j < argsLength; j++){\n            result[j] = args[j].toString();\n        }\n        try{\n            for (int i = daylist.length() - 1; i >= 0; i--) {\n                char c = daylist.charAt(i);\n                if (c == 49) {\n                    int diff = 0-(daylist.length() - i);\n                    SimpleDateFormat sdf=new SimpleDateFormat(\"yyyyMMdd\");\n                    Date dt=sdf.parse(dayId);\n                    Calendar rightNow = Calendar.getInstance();\n                    rightNow.setTime(dt);\n                    rightNow.add(Calendar.DAY_OF_YEAR,diff);\n                    Date dt1=rightNow.getTime();\n                    String reStr = sdf.format(dt1);\n                    result[resultLength-1] = reStr;\n                    forward(result);\n                }\n            }\n        }catch (Exception e){\n            System.out.println(\"error:\"+e.toString());\n        }\n    }\n    @Override\n    public StructObjectInspector initialize(ObjectInspector[] args)\n            throws UDFArgumentException {\n        ArrayList<String> fieldNames = new ArrayList<String>();\n        ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();\n        for (int i = 1; i <= 14; i++)\n        {\n            fieldNames.add(\"col\" + i);\n            fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);\n        }\n        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,fieldOIs);\n    }\n}\n```\n## 四、感谢大佬\n&emsp;[Hive 自定义函数 UDF UDAF UDTF](https://www.cnblogs.com/mzzcy/p/7119423.html)  \n&emsp;[hive udaf开发入门和运行过程详解](http://www.cnblogs.com/ggjucheng/archive/2013/02/01/2888051.html)  \n&emsp;[hive中udtf编写及使用](https://blog.csdn.net/u012485099/article/details/80790908)","source":"_posts/hive开发udaf和udtf.md","raw":"---\ntitle: hive开发udaf和udtf\ntags:\n  - hive\n  - udf\nabbrlink: 37d63f90\ndate: 2019-03-27 10:58:54\n---\n&emsp;之前开发过udf，但是udf只能处理一对一的情况，也就是一个输入对应一个输出。而日常开发中却会遇到多种情况，普通的udf不能满足，这时候就需要引入udtf和udaf了。\n<!--more-->\n## 一、简介\n### 1.1 UDAF\n&emsp;UDAF(User- Defined Aggregation Funcation)用户定义聚合函数，可对多行数据产生作用；等同与SQL中常用的SUM()，AVG()，也是聚合函数；  \n简单说就是多行输入一行输出。\n### 1.2 UDTF\n&emsp;UDTF(User-Defined Table-Generating Functions)用户定义表生成函数，用来解决输入一行输出多行；  \n简单说就是一行输入多行输出。\n## 二、UDAF编写例子\n### 2.1 说明\n```\n1.引入如下两下类\n    import org.apache.hadoop.hive.ql.exec.UDAF  \n    import org.apache.hadoop.hive.ql.exec.UDAFEvaluator  \n2.函数类需要继承UDAF类，计算类Evaluator实现UDAFEvaluator接口\n3.Evaluator需要实现UDAFEvaluator的init、iterate、terminatePartial、merge、terminate这几个函数。\n    a）init函数实现接口UDAFEvaluator的init函数。\n    b）iterate接收传入的参数，并进行内部的迭代。其返回类型为boolean。\n    c）terminatePartial无参数，其为iterate函数遍历结束后，返回遍历得到的数据，terminatePartial类似于 hadoop的Combiner。\n    d）merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean。\n    e）terminate返回最终的聚集函数结果。\n```\n### 2.2 实例\n```\npackage hive.udaf;  \n  \nimport org.apache.hadoop.hive.ql.exec.UDAF;  \nimport org.apache.hadoop.hive.ql.exec.UDAFEvaluator;  \n//create temporary function udaf_avg 'hive.udaf.Avg'\npublic class Avg extends UDAF {  \n    public static class AvgState {  \n        private long mCount;  \n        private double mSum;  \n  \n    }  \n  \n    public static class AvgEvaluator implements UDAFEvaluator {  \n        AvgState state;  \n  \n        public AvgEvaluator() {  \n            super();  \n            state = new AvgState();  \n            init();  \n        }  \n  \n        /** \n         * init函数类似于构造函数，用于UDAF的初始化 \n         */  \n        public void init() {  \n            state.mSum = 0;  \n            state.mCount = 0;  \n        }  \n  \n        /** \n         * iterate接收传入的参数，并进行内部的轮转。其返回类型为boolean * * @param o * @return \n         */  \n  \n        public boolean iterate(Double o) {  \n            if (o != null) {  \n                state.mSum += o;  \n                state.mCount++;  \n            }  \n            return true;  \n        }  \n  \n        /** \n         * terminatePartial无参数，其为iterate函数遍历结束后，返回轮转数据， * terminatePartial类似于hadoop的Combiner * * @return \n         */  \n  \n        public AvgState terminatePartial() {  \n            // combiner  \n            return state.mCount == 0 ? null : state;  \n        }  \n  \n        /** \n         * merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean * * @param o * @return \n         */  \n  \n        public boolean merge(AvgState avgState) {  \n            if (avgState != null) {  \n                state.mCount += avgState.mCount;  \n                state.mSum += avgState.mSum;  \n            }  \n            return true;  \n        }  \n  \n        /** \n         * terminate返回最终的聚集函数结果 * * @return \n         */  \n        public Double terminate() {  \n            return state.mCount == 0 ? null : Double.valueOf(state.mSum / state.mCount);  \n        }  \n    }  \n}\n```\n## 三、UDTF编写例子\n### 3.1 说明\n```\n1.引入import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;\n2.继承GenericUDTF类\n3.重写initialize（返回输出行信息：列个数，类型）, process, close三方法\n```\n### 3.2 实例\n```\n//create temporary function DaylistUDTF as 'com.ai.hive.udf.uc.DaylistUDTF';\npublic class DaylistUDTF extends GenericUDTF{\n\n    @Override\n    public void close() throws HiveException {\n        // TODO Auto-generated method stub\n\n    }\n    @Override\n    public void process(Object[] args) throws HiveException {\n        int argsLength = args.length;\n        int resultLength = argsLength+1;\n        String daylist = args[4].toString();\n        String dayId = args[11].toString();\n        String[] result = new String[resultLength];\n        for(int j = 0; j < argsLength; j++){\n            result[j] = args[j].toString();\n        }\n        try{\n            for (int i = daylist.length() - 1; i >= 0; i--) {\n                char c = daylist.charAt(i);\n                if (c == 49) {\n                    int diff = 0-(daylist.length() - i);\n                    SimpleDateFormat sdf=new SimpleDateFormat(\"yyyyMMdd\");\n                    Date dt=sdf.parse(dayId);\n                    Calendar rightNow = Calendar.getInstance();\n                    rightNow.setTime(dt);\n                    rightNow.add(Calendar.DAY_OF_YEAR,diff);\n                    Date dt1=rightNow.getTime();\n                    String reStr = sdf.format(dt1);\n                    result[resultLength-1] = reStr;\n                    forward(result);\n                }\n            }\n        }catch (Exception e){\n            System.out.println(\"error:\"+e.toString());\n        }\n    }\n    @Override\n    public StructObjectInspector initialize(ObjectInspector[] args)\n            throws UDFArgumentException {\n        ArrayList<String> fieldNames = new ArrayList<String>();\n        ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();\n        for (int i = 1; i <= 14; i++)\n        {\n            fieldNames.add(\"col\" + i);\n            fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);\n        }\n        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,fieldOIs);\n    }\n}\n```\n## 四、感谢大佬\n&emsp;[Hive 自定义函数 UDF UDAF UDTF](https://www.cnblogs.com/mzzcy/p/7119423.html)  \n&emsp;[hive udaf开发入门和运行过程详解](http://www.cnblogs.com/ggjucheng/archive/2013/02/01/2888051.html)  \n&emsp;[hive中udtf编写及使用](https://blog.csdn.net/u012485099/article/details/80790908)","slug":"hive开发udaf和udtf","published":1,"updated":"2019-09-18T13:11:36.914Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0q92d0c00067guo0yqoh4jq","content":"<p>&emsp;之前开发过udf，但是udf只能处理一对一的情况，也就是一个输入对应一个输出。而日常开发中却会遇到多种情况，普通的udf不能满足，这时候就需要引入udtf和udaf了。<br><a id=\"more\"></a></p>\n<h2 id=\"一、简介\"><a href=\"#一、简介\" class=\"headerlink\" title=\"一、简介\"></a>一、简介</h2><h3 id=\"1-1-UDAF\"><a href=\"#1-1-UDAF\" class=\"headerlink\" title=\"1.1 UDAF\"></a>1.1 UDAF</h3><p>&emsp;UDAF(User- Defined Aggregation Funcation)用户定义聚合函数，可对多行数据产生作用；等同与SQL中常用的SUM()，AVG()，也是聚合函数；<br>简单说就是多行输入一行输出。</p>\n<h3 id=\"1-2-UDTF\"><a href=\"#1-2-UDTF\" class=\"headerlink\" title=\"1.2 UDTF\"></a>1.2 UDTF</h3><p>&emsp;UDTF(User-Defined Table-Generating Functions)用户定义表生成函数，用来解决输入一行输出多行；<br>简单说就是一行输入多行输出。</p>\n<h2 id=\"二、UDAF编写例子\"><a href=\"#二、UDAF编写例子\" class=\"headerlink\" title=\"二、UDAF编写例子\"></a>二、UDAF编写例子</h2><h3 id=\"2-1-说明\"><a href=\"#2-1-说明\" class=\"headerlink\" title=\"2.1 说明\"></a>2.1 说明</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1.引入如下两下类</span><br><span class=\"line\">    import org.apache.hadoop.hive.ql.exec.UDAF  </span><br><span class=\"line\">    import org.apache.hadoop.hive.ql.exec.UDAFEvaluator  </span><br><span class=\"line\">2.函数类需要继承UDAF类，计算类Evaluator实现UDAFEvaluator接口</span><br><span class=\"line\">3.Evaluator需要实现UDAFEvaluator的init、iterate、terminatePartial、merge、terminate这几个函数。</span><br><span class=\"line\">    a）init函数实现接口UDAFEvaluator的init函数。</span><br><span class=\"line\">    b）iterate接收传入的参数，并进行内部的迭代。其返回类型为boolean。</span><br><span class=\"line\">    c）terminatePartial无参数，其为iterate函数遍历结束后，返回遍历得到的数据，terminatePartial类似于 hadoop的Combiner。</span><br><span class=\"line\">    d）merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean。</span><br><span class=\"line\">    e）terminate返回最终的聚集函数结果。</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-2-实例\"><a href=\"#2-2-实例\" class=\"headerlink\" title=\"2.2 实例\"></a>2.2 实例</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package hive.udaf;  </span><br><span class=\"line\">  </span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDAF;  </span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;  </span><br><span class=\"line\">//create temporary function udaf_avg &apos;hive.udaf.Avg&apos;</span><br><span class=\"line\">public class Avg extends UDAF &#123;  </span><br><span class=\"line\">    public static class AvgState &#123;  </span><br><span class=\"line\">        private long mCount;  </span><br><span class=\"line\">        private double mSum;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    public static class AvgEvaluator implements UDAFEvaluator &#123;  </span><br><span class=\"line\">        AvgState state;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        public AvgEvaluator() &#123;  </span><br><span class=\"line\">            super();  </span><br><span class=\"line\">            state = new AvgState();  </span><br><span class=\"line\">            init();  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        /** </span><br><span class=\"line\">         * init函数类似于构造函数，用于UDAF的初始化 </span><br><span class=\"line\">         */  </span><br><span class=\"line\">        public void init() &#123;  </span><br><span class=\"line\">            state.mSum = 0;  </span><br><span class=\"line\">            state.mCount = 0;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        /** </span><br><span class=\"line\">         * iterate接收传入的参数，并进行内部的轮转。其返回类型为boolean * * @param o * @return </span><br><span class=\"line\">         */  </span><br><span class=\"line\">  </span><br><span class=\"line\">        public boolean iterate(Double o) &#123;  </span><br><span class=\"line\">            if (o != null) &#123;  </span><br><span class=\"line\">                state.mSum += o;  </span><br><span class=\"line\">                state.mCount++;  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">            return true;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        /** </span><br><span class=\"line\">         * terminatePartial无参数，其为iterate函数遍历结束后，返回轮转数据， * terminatePartial类似于hadoop的Combiner * * @return </span><br><span class=\"line\">         */  </span><br><span class=\"line\">  </span><br><span class=\"line\">        public AvgState terminatePartial() &#123;  </span><br><span class=\"line\">            // combiner  </span><br><span class=\"line\">            return state.mCount == 0 ? null : state;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        /** </span><br><span class=\"line\">         * merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean * * @param o * @return </span><br><span class=\"line\">         */  </span><br><span class=\"line\">  </span><br><span class=\"line\">        public boolean merge(AvgState avgState) &#123;  </span><br><span class=\"line\">            if (avgState != null) &#123;  </span><br><span class=\"line\">                state.mCount += avgState.mCount;  </span><br><span class=\"line\">                state.mSum += avgState.mSum;  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">            return true;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        /** </span><br><span class=\"line\">         * terminate返回最终的聚集函数结果 * * @return </span><br><span class=\"line\">         */  </span><br><span class=\"line\">        public Double terminate() &#123;  </span><br><span class=\"line\">            return state.mCount == 0 ? null : Double.valueOf(state.mSum / state.mCount);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"三、UDTF编写例子\"><a href=\"#三、UDTF编写例子\" class=\"headerlink\" title=\"三、UDTF编写例子\"></a>三、UDTF编写例子</h2><h3 id=\"3-1-说明\"><a href=\"#3-1-说明\" class=\"headerlink\" title=\"3.1 说明\"></a>3.1 说明</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1.引入import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span><br><span class=\"line\">2.继承GenericUDTF类</span><br><span class=\"line\">3.重写initialize（返回输出行信息：列个数，类型）, process, close三方法</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-实例\"><a href=\"#3-2-实例\" class=\"headerlink\" title=\"3.2 实例\"></a>3.2 实例</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//create temporary function DaylistUDTF as &apos;com.ai.hive.udf.uc.DaylistUDTF&apos;;</span><br><span class=\"line\">public class DaylistUDTF extends GenericUDTF&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public void close() throws HiveException &#123;</span><br><span class=\"line\">        // TODO Auto-generated method stub</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public void process(Object[] args) throws HiveException &#123;</span><br><span class=\"line\">        int argsLength = args.length;</span><br><span class=\"line\">        int resultLength = argsLength+1;</span><br><span class=\"line\">        String daylist = args[4].toString();</span><br><span class=\"line\">        String dayId = args[11].toString();</span><br><span class=\"line\">        String[] result = new String[resultLength];</span><br><span class=\"line\">        for(int j = 0; j &lt; argsLength; j++)&#123;</span><br><span class=\"line\">            result[j] = args[j].toString();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        try&#123;</span><br><span class=\"line\">            for (int i = daylist.length() - 1; i &gt;= 0; i--) &#123;</span><br><span class=\"line\">                char c = daylist.charAt(i);</span><br><span class=\"line\">                if (c == 49) &#123;</span><br><span class=\"line\">                    int diff = 0-(daylist.length() - i);</span><br><span class=\"line\">                    SimpleDateFormat sdf=new SimpleDateFormat(&quot;yyyyMMdd&quot;);</span><br><span class=\"line\">                    Date dt=sdf.parse(dayId);</span><br><span class=\"line\">                    Calendar rightNow = Calendar.getInstance();</span><br><span class=\"line\">                    rightNow.setTime(dt);</span><br><span class=\"line\">                    rightNow.add(Calendar.DAY_OF_YEAR,diff);</span><br><span class=\"line\">                    Date dt1=rightNow.getTime();</span><br><span class=\"line\">                    String reStr = sdf.format(dt1);</span><br><span class=\"line\">                    result[resultLength-1] = reStr;</span><br><span class=\"line\">                    forward(result);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;catch (Exception e)&#123;</span><br><span class=\"line\">            System.out.println(&quot;error:&quot;+e.toString());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public StructObjectInspector initialize(ObjectInspector[] args)</span><br><span class=\"line\">            throws UDFArgumentException &#123;</span><br><span class=\"line\">        ArrayList&lt;String&gt; fieldNames = new ArrayList&lt;String&gt;();</span><br><span class=\"line\">        ArrayList&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;ObjectInspector&gt;();</span><br><span class=\"line\">        for (int i = 1; i &lt;= 14; i++)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            fieldNames.add(&quot;col&quot; + i);</span><br><span class=\"line\">            fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,fieldOIs);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"四、感谢大佬\"><a href=\"#四、感谢大佬\" class=\"headerlink\" title=\"四、感谢大佬\"></a>四、感谢大佬</h2><p>&emsp;<a href=\"https://www.cnblogs.com/mzzcy/p/7119423.html\" target=\"_blank\" rel=\"noopener\">Hive 自定义函数 UDF UDAF UDTF</a><br>&emsp;<a href=\"http://www.cnblogs.com/ggjucheng/archive/2013/02/01/2888051.html\" target=\"_blank\" rel=\"noopener\">hive udaf开发入门和运行过程详解</a><br>&emsp;<a href=\"https://blog.csdn.net/u012485099/article/details/80790908\" target=\"_blank\" rel=\"noopener\">hive中udtf编写及使用</a></p>\n","site":{"data":{}},"excerpt":"<p>&emsp;之前开发过udf，但是udf只能处理一对一的情况，也就是一个输入对应一个输出。而日常开发中却会遇到多种情况，普通的udf不能满足，这时候就需要引入udtf和udaf了。<br>","more":"</p>\n<h2 id=\"一、简介\"><a href=\"#一、简介\" class=\"headerlink\" title=\"一、简介\"></a>一、简介</h2><h3 id=\"1-1-UDAF\"><a href=\"#1-1-UDAF\" class=\"headerlink\" title=\"1.1 UDAF\"></a>1.1 UDAF</h3><p>&emsp;UDAF(User- Defined Aggregation Funcation)用户定义聚合函数，可对多行数据产生作用；等同与SQL中常用的SUM()，AVG()，也是聚合函数；<br>简单说就是多行输入一行输出。</p>\n<h3 id=\"1-2-UDTF\"><a href=\"#1-2-UDTF\" class=\"headerlink\" title=\"1.2 UDTF\"></a>1.2 UDTF</h3><p>&emsp;UDTF(User-Defined Table-Generating Functions)用户定义表生成函数，用来解决输入一行输出多行；<br>简单说就是一行输入多行输出。</p>\n<h2 id=\"二、UDAF编写例子\"><a href=\"#二、UDAF编写例子\" class=\"headerlink\" title=\"二、UDAF编写例子\"></a>二、UDAF编写例子</h2><h3 id=\"2-1-说明\"><a href=\"#2-1-说明\" class=\"headerlink\" title=\"2.1 说明\"></a>2.1 说明</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1.引入如下两下类</span><br><span class=\"line\">    import org.apache.hadoop.hive.ql.exec.UDAF  </span><br><span class=\"line\">    import org.apache.hadoop.hive.ql.exec.UDAFEvaluator  </span><br><span class=\"line\">2.函数类需要继承UDAF类，计算类Evaluator实现UDAFEvaluator接口</span><br><span class=\"line\">3.Evaluator需要实现UDAFEvaluator的init、iterate、terminatePartial、merge、terminate这几个函数。</span><br><span class=\"line\">    a）init函数实现接口UDAFEvaluator的init函数。</span><br><span class=\"line\">    b）iterate接收传入的参数，并进行内部的迭代。其返回类型为boolean。</span><br><span class=\"line\">    c）terminatePartial无参数，其为iterate函数遍历结束后，返回遍历得到的数据，terminatePartial类似于 hadoop的Combiner。</span><br><span class=\"line\">    d）merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean。</span><br><span class=\"line\">    e）terminate返回最终的聚集函数结果。</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-2-实例\"><a href=\"#2-2-实例\" class=\"headerlink\" title=\"2.2 实例\"></a>2.2 实例</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package hive.udaf;  </span><br><span class=\"line\">  </span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDAF;  </span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;  </span><br><span class=\"line\">//create temporary function udaf_avg &apos;hive.udaf.Avg&apos;</span><br><span class=\"line\">public class Avg extends UDAF &#123;  </span><br><span class=\"line\">    public static class AvgState &#123;  </span><br><span class=\"line\">        private long mCount;  </span><br><span class=\"line\">        private double mSum;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    public static class AvgEvaluator implements UDAFEvaluator &#123;  </span><br><span class=\"line\">        AvgState state;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        public AvgEvaluator() &#123;  </span><br><span class=\"line\">            super();  </span><br><span class=\"line\">            state = new AvgState();  </span><br><span class=\"line\">            init();  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        /** </span><br><span class=\"line\">         * init函数类似于构造函数，用于UDAF的初始化 </span><br><span class=\"line\">         */  </span><br><span class=\"line\">        public void init() &#123;  </span><br><span class=\"line\">            state.mSum = 0;  </span><br><span class=\"line\">            state.mCount = 0;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        /** </span><br><span class=\"line\">         * iterate接收传入的参数，并进行内部的轮转。其返回类型为boolean * * @param o * @return </span><br><span class=\"line\">         */  </span><br><span class=\"line\">  </span><br><span class=\"line\">        public boolean iterate(Double o) &#123;  </span><br><span class=\"line\">            if (o != null) &#123;  </span><br><span class=\"line\">                state.mSum += o;  </span><br><span class=\"line\">                state.mCount++;  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">            return true;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        /** </span><br><span class=\"line\">         * terminatePartial无参数，其为iterate函数遍历结束后，返回轮转数据， * terminatePartial类似于hadoop的Combiner * * @return </span><br><span class=\"line\">         */  </span><br><span class=\"line\">  </span><br><span class=\"line\">        public AvgState terminatePartial() &#123;  </span><br><span class=\"line\">            // combiner  </span><br><span class=\"line\">            return state.mCount == 0 ? null : state;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        /** </span><br><span class=\"line\">         * merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean * * @param o * @return </span><br><span class=\"line\">         */  </span><br><span class=\"line\">  </span><br><span class=\"line\">        public boolean merge(AvgState avgState) &#123;  </span><br><span class=\"line\">            if (avgState != null) &#123;  </span><br><span class=\"line\">                state.mCount += avgState.mCount;  </span><br><span class=\"line\">                state.mSum += avgState.mSum;  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">            return true;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        /** </span><br><span class=\"line\">         * terminate返回最终的聚集函数结果 * * @return </span><br><span class=\"line\">         */  </span><br><span class=\"line\">        public Double terminate() &#123;  </span><br><span class=\"line\">            return state.mCount == 0 ? null : Double.valueOf(state.mSum / state.mCount);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"三、UDTF编写例子\"><a href=\"#三、UDTF编写例子\" class=\"headerlink\" title=\"三、UDTF编写例子\"></a>三、UDTF编写例子</h2><h3 id=\"3-1-说明\"><a href=\"#3-1-说明\" class=\"headerlink\" title=\"3.1 说明\"></a>3.1 说明</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1.引入import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span><br><span class=\"line\">2.继承GenericUDTF类</span><br><span class=\"line\">3.重写initialize（返回输出行信息：列个数，类型）, process, close三方法</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-实例\"><a href=\"#3-2-实例\" class=\"headerlink\" title=\"3.2 实例\"></a>3.2 实例</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//create temporary function DaylistUDTF as &apos;com.ai.hive.udf.uc.DaylistUDTF&apos;;</span><br><span class=\"line\">public class DaylistUDTF extends GenericUDTF&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public void close() throws HiveException &#123;</span><br><span class=\"line\">        // TODO Auto-generated method stub</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public void process(Object[] args) throws HiveException &#123;</span><br><span class=\"line\">        int argsLength = args.length;</span><br><span class=\"line\">        int resultLength = argsLength+1;</span><br><span class=\"line\">        String daylist = args[4].toString();</span><br><span class=\"line\">        String dayId = args[11].toString();</span><br><span class=\"line\">        String[] result = new String[resultLength];</span><br><span class=\"line\">        for(int j = 0; j &lt; argsLength; j++)&#123;</span><br><span class=\"line\">            result[j] = args[j].toString();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        try&#123;</span><br><span class=\"line\">            for (int i = daylist.length() - 1; i &gt;= 0; i--) &#123;</span><br><span class=\"line\">                char c = daylist.charAt(i);</span><br><span class=\"line\">                if (c == 49) &#123;</span><br><span class=\"line\">                    int diff = 0-(daylist.length() - i);</span><br><span class=\"line\">                    SimpleDateFormat sdf=new SimpleDateFormat(&quot;yyyyMMdd&quot;);</span><br><span class=\"line\">                    Date dt=sdf.parse(dayId);</span><br><span class=\"line\">                    Calendar rightNow = Calendar.getInstance();</span><br><span class=\"line\">                    rightNow.setTime(dt);</span><br><span class=\"line\">                    rightNow.add(Calendar.DAY_OF_YEAR,diff);</span><br><span class=\"line\">                    Date dt1=rightNow.getTime();</span><br><span class=\"line\">                    String reStr = sdf.format(dt1);</span><br><span class=\"line\">                    result[resultLength-1] = reStr;</span><br><span class=\"line\">                    forward(result);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;catch (Exception e)&#123;</span><br><span class=\"line\">            System.out.println(&quot;error:&quot;+e.toString());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public StructObjectInspector initialize(ObjectInspector[] args)</span><br><span class=\"line\">            throws UDFArgumentException &#123;</span><br><span class=\"line\">        ArrayList&lt;String&gt; fieldNames = new ArrayList&lt;String&gt;();</span><br><span class=\"line\">        ArrayList&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;ObjectInspector&gt;();</span><br><span class=\"line\">        for (int i = 1; i &lt;= 14; i++)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            fieldNames.add(&quot;col&quot; + i);</span><br><span class=\"line\">            fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,fieldOIs);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"四、感谢大佬\"><a href=\"#四、感谢大佬\" class=\"headerlink\" title=\"四、感谢大佬\"></a>四、感谢大佬</h2><p>&emsp;<a href=\"https://www.cnblogs.com/mzzcy/p/7119423.html\" target=\"_blank\" rel=\"noopener\">Hive 自定义函数 UDF UDAF UDTF</a><br>&emsp;<a href=\"http://www.cnblogs.com/ggjucheng/archive/2013/02/01/2888051.html\" target=\"_blank\" rel=\"noopener\">hive udaf开发入门和运行过程详解</a><br>&emsp;<a href=\"https://blog.csdn.net/u012485099/article/details/80790908\" target=\"_blank\" rel=\"noopener\">hive中udtf编写及使用</a></p>"},{"title":"hive日常使用的几个小技巧（长期维护中...）","abbrlink":"d4f0dd5b","date":"2019-03-27T08:57:03.000Z","_content":"&emsp;长期维护中。。。。主要记录日常使用hive中会用到的小技巧\n<!--more-->\n### 1.简单查询不跑MapReduce\n&emsp;如果你想直接查询（select * from table），却不想执行MapReduce，可以使用FetchTask，FetchTask不同于MapReduce任务，它不会启动mapreduce，而是直接读取文件，输出结果。  \n```\n<property>\n  <name>hive.fetch.task.conversion</name>\n  <value>minimal</value>\n  <description>\n    Some select queries can be converted to single FETCH task \n    minimizing latency.Currently the query should be single \n    sourced not having any subquery and should not have\n    any aggregations or distincts (which incurrs RS), \n    lateral views and joins.\n    1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only\n    2. more    : SELECT, FILTER, LIMIT only (+TABLESAMPLE, virtual columns)\n  </description>\n</property>\n```\n&emsp;该参数默认值为minimal，表示运行“select * ”并带有limit查询时候，会将其转换为FetchTask；如果参数值为more，则select某一些列并带有limit条件时，也会将其转换为FetchTask任务。  \n&emsp;使用前提:单一数据源，即输入来源一个表或者分区；没有子查询；没有聚合运算和distinct；不能用于视图和join\n```\n实现：\nset hive.fetch.task.conversion=more \n```\n### 2.小文件合并\n&emsp;Hive中存在过多的小文件会给namecode带来巨大的性能压力,同时小文件过多会影响JOB的执行，hadoop会将一个job转换成多个task，即使对于每个小文件也需要一个task去单独处理，task作为一个独立的jvm实例，其开启和停止的开销可能会大大超过实际的任务处理时间。hive输出最终是mr的输出，即reducer（或mapper）的输出，有多少个reducer（mapper）输出就会生成多少个输出文件，根据shuffle/sort的原理，每个文件按照某个值进行shuffle后的结果。为了防止生成过多小文件，hive可以通过配置参数在mr过程中合并小文件。而且在执行sql之前将小文件都进行Merge，也会提高程序的性能。我们可以从两个方面进行优化，其一是map执行之前将小文件进行合并会提高性能，其二是输出的时候进行合并压缩，减少IO压力。\n```\nMap操作之前合并小文件：\n    set mapred.max.split.size=2048000000\n    #每个Map最大输入大小设置为2GB（单位：字节）\n    \n    set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat\n    #执行Map前进行小文件合并\n\n\n输出时进行合并：\n    sethive.merge.mapfiles = true\n    #在Map-only的任务结束时合并小文件\n\n    set hive.merge.mapredfiles= true\n    #在Map-Reduce的任务结束时合并小文件\n\n    set hive.merge.size.per.task = 1024000000\n    #合并后文件的大小为1GB左右\n\n    set hive.merge.smallfiles.avgsize=1024000000\n    #当输出文件的平均大小小于1GB时，启动一个独立的map-reduce任务进行文件merge\n\n\n如果需要压缩输出文件，就需要增加一个压缩编解码器，同时还有两个压缩方式和多种压缩编码器，压缩方式一个是压缩输出结果，一个是压缩中间结果，按照自己的需求选择，我需要的是gzip就选择的GzipCodec，同时也可以选择使用BZip2Codec、SnappyCodec、LzopCodec进行压缩。\n    压缩文件：\n    set hive.exec.compress.output=true;\n    #默认false，是否对输出结果压缩\n\n    set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;\n    #压缩格式设置\n\n    set mapred.output.compression.type=BLOCK;\n    #一共三种压缩方式（NONE, RECORD,BLOCK），BLOCK压缩率最高，一般用BLOCK。\n```\n```\n老哥关于map和reduce个数配置的说明：\nhttps://irwenqiang.iteye.com/blog/1535809\n```\n### 3.动态分区\n&emsp;有时候需要根据数据去动态生成分区，这时候就需要用到动态分区\n```\nset hive.exec.dynamic.partition=true; \n#开启动态分区功能\nset hive.exec.dynamic.partition.mode=nonstrict;  \n#表示允许所有分区都是动态的，否则必须有静态分区字段\nset hive.exec.max.dynamic.partitions=100000;\n#表示一个动态分区语句可以创建的最大动态分区个数，超出报错\nset hive.exec.max.dynamic.partitions.pernode=100000;\n#表示每个maper或reducer可以允许创建的最大动态分区个数，默认是100，超出则会报错。\nset hive.exec.max.created.files =10000\n#全局可以创建的最大文件个数，超出报错。\n```\n注意：动态分区不允许主分区采用动态列而副分区采用静态列，这样将导致所有的主分区都要创建副分区静态列所定义的分区\n### 4.容错\n&emsp;有时候因为各种原因难免会有hive执行时出错的问题，例如个别数据不规范等。这时候需要允许部分错误发生。\n```\nset mapreduce.map.failures.maxpercent=10; \n#设置map任务失败的比率，可以容许10%的任务失败\nset mapreduce.reduce.failures.maxpercent = 10; \n#设置reduce任务失败的比率，可以容许10%的任务失败\n```\n### 5.交集并集差集\n&emsp;交集（join），并集（union all）。这俩简单没啥说的。差集（left outer join、not in、not exists）\n```\n具体参考：\nhttps://blog.csdn.net/u010003835/article/details/80928732\n```\n这里涉及到left outer join和left semi join\n```\n#left semi join解决的是IN/EXISTS的问题\nselect a.id from a left semi join b on (a.id = b.id);\n\n#left outer join解决的是a差b的问题\nselect a.id from a left outer join b on (a.id = b.id) where b.id is null;\n```\n### 6.reduce资源申请时间\n&emsp;为了节省时间，map未执行完时就申请reduce资源。mapreduce.job.reduce.slowstart.completedmaps，这个参数可以控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源。  \n配置在mapred-site.xml，如下\n```\n<property>\n    <name>\n         mapreduce.job.reduce.slowstart.completedmaps\n    </name>\n    <value>\n        0.05\n    </value>\n    <description>\n        Fraction of the number of maps in the job which should be complete before reduces are scheduled for the job.\n     </description>\n</property>\n```\n&emsp;默认map5%时申请reduce资源，开始执行reduce操作，reduce可以开始进行拷贝map结果数据和做reduce shuffle操作。    \n&emsp;mapreduce.job.reduce.slowstart.completedmaps这个参数如果设置的过低，那么reduce就会过早地申请资源，造成资源浪费；如果这个参数设置的过高，比如为1，那么只有当map全部完成后，才为reduce申请资源，开始进行reduce操作，实际上是串行执行，不能采用并行方式充分利用资源。  \n&emsp;如果map数量比较多，一般建议提前开始为reduce申请资源。\n### 7.三种常用的判断空后赋值的方法\n```\n1)if(boolean testCondition, T valueTrue, T valueFalseOrNull)\n说明：当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNull\n应用：select if(a is null,0,1) from table;\n\n2)COALESCE(T v1, T v2, …) \n说明：返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL\n应用：select coalesce(a,0) from table;\n\n3）case a when b then c [else f] end\n说明：如果a等于b，那么返回c；如果a等于d，那么返回e；否则返回f\n应用：select case a\n            when a is null\n            then a=0\n            else a\n            end\n      from table;\n```\n\n### 8.hive transform\n&emsp;hive自定义函数除了支持udf还支持transform,可以引入脚本\n```\n首先添加脚本文件\nadd file /data/users/olicity/a.py;\nselect transform(a, b, c, d, e) using 'python a.py' as (f , g)\nfrom table;\n```\n自己没有比较过速度，不过看大佬们说是要比udf慢很多。","source":"_posts/hive日常使用的几个小技巧（长期维护中-）.md","raw":"---\ntitle: hive日常使用的几个小技巧（长期维护中...）\ntags:\n  - hive\nabbrlink: d4f0dd5b\ndate: 2019-03-27 16:57:03\n---\n&emsp;长期维护中。。。。主要记录日常使用hive中会用到的小技巧\n<!--more-->\n### 1.简单查询不跑MapReduce\n&emsp;如果你想直接查询（select * from table），却不想执行MapReduce，可以使用FetchTask，FetchTask不同于MapReduce任务，它不会启动mapreduce，而是直接读取文件，输出结果。  \n```\n<property>\n  <name>hive.fetch.task.conversion</name>\n  <value>minimal</value>\n  <description>\n    Some select queries can be converted to single FETCH task \n    minimizing latency.Currently the query should be single \n    sourced not having any subquery and should not have\n    any aggregations or distincts (which incurrs RS), \n    lateral views and joins.\n    1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only\n    2. more    : SELECT, FILTER, LIMIT only (+TABLESAMPLE, virtual columns)\n  </description>\n</property>\n```\n&emsp;该参数默认值为minimal，表示运行“select * ”并带有limit查询时候，会将其转换为FetchTask；如果参数值为more，则select某一些列并带有limit条件时，也会将其转换为FetchTask任务。  \n&emsp;使用前提:单一数据源，即输入来源一个表或者分区；没有子查询；没有聚合运算和distinct；不能用于视图和join\n```\n实现：\nset hive.fetch.task.conversion=more \n```\n### 2.小文件合并\n&emsp;Hive中存在过多的小文件会给namecode带来巨大的性能压力,同时小文件过多会影响JOB的执行，hadoop会将一个job转换成多个task，即使对于每个小文件也需要一个task去单独处理，task作为一个独立的jvm实例，其开启和停止的开销可能会大大超过实际的任务处理时间。hive输出最终是mr的输出，即reducer（或mapper）的输出，有多少个reducer（mapper）输出就会生成多少个输出文件，根据shuffle/sort的原理，每个文件按照某个值进行shuffle后的结果。为了防止生成过多小文件，hive可以通过配置参数在mr过程中合并小文件。而且在执行sql之前将小文件都进行Merge，也会提高程序的性能。我们可以从两个方面进行优化，其一是map执行之前将小文件进行合并会提高性能，其二是输出的时候进行合并压缩，减少IO压力。\n```\nMap操作之前合并小文件：\n    set mapred.max.split.size=2048000000\n    #每个Map最大输入大小设置为2GB（单位：字节）\n    \n    set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat\n    #执行Map前进行小文件合并\n\n\n输出时进行合并：\n    sethive.merge.mapfiles = true\n    #在Map-only的任务结束时合并小文件\n\n    set hive.merge.mapredfiles= true\n    #在Map-Reduce的任务结束时合并小文件\n\n    set hive.merge.size.per.task = 1024000000\n    #合并后文件的大小为1GB左右\n\n    set hive.merge.smallfiles.avgsize=1024000000\n    #当输出文件的平均大小小于1GB时，启动一个独立的map-reduce任务进行文件merge\n\n\n如果需要压缩输出文件，就需要增加一个压缩编解码器，同时还有两个压缩方式和多种压缩编码器，压缩方式一个是压缩输出结果，一个是压缩中间结果，按照自己的需求选择，我需要的是gzip就选择的GzipCodec，同时也可以选择使用BZip2Codec、SnappyCodec、LzopCodec进行压缩。\n    压缩文件：\n    set hive.exec.compress.output=true;\n    #默认false，是否对输出结果压缩\n\n    set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;\n    #压缩格式设置\n\n    set mapred.output.compression.type=BLOCK;\n    #一共三种压缩方式（NONE, RECORD,BLOCK），BLOCK压缩率最高，一般用BLOCK。\n```\n```\n老哥关于map和reduce个数配置的说明：\nhttps://irwenqiang.iteye.com/blog/1535809\n```\n### 3.动态分区\n&emsp;有时候需要根据数据去动态生成分区，这时候就需要用到动态分区\n```\nset hive.exec.dynamic.partition=true; \n#开启动态分区功能\nset hive.exec.dynamic.partition.mode=nonstrict;  \n#表示允许所有分区都是动态的，否则必须有静态分区字段\nset hive.exec.max.dynamic.partitions=100000;\n#表示一个动态分区语句可以创建的最大动态分区个数，超出报错\nset hive.exec.max.dynamic.partitions.pernode=100000;\n#表示每个maper或reducer可以允许创建的最大动态分区个数，默认是100，超出则会报错。\nset hive.exec.max.created.files =10000\n#全局可以创建的最大文件个数，超出报错。\n```\n注意：动态分区不允许主分区采用动态列而副分区采用静态列，这样将导致所有的主分区都要创建副分区静态列所定义的分区\n### 4.容错\n&emsp;有时候因为各种原因难免会有hive执行时出错的问题，例如个别数据不规范等。这时候需要允许部分错误发生。\n```\nset mapreduce.map.failures.maxpercent=10; \n#设置map任务失败的比率，可以容许10%的任务失败\nset mapreduce.reduce.failures.maxpercent = 10; \n#设置reduce任务失败的比率，可以容许10%的任务失败\n```\n### 5.交集并集差集\n&emsp;交集（join），并集（union all）。这俩简单没啥说的。差集（left outer join、not in、not exists）\n```\n具体参考：\nhttps://blog.csdn.net/u010003835/article/details/80928732\n```\n这里涉及到left outer join和left semi join\n```\n#left semi join解决的是IN/EXISTS的问题\nselect a.id from a left semi join b on (a.id = b.id);\n\n#left outer join解决的是a差b的问题\nselect a.id from a left outer join b on (a.id = b.id) where b.id is null;\n```\n### 6.reduce资源申请时间\n&emsp;为了节省时间，map未执行完时就申请reduce资源。mapreduce.job.reduce.slowstart.completedmaps，这个参数可以控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源。  \n配置在mapred-site.xml，如下\n```\n<property>\n    <name>\n         mapreduce.job.reduce.slowstart.completedmaps\n    </name>\n    <value>\n        0.05\n    </value>\n    <description>\n        Fraction of the number of maps in the job which should be complete before reduces are scheduled for the job.\n     </description>\n</property>\n```\n&emsp;默认map5%时申请reduce资源，开始执行reduce操作，reduce可以开始进行拷贝map结果数据和做reduce shuffle操作。    \n&emsp;mapreduce.job.reduce.slowstart.completedmaps这个参数如果设置的过低，那么reduce就会过早地申请资源，造成资源浪费；如果这个参数设置的过高，比如为1，那么只有当map全部完成后，才为reduce申请资源，开始进行reduce操作，实际上是串行执行，不能采用并行方式充分利用资源。  \n&emsp;如果map数量比较多，一般建议提前开始为reduce申请资源。\n### 7.三种常用的判断空后赋值的方法\n```\n1)if(boolean testCondition, T valueTrue, T valueFalseOrNull)\n说明：当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNull\n应用：select if(a is null,0,1) from table;\n\n2)COALESCE(T v1, T v2, …) \n说明：返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL\n应用：select coalesce(a,0) from table;\n\n3）case a when b then c [else f] end\n说明：如果a等于b，那么返回c；如果a等于d，那么返回e；否则返回f\n应用：select case a\n            when a is null\n            then a=0\n            else a\n            end\n      from table;\n```\n\n### 8.hive transform\n&emsp;hive自定义函数除了支持udf还支持transform,可以引入脚本\n```\n首先添加脚本文件\nadd file /data/users/olicity/a.py;\nselect transform(a, b, c, d, e) using 'python a.py' as (f , g)\nfrom table;\n```\n自己没有比较过速度，不过看大佬们说是要比udf慢很多。","slug":"hive日常使用的几个小技巧（长期维护中-）","published":1,"updated":"2019-09-18T13:11:36.915Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0q92d0e00097guoofbno2tr","content":"<p>&emsp;长期维护中。。。。主要记录日常使用hive中会用到的小技巧<br><a id=\"more\"></a></p>\n<h3 id=\"1-简单查询不跑MapReduce\"><a href=\"#1-简单查询不跑MapReduce\" class=\"headerlink\" title=\"1.简单查询不跑MapReduce\"></a>1.简单查询不跑MapReduce</h3><p>&emsp;如果你想直接查询（select * from table），却不想执行MapReduce，可以使用FetchTask，FetchTask不同于MapReduce任务，它不会启动mapreduce，而是直接读取文件，输出结果。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt;</span><br><span class=\"line\">  &lt;value&gt;minimal&lt;/value&gt;</span><br><span class=\"line\">  &lt;description&gt;</span><br><span class=\"line\">    Some select queries can be converted to single FETCH task </span><br><span class=\"line\">    minimizing latency.Currently the query should be single </span><br><span class=\"line\">    sourced not having any subquery and should not have</span><br><span class=\"line\">    any aggregations or distincts (which incurrs RS), </span><br><span class=\"line\">    lateral views and joins.</span><br><span class=\"line\">    1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only</span><br><span class=\"line\">    2. more    : SELECT, FILTER, LIMIT only (+TABLESAMPLE, virtual columns)</span><br><span class=\"line\">  &lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;该参数默认值为minimal，表示运行“select * ”并带有limit查询时候，会将其转换为FetchTask；如果参数值为more，则select某一些列并带有limit条件时，也会将其转换为FetchTask任务。<br>&emsp;使用前提:单一数据源，即输入来源一个表或者分区；没有子查询；没有聚合运算和distinct；不能用于视图和join<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">实现：</span><br><span class=\"line\">set hive.fetch.task.conversion=more</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-小文件合并\"><a href=\"#2-小文件合并\" class=\"headerlink\" title=\"2.小文件合并\"></a>2.小文件合并</h3><p>&emsp;Hive中存在过多的小文件会给namecode带来巨大的性能压力,同时小文件过多会影响JOB的执行，hadoop会将一个job转换成多个task，即使对于每个小文件也需要一个task去单独处理，task作为一个独立的jvm实例，其开启和停止的开销可能会大大超过实际的任务处理时间。hive输出最终是mr的输出，即reducer（或mapper）的输出，有多少个reducer（mapper）输出就会生成多少个输出文件，根据shuffle/sort的原理，每个文件按照某个值进行shuffle后的结果。为了防止生成过多小文件，hive可以通过配置参数在mr过程中合并小文件。而且在执行sql之前将小文件都进行Merge，也会提高程序的性能。我们可以从两个方面进行优化，其一是map执行之前将小文件进行合并会提高性能，其二是输出的时候进行合并压缩，减少IO压力。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Map操作之前合并小文件：</span><br><span class=\"line\">    set mapred.max.split.size=2048000000</span><br><span class=\"line\">    #每个Map最大输入大小设置为2GB（单位：字节）</span><br><span class=\"line\">    </span><br><span class=\"line\">    set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</span><br><span class=\"line\">    #执行Map前进行小文件合并</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">输出时进行合并：</span><br><span class=\"line\">    sethive.merge.mapfiles = true</span><br><span class=\"line\">    #在Map-only的任务结束时合并小文件</span><br><span class=\"line\"></span><br><span class=\"line\">    set hive.merge.mapredfiles= true</span><br><span class=\"line\">    #在Map-Reduce的任务结束时合并小文件</span><br><span class=\"line\"></span><br><span class=\"line\">    set hive.merge.size.per.task = 1024000000</span><br><span class=\"line\">    #合并后文件的大小为1GB左右</span><br><span class=\"line\"></span><br><span class=\"line\">    set hive.merge.smallfiles.avgsize=1024000000</span><br><span class=\"line\">    #当输出文件的平均大小小于1GB时，启动一个独立的map-reduce任务进行文件merge</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">如果需要压缩输出文件，就需要增加一个压缩编解码器，同时还有两个压缩方式和多种压缩编码器，压缩方式一个是压缩输出结果，一个是压缩中间结果，按照自己的需求选择，我需要的是gzip就选择的GzipCodec，同时也可以选择使用BZip2Codec、SnappyCodec、LzopCodec进行压缩。</span><br><span class=\"line\">    压缩文件：</span><br><span class=\"line\">    set hive.exec.compress.output=true;</span><br><span class=\"line\">    #默认false，是否对输出结果压缩</span><br><span class=\"line\"></span><br><span class=\"line\">    set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;</span><br><span class=\"line\">    #压缩格式设置</span><br><span class=\"line\"></span><br><span class=\"line\">    set mapred.output.compression.type=BLOCK;</span><br><span class=\"line\">    #一共三种压缩方式（NONE, RECORD,BLOCK），BLOCK压缩率最高，一般用BLOCK。</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">老哥关于map和reduce个数配置的说明：</span><br><span class=\"line\">https://irwenqiang.iteye.com/blog/1535809</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-动态分区\"><a href=\"#3-动态分区\" class=\"headerlink\" title=\"3.动态分区\"></a>3.动态分区</h3><p>&emsp;有时候需要根据数据去动态生成分区，这时候就需要用到动态分区<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set hive.exec.dynamic.partition=true; </span><br><span class=\"line\">#开启动态分区功能</span><br><span class=\"line\">set hive.exec.dynamic.partition.mode=nonstrict;  </span><br><span class=\"line\">#表示允许所有分区都是动态的，否则必须有静态分区字段</span><br><span class=\"line\">set hive.exec.max.dynamic.partitions=100000;</span><br><span class=\"line\">#表示一个动态分区语句可以创建的最大动态分区个数，超出报错</span><br><span class=\"line\">set hive.exec.max.dynamic.partitions.pernode=100000;</span><br><span class=\"line\">#表示每个maper或reducer可以允许创建的最大动态分区个数，默认是100，超出则会报错。</span><br><span class=\"line\">set hive.exec.max.created.files =10000</span><br><span class=\"line\">#全局可以创建的最大文件个数，超出报错。</span><br></pre></td></tr></table></figure></p>\n<p>注意：动态分区不允许主分区采用动态列而副分区采用静态列，这样将导致所有的主分区都要创建副分区静态列所定义的分区</p>\n<h3 id=\"4-容错\"><a href=\"#4-容错\" class=\"headerlink\" title=\"4.容错\"></a>4.容错</h3><p>&emsp;有时候因为各种原因难免会有hive执行时出错的问题，例如个别数据不规范等。这时候需要允许部分错误发生。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set mapreduce.map.failures.maxpercent=10; </span><br><span class=\"line\">#设置map任务失败的比率，可以容许10%的任务失败</span><br><span class=\"line\">set mapreduce.reduce.failures.maxpercent = 10; </span><br><span class=\"line\">#设置reduce任务失败的比率，可以容许10%的任务失败</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-交集并集差集\"><a href=\"#5-交集并集差集\" class=\"headerlink\" title=\"5.交集并集差集\"></a>5.交集并集差集</h3><p>&emsp;交集（join），并集（union all）。这俩简单没啥说的。差集（left outer join、not in、not exists）<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">具体参考：</span><br><span class=\"line\">https://blog.csdn.net/u010003835/article/details/80928732</span><br></pre></td></tr></table></figure></p>\n<p>这里涉及到left outer join和left semi join<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#left semi join解决的是IN/EXISTS的问题</span><br><span class=\"line\">select a.id from a left semi join b on (a.id = b.id);</span><br><span class=\"line\"></span><br><span class=\"line\">#left outer join解决的是a差b的问题</span><br><span class=\"line\">select a.id from a left outer join b on (a.id = b.id) where b.id is null;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-reduce资源申请时间\"><a href=\"#6-reduce资源申请时间\" class=\"headerlink\" title=\"6.reduce资源申请时间\"></a>6.reduce资源申请时间</h3><p>&emsp;为了节省时间，map未执行完时就申请reduce资源。mapreduce.job.reduce.slowstart.completedmaps，这个参数可以控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源。<br>配置在mapred-site.xml，如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;</span><br><span class=\"line\">         mapreduce.job.reduce.slowstart.completedmaps</span><br><span class=\"line\">    &lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;</span><br><span class=\"line\">        0.05</span><br><span class=\"line\">    &lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">        Fraction of the number of maps in the job which should be complete before reduces are scheduled for the job.</span><br><span class=\"line\">     &lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;默认map5%时申请reduce资源，开始执行reduce操作，reduce可以开始进行拷贝map结果数据和做reduce shuffle操作。<br>&emsp;mapreduce.job.reduce.slowstart.completedmaps这个参数如果设置的过低，那么reduce就会过早地申请资源，造成资源浪费；如果这个参数设置的过高，比如为1，那么只有当map全部完成后，才为reduce申请资源，开始进行reduce操作，实际上是串行执行，不能采用并行方式充分利用资源。<br>&emsp;如果map数量比较多，一般建议提前开始为reduce申请资源。</p>\n<h3 id=\"7-三种常用的判断空后赋值的方法\"><a href=\"#7-三种常用的判断空后赋值的方法\" class=\"headerlink\" title=\"7.三种常用的判断空后赋值的方法\"></a>7.三种常用的判断空后赋值的方法</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1)if(boolean testCondition, T valueTrue, T valueFalseOrNull)</span><br><span class=\"line\">说明：当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNull</span><br><span class=\"line\">应用：select if(a is null,0,1) from table;</span><br><span class=\"line\"></span><br><span class=\"line\">2)COALESCE(T v1, T v2, …) </span><br><span class=\"line\">说明：返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL</span><br><span class=\"line\">应用：select coalesce(a,0) from table;</span><br><span class=\"line\"></span><br><span class=\"line\">3）case a when b then c [else f] end</span><br><span class=\"line\">说明：如果a等于b，那么返回c；如果a等于d，那么返回e；否则返回f</span><br><span class=\"line\">应用：select case a</span><br><span class=\"line\">            when a is null</span><br><span class=\"line\">            then a=0</span><br><span class=\"line\">            else a</span><br><span class=\"line\">            end</span><br><span class=\"line\">      from table;</span><br></pre></td></tr></table></figure>\n<h3 id=\"8-hive-transform\"><a href=\"#8-hive-transform\" class=\"headerlink\" title=\"8.hive transform\"></a>8.hive transform</h3><p>&emsp;hive自定义函数除了支持udf还支持transform,可以引入脚本<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">首先添加脚本文件</span><br><span class=\"line\">add file /data/users/olicity/a.py;</span><br><span class=\"line\">select transform(a, b, c, d, e) using &apos;python a.py&apos; as (f , g)</span><br><span class=\"line\">from table;</span><br></pre></td></tr></table></figure></p>\n<p>自己没有比较过速度，不过看大佬们说是要比udf慢很多。</p>\n","site":{"data":{}},"excerpt":"<p>&emsp;长期维护中。。。。主要记录日常使用hive中会用到的小技巧<br>","more":"</p>\n<h3 id=\"1-简单查询不跑MapReduce\"><a href=\"#1-简单查询不跑MapReduce\" class=\"headerlink\" title=\"1.简单查询不跑MapReduce\"></a>1.简单查询不跑MapReduce</h3><p>&emsp;如果你想直接查询（select * from table），却不想执行MapReduce，可以使用FetchTask，FetchTask不同于MapReduce任务，它不会启动mapreduce，而是直接读取文件，输出结果。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt;</span><br><span class=\"line\">  &lt;value&gt;minimal&lt;/value&gt;</span><br><span class=\"line\">  &lt;description&gt;</span><br><span class=\"line\">    Some select queries can be converted to single FETCH task </span><br><span class=\"line\">    minimizing latency.Currently the query should be single </span><br><span class=\"line\">    sourced not having any subquery and should not have</span><br><span class=\"line\">    any aggregations or distincts (which incurrs RS), </span><br><span class=\"line\">    lateral views and joins.</span><br><span class=\"line\">    1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only</span><br><span class=\"line\">    2. more    : SELECT, FILTER, LIMIT only (+TABLESAMPLE, virtual columns)</span><br><span class=\"line\">  &lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;该参数默认值为minimal，表示运行“select * ”并带有limit查询时候，会将其转换为FetchTask；如果参数值为more，则select某一些列并带有limit条件时，也会将其转换为FetchTask任务。<br>&emsp;使用前提:单一数据源，即输入来源一个表或者分区；没有子查询；没有聚合运算和distinct；不能用于视图和join<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">实现：</span><br><span class=\"line\">set hive.fetch.task.conversion=more</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-小文件合并\"><a href=\"#2-小文件合并\" class=\"headerlink\" title=\"2.小文件合并\"></a>2.小文件合并</h3><p>&emsp;Hive中存在过多的小文件会给namecode带来巨大的性能压力,同时小文件过多会影响JOB的执行，hadoop会将一个job转换成多个task，即使对于每个小文件也需要一个task去单独处理，task作为一个独立的jvm实例，其开启和停止的开销可能会大大超过实际的任务处理时间。hive输出最终是mr的输出，即reducer（或mapper）的输出，有多少个reducer（mapper）输出就会生成多少个输出文件，根据shuffle/sort的原理，每个文件按照某个值进行shuffle后的结果。为了防止生成过多小文件，hive可以通过配置参数在mr过程中合并小文件。而且在执行sql之前将小文件都进行Merge，也会提高程序的性能。我们可以从两个方面进行优化，其一是map执行之前将小文件进行合并会提高性能，其二是输出的时候进行合并压缩，减少IO压力。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Map操作之前合并小文件：</span><br><span class=\"line\">    set mapred.max.split.size=2048000000</span><br><span class=\"line\">    #每个Map最大输入大小设置为2GB（单位：字节）</span><br><span class=\"line\">    </span><br><span class=\"line\">    set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</span><br><span class=\"line\">    #执行Map前进行小文件合并</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">输出时进行合并：</span><br><span class=\"line\">    sethive.merge.mapfiles = true</span><br><span class=\"line\">    #在Map-only的任务结束时合并小文件</span><br><span class=\"line\"></span><br><span class=\"line\">    set hive.merge.mapredfiles= true</span><br><span class=\"line\">    #在Map-Reduce的任务结束时合并小文件</span><br><span class=\"line\"></span><br><span class=\"line\">    set hive.merge.size.per.task = 1024000000</span><br><span class=\"line\">    #合并后文件的大小为1GB左右</span><br><span class=\"line\"></span><br><span class=\"line\">    set hive.merge.smallfiles.avgsize=1024000000</span><br><span class=\"line\">    #当输出文件的平均大小小于1GB时，启动一个独立的map-reduce任务进行文件merge</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">如果需要压缩输出文件，就需要增加一个压缩编解码器，同时还有两个压缩方式和多种压缩编码器，压缩方式一个是压缩输出结果，一个是压缩中间结果，按照自己的需求选择，我需要的是gzip就选择的GzipCodec，同时也可以选择使用BZip2Codec、SnappyCodec、LzopCodec进行压缩。</span><br><span class=\"line\">    压缩文件：</span><br><span class=\"line\">    set hive.exec.compress.output=true;</span><br><span class=\"line\">    #默认false，是否对输出结果压缩</span><br><span class=\"line\"></span><br><span class=\"line\">    set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;</span><br><span class=\"line\">    #压缩格式设置</span><br><span class=\"line\"></span><br><span class=\"line\">    set mapred.output.compression.type=BLOCK;</span><br><span class=\"line\">    #一共三种压缩方式（NONE, RECORD,BLOCK），BLOCK压缩率最高，一般用BLOCK。</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">老哥关于map和reduce个数配置的说明：</span><br><span class=\"line\">https://irwenqiang.iteye.com/blog/1535809</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-动态分区\"><a href=\"#3-动态分区\" class=\"headerlink\" title=\"3.动态分区\"></a>3.动态分区</h3><p>&emsp;有时候需要根据数据去动态生成分区，这时候就需要用到动态分区<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set hive.exec.dynamic.partition=true; </span><br><span class=\"line\">#开启动态分区功能</span><br><span class=\"line\">set hive.exec.dynamic.partition.mode=nonstrict;  </span><br><span class=\"line\">#表示允许所有分区都是动态的，否则必须有静态分区字段</span><br><span class=\"line\">set hive.exec.max.dynamic.partitions=100000;</span><br><span class=\"line\">#表示一个动态分区语句可以创建的最大动态分区个数，超出报错</span><br><span class=\"line\">set hive.exec.max.dynamic.partitions.pernode=100000;</span><br><span class=\"line\">#表示每个maper或reducer可以允许创建的最大动态分区个数，默认是100，超出则会报错。</span><br><span class=\"line\">set hive.exec.max.created.files =10000</span><br><span class=\"line\">#全局可以创建的最大文件个数，超出报错。</span><br></pre></td></tr></table></figure></p>\n<p>注意：动态分区不允许主分区采用动态列而副分区采用静态列，这样将导致所有的主分区都要创建副分区静态列所定义的分区</p>\n<h3 id=\"4-容错\"><a href=\"#4-容错\" class=\"headerlink\" title=\"4.容错\"></a>4.容错</h3><p>&emsp;有时候因为各种原因难免会有hive执行时出错的问题，例如个别数据不规范等。这时候需要允许部分错误发生。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set mapreduce.map.failures.maxpercent=10; </span><br><span class=\"line\">#设置map任务失败的比率，可以容许10%的任务失败</span><br><span class=\"line\">set mapreduce.reduce.failures.maxpercent = 10; </span><br><span class=\"line\">#设置reduce任务失败的比率，可以容许10%的任务失败</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-交集并集差集\"><a href=\"#5-交集并集差集\" class=\"headerlink\" title=\"5.交集并集差集\"></a>5.交集并集差集</h3><p>&emsp;交集（join），并集（union all）。这俩简单没啥说的。差集（left outer join、not in、not exists）<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">具体参考：</span><br><span class=\"line\">https://blog.csdn.net/u010003835/article/details/80928732</span><br></pre></td></tr></table></figure></p>\n<p>这里涉及到left outer join和left semi join<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#left semi join解决的是IN/EXISTS的问题</span><br><span class=\"line\">select a.id from a left semi join b on (a.id = b.id);</span><br><span class=\"line\"></span><br><span class=\"line\">#left outer join解决的是a差b的问题</span><br><span class=\"line\">select a.id from a left outer join b on (a.id = b.id) where b.id is null;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-reduce资源申请时间\"><a href=\"#6-reduce资源申请时间\" class=\"headerlink\" title=\"6.reduce资源申请时间\"></a>6.reduce资源申请时间</h3><p>&emsp;为了节省时间，map未执行完时就申请reduce资源。mapreduce.job.reduce.slowstart.completedmaps，这个参数可以控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源。<br>配置在mapred-site.xml，如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;</span><br><span class=\"line\">         mapreduce.job.reduce.slowstart.completedmaps</span><br><span class=\"line\">    &lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;</span><br><span class=\"line\">        0.05</span><br><span class=\"line\">    &lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">        Fraction of the number of maps in the job which should be complete before reduces are scheduled for the job.</span><br><span class=\"line\">     &lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;默认map5%时申请reduce资源，开始执行reduce操作，reduce可以开始进行拷贝map结果数据和做reduce shuffle操作。<br>&emsp;mapreduce.job.reduce.slowstart.completedmaps这个参数如果设置的过低，那么reduce就会过早地申请资源，造成资源浪费；如果这个参数设置的过高，比如为1，那么只有当map全部完成后，才为reduce申请资源，开始进行reduce操作，实际上是串行执行，不能采用并行方式充分利用资源。<br>&emsp;如果map数量比较多，一般建议提前开始为reduce申请资源。</p>\n<h3 id=\"7-三种常用的判断空后赋值的方法\"><a href=\"#7-三种常用的判断空后赋值的方法\" class=\"headerlink\" title=\"7.三种常用的判断空后赋值的方法\"></a>7.三种常用的判断空后赋值的方法</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1)if(boolean testCondition, T valueTrue, T valueFalseOrNull)</span><br><span class=\"line\">说明：当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNull</span><br><span class=\"line\">应用：select if(a is null,0,1) from table;</span><br><span class=\"line\"></span><br><span class=\"line\">2)COALESCE(T v1, T v2, …) </span><br><span class=\"line\">说明：返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL</span><br><span class=\"line\">应用：select coalesce(a,0) from table;</span><br><span class=\"line\"></span><br><span class=\"line\">3）case a when b then c [else f] end</span><br><span class=\"line\">说明：如果a等于b，那么返回c；如果a等于d，那么返回e；否则返回f</span><br><span class=\"line\">应用：select case a</span><br><span class=\"line\">            when a is null</span><br><span class=\"line\">            then a=0</span><br><span class=\"line\">            else a</span><br><span class=\"line\">            end</span><br><span class=\"line\">      from table;</span><br></pre></td></tr></table></figure>\n<h3 id=\"8-hive-transform\"><a href=\"#8-hive-transform\" class=\"headerlink\" title=\"8.hive transform\"></a>8.hive transform</h3><p>&emsp;hive自定义函数除了支持udf还支持transform,可以引入脚本<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">首先添加脚本文件</span><br><span class=\"line\">add file /data/users/olicity/a.py;</span><br><span class=\"line\">select transform(a, b, c, d, e) using &apos;python a.py&apos; as (f , g)</span><br><span class=\"line\">from table;</span><br></pre></td></tr></table></figure></p>\n<p>自己没有比较过速度，不过看大佬们说是要比udf慢很多。</p>"},{"title":"kafka动态配置topic","abbrlink":"bf5e9970","date":"2019-01-10T19:31:59.000Z","_content":"\n&emsp;之前使用@org.springframework.kafka.annotation.KafkaListener这个注解的时候，是在yml文件中配置，然后使用@KafkaListener(topics = {\"${kafka.topic.a2b.name}\"})，这样去单独监听某一个topic，生产者也固定在代码里定义变量读取配置文件。昨天改了个需求，希望以后通过配置文件去动态配置生产者和消费者的topic（不知道个数和topic名字），而不需要改代码。   \n<!--more-->\n## 一、踩坑\n&emsp;刚开始的时候，由于考虑不充分（没有考虑到topic个数未知），想到@KafkaListener注解中的topics本身就是个字符串数组，于是想通过传入变量的形式。产生了以下两种方法：  \n### 1.传入变量方法一\n&emsp; 使用@Value注解提取配置文件中相关配置，@KafkaListener中传入变量\n```\n    public static String[] topicArr;\n    @Value(\"${kafka.bootstrap.servers}\")\n    public void setTopicArr(String[] value){\n        String topicArr = value;\n    }\n    @KafkaListener(topics= topicArr)\n```\nemmmm。。。结果可想而知，不行。\n### 2.传入变量方法二\n&emsp;还是传入变量，不过这次写了个动态配置的代码\n```\n    注解里这么写\n    @KafkaListener(topics = \"${topicName1}\",\"${topicName2}\",\"${topicName3}\")\n    提前将yml文件里添加\n    topics: topicName1,topicName2,topicName3\n    然后加载进来\n    @Value(\"${kafka.topics}\")\n    public void setTopics(String value){\n        topics = value;\n    }\n    动态配置代码：\n    @Configuration\n    public class KafkaTopicConfiguration implements InitializingBean {\n        @Autowired\n        private KafkaConfig kafkaconfig;\n        @Override\n        public void afterPropertiesSet() throws Exception {\n            String[] topicArr = kafkaconfig.split(\",\");\n            int i = 1;\n            for(String topic : topicArr){\n                String topicName = \"topicName\"+i;\n                System.setProperty(topicName, topic);\n            }\n        }\n    }\n```\n相比方法一，可行。但是未知topic数量呢。GG。\n### 3.不用注解\n&emsp;百度找到几个老哥的动态获取并创建topic的方法\n```\nhttps://www.cnblogs.com/gaoyawei/p/7723974.html\nhttps://www.cnblogs.com/huxi2b/p/7040617.html\nhttps://blog.csdn.net/qq_27232757/article/details/78970830\n```\n写了几版，各种各样的问题，还是我太菜。就想再看看有没有别的简单点的解决办法，没有了再回来搞这个。\n### 4.正则匹配topic\n&emsp;这期间又找到一个使用正则匹配topic的。直接贴[链接](https://www.jianshu.com/p/4c422a6a6c7a)。\n```\n@KafkaListener(topicPattern = \"showcase.*\")\n这里使用正则匹配topic，其中【*】之前得加上【.】才能匹配到。\n```\n中间模仿写了一版使用正则匹配的，其实也可以糊弄实现需求，除了topic取名的时候一定得规范以外，还得考虑到如果不想用某个topic了又得想怎么去避免他。  \n这种方法不太严谨，继续踩坑吧。\n\n## 二、问题解决\n&emsp;用蹩脚的英语google了一下，嗯?好多老哥们也是用的以上差不多的方法。然而最后在某个老哥github的[issues](https://github.com/spring-projects/spring-kafka/issues/361)中看到了解决办法。老哥的需求跟我差不多，感谢大佬,贴上最终问题解决方案。  \n### 1.kafka消费者监听配置\n```\n还是注解的形式\n@KafkaListener(topics = \"#{'${kafka.listener_topics}'.split(',')}\")\n```\n读取yml文件中kafka.listener_topics的参数，然后根据“,”去split,得到一个topics数组。  \n这么做就可以根据配置文件动态的去监听topic。\n\n### 2.yml配置文件\n```\n只列出topic相关部分（mqTypes是我用来判断使用哪个topic发送的）\n    kafka:\n      listener_topics: kafka-topic-a2b,kafka-topic-c2b\n      consume:\n        topic:\n          - name: kafka-topic-a2b\n            partitions: 12\n            replication_factor: 2\n          - name: kafka-topic-c2b\n            partitions: 12\n            replication_factor: 2\n      product:\n        topic:\n          - name: kafka-topic-b2a\n            partitions: 12\n            replication_factor: 2\n            mqTypes: type1\n          - name: kafka-topic-b2c\n            partitions: 12\n            replication_factor: 2\n            mqTypes: type1\n```\n### 3.yml参数解析\n这里我将kafka的topic相关加载到bean中处理。  \n创建KafkaConsumerBean和KafkaProducerBean分别用来存储yml中生产者和消费者的topic相关参数\n```\n//KafkaConsumerBean\n@Component\n@ConfigurationProperties(prefix = \"kafka.consume\")\npublic class KafkaConsumerBean {\n    private List<Map<String,String>> topic;\n    public void setTopic(List<Map<String, String>> topic) {\n        this.topic = topic;\n    }\n    public List<Map<String, String>> getTopic() {\n        return topic;\n    }\n}\n\n//KafkaProducerBean\n@Component\n@ConfigurationProperties(prefix = \"kafka.product\")\npublic class KafkaProducerBean {\n    private List<Map<String,String>> topic;\n    public void setTopic(List<Map<String, String>> topic) {\n        this.topic = topic;\n    }\n\n    private Map<String,String> mqType2NameMap = new HashMap<String,String>();\n    public List<Map<String, String>> getTopic() {\n        return topic;\n    }\n\n    public String getTopic(String mqType){\n        String name = mqType2NameMap.get(mqType);\n        if(name != null){\n            return name;\n        }else{\n            for(Map<String,String> topicProperty : topic){\n                if (topicProperty.get(\"mqTypes\").indexOf(mqType) >= 0){\n                    name = topicProperty.get(\"name\");\n                    mqType2NameMap.put(mqType,name);\n                    return name;\n                }\n            }\n            return null;\n        }\n\n    }\n}\n\n```\n### 4.创建topic\n```\n    List<Map<String,String>> producerTopicList = kafkaProducerBean.getTopic();\n    for (Map<String,String> topicProperty : producerTopicList){\n        KafkaClient.createTopic(topicProperty.get(\"name\"),Integer.parseInt(topicProperty.get(\"partitions\")),Integer.parseInt(topicProperty.get(\"replication_factor\")));\n    }\n    List<Map<String,String>> consumerTopicList = kafkaConsumerBean.getTopic();\n    for (Map<String,String> topicProperty : consumerTopicList){\n        KafkaClient.createTopic(topicProperty.get(\"name\"),Integer.parseInt(topicProperty.get(\"partitions\")),Integer.parseInt(topicProperty.get(\"replication_factor\")));\n    }\n```\n## 三、总结\n&emsp;上面解决问题的方法关键在于\n```\n@KafkaListener(topics = \"#{'${kafka.listener_topics}'.split(',')}\")\n```\n@KafkaListener这个注解会去读取spring的yml配置文件中\n```\nkafka:\n      listener_topics: kafka-topic-a2b,kafka-topic-c2b\n```\n这块listener_topics配置信息，然后通过','分割成topic数组，KafkaListener注解中的 topics 参数，本身就是个数组，如下\n```\n//\n// Source code recreated from a .class file by IntelliJ IDEA\n// (powered by Fernflower decompiler)\n//\n\npackage org.springframework.kafka.annotation;\n\nimport java.lang.annotation.Documented;\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Repeatable;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\nimport org.springframework.messaging.handler.annotation.MessageMapping;\n\n@Target({ElementType.TYPE, ElementType.METHOD, ElementType.ANNOTATION_TYPE})\n@Retention(RetentionPolicy.RUNTIME)\n@MessageMapping\n@Documented\n@Repeatable(KafkaListeners.class)\npublic @interface KafkaListener {\n    String id() default \"\";\n\n    String containerFactory() default \"\";\n\n    String[] topics() default {};\n\n    String topicPattern() default \"\";\n\n    TopicPartition[] topicPartitions() default {};\n\n    String group() default \"\";\n}\n```\n&emsp;结合我之前的kafka文章，应该是可以拼出一套成型的。","source":"_posts/kafka动态配置topic.md","raw":"---\ntitle: kafka动态配置topic\ntags:\n  - kafka\nabbrlink: bf5e9970\ndate: 2019-01-11 03:31:59\n---\n\n&emsp;之前使用@org.springframework.kafka.annotation.KafkaListener这个注解的时候，是在yml文件中配置，然后使用@KafkaListener(topics = {\"${kafka.topic.a2b.name}\"})，这样去单独监听某一个topic，生产者也固定在代码里定义变量读取配置文件。昨天改了个需求，希望以后通过配置文件去动态配置生产者和消费者的topic（不知道个数和topic名字），而不需要改代码。   \n<!--more-->\n## 一、踩坑\n&emsp;刚开始的时候，由于考虑不充分（没有考虑到topic个数未知），想到@KafkaListener注解中的topics本身就是个字符串数组，于是想通过传入变量的形式。产生了以下两种方法：  \n### 1.传入变量方法一\n&emsp; 使用@Value注解提取配置文件中相关配置，@KafkaListener中传入变量\n```\n    public static String[] topicArr;\n    @Value(\"${kafka.bootstrap.servers}\")\n    public void setTopicArr(String[] value){\n        String topicArr = value;\n    }\n    @KafkaListener(topics= topicArr)\n```\nemmmm。。。结果可想而知，不行。\n### 2.传入变量方法二\n&emsp;还是传入变量，不过这次写了个动态配置的代码\n```\n    注解里这么写\n    @KafkaListener(topics = \"${topicName1}\",\"${topicName2}\",\"${topicName3}\")\n    提前将yml文件里添加\n    topics: topicName1,topicName2,topicName3\n    然后加载进来\n    @Value(\"${kafka.topics}\")\n    public void setTopics(String value){\n        topics = value;\n    }\n    动态配置代码：\n    @Configuration\n    public class KafkaTopicConfiguration implements InitializingBean {\n        @Autowired\n        private KafkaConfig kafkaconfig;\n        @Override\n        public void afterPropertiesSet() throws Exception {\n            String[] topicArr = kafkaconfig.split(\",\");\n            int i = 1;\n            for(String topic : topicArr){\n                String topicName = \"topicName\"+i;\n                System.setProperty(topicName, topic);\n            }\n        }\n    }\n```\n相比方法一，可行。但是未知topic数量呢。GG。\n### 3.不用注解\n&emsp;百度找到几个老哥的动态获取并创建topic的方法\n```\nhttps://www.cnblogs.com/gaoyawei/p/7723974.html\nhttps://www.cnblogs.com/huxi2b/p/7040617.html\nhttps://blog.csdn.net/qq_27232757/article/details/78970830\n```\n写了几版，各种各样的问题，还是我太菜。就想再看看有没有别的简单点的解决办法，没有了再回来搞这个。\n### 4.正则匹配topic\n&emsp;这期间又找到一个使用正则匹配topic的。直接贴[链接](https://www.jianshu.com/p/4c422a6a6c7a)。\n```\n@KafkaListener(topicPattern = \"showcase.*\")\n这里使用正则匹配topic，其中【*】之前得加上【.】才能匹配到。\n```\n中间模仿写了一版使用正则匹配的，其实也可以糊弄实现需求，除了topic取名的时候一定得规范以外，还得考虑到如果不想用某个topic了又得想怎么去避免他。  \n这种方法不太严谨，继续踩坑吧。\n\n## 二、问题解决\n&emsp;用蹩脚的英语google了一下，嗯?好多老哥们也是用的以上差不多的方法。然而最后在某个老哥github的[issues](https://github.com/spring-projects/spring-kafka/issues/361)中看到了解决办法。老哥的需求跟我差不多，感谢大佬,贴上最终问题解决方案。  \n### 1.kafka消费者监听配置\n```\n还是注解的形式\n@KafkaListener(topics = \"#{'${kafka.listener_topics}'.split(',')}\")\n```\n读取yml文件中kafka.listener_topics的参数，然后根据“,”去split,得到一个topics数组。  \n这么做就可以根据配置文件动态的去监听topic。\n\n### 2.yml配置文件\n```\n只列出topic相关部分（mqTypes是我用来判断使用哪个topic发送的）\n    kafka:\n      listener_topics: kafka-topic-a2b,kafka-topic-c2b\n      consume:\n        topic:\n          - name: kafka-topic-a2b\n            partitions: 12\n            replication_factor: 2\n          - name: kafka-topic-c2b\n            partitions: 12\n            replication_factor: 2\n      product:\n        topic:\n          - name: kafka-topic-b2a\n            partitions: 12\n            replication_factor: 2\n            mqTypes: type1\n          - name: kafka-topic-b2c\n            partitions: 12\n            replication_factor: 2\n            mqTypes: type1\n```\n### 3.yml参数解析\n这里我将kafka的topic相关加载到bean中处理。  \n创建KafkaConsumerBean和KafkaProducerBean分别用来存储yml中生产者和消费者的topic相关参数\n```\n//KafkaConsumerBean\n@Component\n@ConfigurationProperties(prefix = \"kafka.consume\")\npublic class KafkaConsumerBean {\n    private List<Map<String,String>> topic;\n    public void setTopic(List<Map<String, String>> topic) {\n        this.topic = topic;\n    }\n    public List<Map<String, String>> getTopic() {\n        return topic;\n    }\n}\n\n//KafkaProducerBean\n@Component\n@ConfigurationProperties(prefix = \"kafka.product\")\npublic class KafkaProducerBean {\n    private List<Map<String,String>> topic;\n    public void setTopic(List<Map<String, String>> topic) {\n        this.topic = topic;\n    }\n\n    private Map<String,String> mqType2NameMap = new HashMap<String,String>();\n    public List<Map<String, String>> getTopic() {\n        return topic;\n    }\n\n    public String getTopic(String mqType){\n        String name = mqType2NameMap.get(mqType);\n        if(name != null){\n            return name;\n        }else{\n            for(Map<String,String> topicProperty : topic){\n                if (topicProperty.get(\"mqTypes\").indexOf(mqType) >= 0){\n                    name = topicProperty.get(\"name\");\n                    mqType2NameMap.put(mqType,name);\n                    return name;\n                }\n            }\n            return null;\n        }\n\n    }\n}\n\n```\n### 4.创建topic\n```\n    List<Map<String,String>> producerTopicList = kafkaProducerBean.getTopic();\n    for (Map<String,String> topicProperty : producerTopicList){\n        KafkaClient.createTopic(topicProperty.get(\"name\"),Integer.parseInt(topicProperty.get(\"partitions\")),Integer.parseInt(topicProperty.get(\"replication_factor\")));\n    }\n    List<Map<String,String>> consumerTopicList = kafkaConsumerBean.getTopic();\n    for (Map<String,String> topicProperty : consumerTopicList){\n        KafkaClient.createTopic(topicProperty.get(\"name\"),Integer.parseInt(topicProperty.get(\"partitions\")),Integer.parseInt(topicProperty.get(\"replication_factor\")));\n    }\n```\n## 三、总结\n&emsp;上面解决问题的方法关键在于\n```\n@KafkaListener(topics = \"#{'${kafka.listener_topics}'.split(',')}\")\n```\n@KafkaListener这个注解会去读取spring的yml配置文件中\n```\nkafka:\n      listener_topics: kafka-topic-a2b,kafka-topic-c2b\n```\n这块listener_topics配置信息，然后通过','分割成topic数组，KafkaListener注解中的 topics 参数，本身就是个数组，如下\n```\n//\n// Source code recreated from a .class file by IntelliJ IDEA\n// (powered by Fernflower decompiler)\n//\n\npackage org.springframework.kafka.annotation;\n\nimport java.lang.annotation.Documented;\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Repeatable;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\nimport org.springframework.messaging.handler.annotation.MessageMapping;\n\n@Target({ElementType.TYPE, ElementType.METHOD, ElementType.ANNOTATION_TYPE})\n@Retention(RetentionPolicy.RUNTIME)\n@MessageMapping\n@Documented\n@Repeatable(KafkaListeners.class)\npublic @interface KafkaListener {\n    String id() default \"\";\n\n    String containerFactory() default \"\";\n\n    String[] topics() default {};\n\n    String topicPattern() default \"\";\n\n    TopicPartition[] topicPartitions() default {};\n\n    String group() default \"\";\n}\n```\n&emsp;结合我之前的kafka文章，应该是可以拼出一套成型的。","slug":"kafka动态配置topic","published":1,"updated":"2019-09-18T13:11:36.916Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0q92d0h000a7guo9aynuu28","content":"<p>&emsp;之前使用@org.springframework.kafka.annotation.KafkaListener这个注解的时候，是在yml文件中配置，然后使用@KafkaListener(topics = {“${kafka.topic.a2b.name}”})，这样去单独监听某一个topic，生产者也固定在代码里定义变量读取配置文件。昨天改了个需求，希望以后通过配置文件去动态配置生产者和消费者的topic（不知道个数和topic名字），而不需要改代码。<br><a id=\"more\"></a></p>\n<h2 id=\"一、踩坑\"><a href=\"#一、踩坑\" class=\"headerlink\" title=\"一、踩坑\"></a>一、踩坑</h2><p>&emsp;刚开始的时候，由于考虑不充分（没有考虑到topic个数未知），想到@KafkaListener注解中的topics本身就是个字符串数组，于是想通过传入变量的形式。产生了以下两种方法：  </p>\n<h3 id=\"1-传入变量方法一\"><a href=\"#1-传入变量方法一\" class=\"headerlink\" title=\"1.传入变量方法一\"></a>1.传入变量方法一</h3><p>&emsp; 使用@Value注解提取配置文件中相关配置，@KafkaListener中传入变量<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static String[] topicArr;</span><br><span class=\"line\">@Value(&quot;$&#123;kafka.bootstrap.servers&#125;&quot;)</span><br><span class=\"line\">public void setTopicArr(String[] value)&#123;</span><br><span class=\"line\">    String topicArr = value;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">@KafkaListener(topics= topicArr)</span><br></pre></td></tr></table></figure></p>\n<p>emmmm。。。结果可想而知，不行。</p>\n<h3 id=\"2-传入变量方法二\"><a href=\"#2-传入变量方法二\" class=\"headerlink\" title=\"2.传入变量方法二\"></a>2.传入变量方法二</h3><p>&emsp;还是传入变量，不过这次写了个动态配置的代码<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">注解里这么写</span><br><span class=\"line\">@KafkaListener(topics = &quot;$&#123;topicName1&#125;&quot;,&quot;$&#123;topicName2&#125;&quot;,&quot;$&#123;topicName3&#125;&quot;)</span><br><span class=\"line\">提前将yml文件里添加</span><br><span class=\"line\">topics: topicName1,topicName2,topicName3</span><br><span class=\"line\">然后加载进来</span><br><span class=\"line\">@Value(&quot;$&#123;kafka.topics&#125;&quot;)</span><br><span class=\"line\">public void setTopics(String value)&#123;</span><br><span class=\"line\">    topics = value;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">动态配置代码：</span><br><span class=\"line\">@Configuration</span><br><span class=\"line\">public class KafkaTopicConfiguration implements InitializingBean &#123;</span><br><span class=\"line\">    @Autowired</span><br><span class=\"line\">    private KafkaConfig kafkaconfig;</span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public void afterPropertiesSet() throws Exception &#123;</span><br><span class=\"line\">        String[] topicArr = kafkaconfig.split(&quot;,&quot;);</span><br><span class=\"line\">        int i = 1;</span><br><span class=\"line\">        for(String topic : topicArr)&#123;</span><br><span class=\"line\">            String topicName = &quot;topicName&quot;+i;</span><br><span class=\"line\">            System.setProperty(topicName, topic);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>相比方法一，可行。但是未知topic数量呢。GG。</p>\n<h3 id=\"3-不用注解\"><a href=\"#3-不用注解\" class=\"headerlink\" title=\"3.不用注解\"></a>3.不用注解</h3><p>&emsp;百度找到几个老哥的动态获取并创建topic的方法<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">https://www.cnblogs.com/gaoyawei/p/7723974.html</span><br><span class=\"line\">https://www.cnblogs.com/huxi2b/p/7040617.html</span><br><span class=\"line\">https://blog.csdn.net/qq_27232757/article/details/78970830</span><br></pre></td></tr></table></figure></p>\n<p>写了几版，各种各样的问题，还是我太菜。就想再看看有没有别的简单点的解决办法，没有了再回来搞这个。</p>\n<h3 id=\"4-正则匹配topic\"><a href=\"#4-正则匹配topic\" class=\"headerlink\" title=\"4.正则匹配topic\"></a>4.正则匹配topic</h3><p>&emsp;这期间又找到一个使用正则匹配topic的。直接贴<a href=\"https://www.jianshu.com/p/4c422a6a6c7a\" target=\"_blank\" rel=\"noopener\">链接</a>。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@KafkaListener(topicPattern = &quot;showcase.*&quot;)</span><br><span class=\"line\">这里使用正则匹配topic，其中【*】之前得加上【.】才能匹配到。</span><br></pre></td></tr></table></figure></p>\n<p>中间模仿写了一版使用正则匹配的，其实也可以糊弄实现需求，除了topic取名的时候一定得规范以外，还得考虑到如果不想用某个topic了又得想怎么去避免他。<br>这种方法不太严谨，继续踩坑吧。</p>\n<h2 id=\"二、问题解决\"><a href=\"#二、问题解决\" class=\"headerlink\" title=\"二、问题解决\"></a>二、问题解决</h2><p>&emsp;用蹩脚的英语google了一下，嗯?好多老哥们也是用的以上差不多的方法。然而最后在某个老哥github的<a href=\"https://github.com/spring-projects/spring-kafka/issues/361\" target=\"_blank\" rel=\"noopener\">issues</a>中看到了解决办法。老哥的需求跟我差不多，感谢大佬,贴上最终问题解决方案。  </p>\n<h3 id=\"1-kafka消费者监听配置\"><a href=\"#1-kafka消费者监听配置\" class=\"headerlink\" title=\"1.kafka消费者监听配置\"></a>1.kafka消费者监听配置</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">还是注解的形式</span><br><span class=\"line\">@KafkaListener(topics = &quot;#&#123;&apos;$&#123;kafka.listener_topics&#125;&apos;.split(&apos;,&apos;)&#125;&quot;)</span><br></pre></td></tr></table></figure>\n<p>读取yml文件中kafka.listener_topics的参数，然后根据“,”去split,得到一个topics数组。<br>这么做就可以根据配置文件动态的去监听topic。</p>\n<h3 id=\"2-yml配置文件\"><a href=\"#2-yml配置文件\" class=\"headerlink\" title=\"2.yml配置文件\"></a>2.yml配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">只列出topic相关部分（mqTypes是我用来判断使用哪个topic发送的）</span><br><span class=\"line\">    kafka:</span><br><span class=\"line\">      listener_topics: kafka-topic-a2b,kafka-topic-c2b</span><br><span class=\"line\">      consume:</span><br><span class=\"line\">        topic:</span><br><span class=\"line\">          - name: kafka-topic-a2b</span><br><span class=\"line\">            partitions: 12</span><br><span class=\"line\">            replication_factor: 2</span><br><span class=\"line\">          - name: kafka-topic-c2b</span><br><span class=\"line\">            partitions: 12</span><br><span class=\"line\">            replication_factor: 2</span><br><span class=\"line\">      product:</span><br><span class=\"line\">        topic:</span><br><span class=\"line\">          - name: kafka-topic-b2a</span><br><span class=\"line\">            partitions: 12</span><br><span class=\"line\">            replication_factor: 2</span><br><span class=\"line\">            mqTypes: type1</span><br><span class=\"line\">          - name: kafka-topic-b2c</span><br><span class=\"line\">            partitions: 12</span><br><span class=\"line\">            replication_factor: 2</span><br><span class=\"line\">            mqTypes: type1</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-yml参数解析\"><a href=\"#3-yml参数解析\" class=\"headerlink\" title=\"3.yml参数解析\"></a>3.yml参数解析</h3><p>这里我将kafka的topic相关加载到bean中处理。<br>创建KafkaConsumerBean和KafkaProducerBean分别用来存储yml中生产者和消费者的topic相关参数<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//KafkaConsumerBean</span><br><span class=\"line\">@Component</span><br><span class=\"line\">@ConfigurationProperties(prefix = &quot;kafka.consume&quot;)</span><br><span class=\"line\">public class KafkaConsumerBean &#123;</span><br><span class=\"line\">    private List&lt;Map&lt;String,String&gt;&gt; topic;</span><br><span class=\"line\">    public void setTopic(List&lt;Map&lt;String, String&gt;&gt; topic) &#123;</span><br><span class=\"line\">        this.topic = topic;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    public List&lt;Map&lt;String, String&gt;&gt; getTopic() &#123;</span><br><span class=\"line\">        return topic;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">//KafkaProducerBean</span><br><span class=\"line\">@Component</span><br><span class=\"line\">@ConfigurationProperties(prefix = &quot;kafka.product&quot;)</span><br><span class=\"line\">public class KafkaProducerBean &#123;</span><br><span class=\"line\">    private List&lt;Map&lt;String,String&gt;&gt; topic;</span><br><span class=\"line\">    public void setTopic(List&lt;Map&lt;String, String&gt;&gt; topic) &#123;</span><br><span class=\"line\">        this.topic = topic;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    private Map&lt;String,String&gt; mqType2NameMap = new HashMap&lt;String,String&gt;();</span><br><span class=\"line\">    public List&lt;Map&lt;String, String&gt;&gt; getTopic() &#123;</span><br><span class=\"line\">        return topic;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public String getTopic(String mqType)&#123;</span><br><span class=\"line\">        String name = mqType2NameMap.get(mqType);</span><br><span class=\"line\">        if(name != null)&#123;</span><br><span class=\"line\">            return name;</span><br><span class=\"line\">        &#125;else&#123;</span><br><span class=\"line\">            for(Map&lt;String,String&gt; topicProperty : topic)&#123;</span><br><span class=\"line\">                if (topicProperty.get(&quot;mqTypes&quot;).indexOf(mqType) &gt;= 0)&#123;</span><br><span class=\"line\">                    name = topicProperty.get(&quot;name&quot;);</span><br><span class=\"line\">                    mqType2NameMap.put(mqType,name);</span><br><span class=\"line\">                    return name;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            return null;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-创建topic\"><a href=\"#4-创建topic\" class=\"headerlink\" title=\"4.创建topic\"></a>4.创建topic</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&lt;Map&lt;String,String&gt;&gt; producerTopicList = kafkaProducerBean.getTopic();</span><br><span class=\"line\">for (Map&lt;String,String&gt; topicProperty : producerTopicList)&#123;</span><br><span class=\"line\">    KafkaClient.createTopic(topicProperty.get(&quot;name&quot;),Integer.parseInt(topicProperty.get(&quot;partitions&quot;)),Integer.parseInt(topicProperty.get(&quot;replication_factor&quot;)));</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">List&lt;Map&lt;String,String&gt;&gt; consumerTopicList = kafkaConsumerBean.getTopic();</span><br><span class=\"line\">for (Map&lt;String,String&gt; topicProperty : consumerTopicList)&#123;</span><br><span class=\"line\">    KafkaClient.createTopic(topicProperty.get(&quot;name&quot;),Integer.parseInt(topicProperty.get(&quot;partitions&quot;)),Integer.parseInt(topicProperty.get(&quot;replication_factor&quot;)));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"三、总结\"><a href=\"#三、总结\" class=\"headerlink\" title=\"三、总结\"></a>三、总结</h2><p>&emsp;上面解决问题的方法关键在于<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@KafkaListener(topics = &quot;#&#123;&apos;$&#123;kafka.listener_topics&#125;&apos;.split(&apos;,&apos;)&#125;&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>@KafkaListener这个注解会去读取spring的yml配置文件中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kafka:</span><br><span class=\"line\">      listener_topics: kafka-topic-a2b,kafka-topic-c2b</span><br></pre></td></tr></table></figure></p>\n<p>这块listener_topics配置信息，然后通过’,’分割成topic数组，KafkaListener注解中的 topics 参数，本身就是个数组，如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//</span><br><span class=\"line\">// Source code recreated from a .class file by IntelliJ IDEA</span><br><span class=\"line\">// (powered by Fernflower decompiler)</span><br><span class=\"line\">//</span><br><span class=\"line\"></span><br><span class=\"line\">package org.springframework.kafka.annotation;</span><br><span class=\"line\"></span><br><span class=\"line\">import java.lang.annotation.Documented;</span><br><span class=\"line\">import java.lang.annotation.ElementType;</span><br><span class=\"line\">import java.lang.annotation.Repeatable;</span><br><span class=\"line\">import java.lang.annotation.Retention;</span><br><span class=\"line\">import java.lang.annotation.RetentionPolicy;</span><br><span class=\"line\">import java.lang.annotation.Target;</span><br><span class=\"line\">import org.springframework.messaging.handler.annotation.MessageMapping;</span><br><span class=\"line\"></span><br><span class=\"line\">@Target(&#123;ElementType.TYPE, ElementType.METHOD, ElementType.ANNOTATION_TYPE&#125;)</span><br><span class=\"line\">@Retention(RetentionPolicy.RUNTIME)</span><br><span class=\"line\">@MessageMapping</span><br><span class=\"line\">@Documented</span><br><span class=\"line\">@Repeatable(KafkaListeners.class)</span><br><span class=\"line\">public @interface KafkaListener &#123;</span><br><span class=\"line\">    String id() default &quot;&quot;;</span><br><span class=\"line\"></span><br><span class=\"line\">    String containerFactory() default &quot;&quot;;</span><br><span class=\"line\"></span><br><span class=\"line\">    String[] topics() default &#123;&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    String topicPattern() default &quot;&quot;;</span><br><span class=\"line\"></span><br><span class=\"line\">    TopicPartition[] topicPartitions() default &#123;&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    String group() default &quot;&quot;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;结合我之前的kafka文章，应该是可以拼出一套成型的。</p>\n","site":{"data":{}},"excerpt":"<p>&emsp;之前使用@org.springframework.kafka.annotation.KafkaListener这个注解的时候，是在yml文件中配置，然后使用@KafkaListener(topics = {“${kafka.topic.a2b.name}”})，这样去单独监听某一个topic，生产者也固定在代码里定义变量读取配置文件。昨天改了个需求，希望以后通过配置文件去动态配置生产者和消费者的topic（不知道个数和topic名字），而不需要改代码。<br>","more":"</p>\n<h2 id=\"一、踩坑\"><a href=\"#一、踩坑\" class=\"headerlink\" title=\"一、踩坑\"></a>一、踩坑</h2><p>&emsp;刚开始的时候，由于考虑不充分（没有考虑到topic个数未知），想到@KafkaListener注解中的topics本身就是个字符串数组，于是想通过传入变量的形式。产生了以下两种方法：  </p>\n<h3 id=\"1-传入变量方法一\"><a href=\"#1-传入变量方法一\" class=\"headerlink\" title=\"1.传入变量方法一\"></a>1.传入变量方法一</h3><p>&emsp; 使用@Value注解提取配置文件中相关配置，@KafkaListener中传入变量<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static String[] topicArr;</span><br><span class=\"line\">@Value(&quot;$&#123;kafka.bootstrap.servers&#125;&quot;)</span><br><span class=\"line\">public void setTopicArr(String[] value)&#123;</span><br><span class=\"line\">    String topicArr = value;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">@KafkaListener(topics= topicArr)</span><br></pre></td></tr></table></figure></p>\n<p>emmmm。。。结果可想而知，不行。</p>\n<h3 id=\"2-传入变量方法二\"><a href=\"#2-传入变量方法二\" class=\"headerlink\" title=\"2.传入变量方法二\"></a>2.传入变量方法二</h3><p>&emsp;还是传入变量，不过这次写了个动态配置的代码<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">注解里这么写</span><br><span class=\"line\">@KafkaListener(topics = &quot;$&#123;topicName1&#125;&quot;,&quot;$&#123;topicName2&#125;&quot;,&quot;$&#123;topicName3&#125;&quot;)</span><br><span class=\"line\">提前将yml文件里添加</span><br><span class=\"line\">topics: topicName1,topicName2,topicName3</span><br><span class=\"line\">然后加载进来</span><br><span class=\"line\">@Value(&quot;$&#123;kafka.topics&#125;&quot;)</span><br><span class=\"line\">public void setTopics(String value)&#123;</span><br><span class=\"line\">    topics = value;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">动态配置代码：</span><br><span class=\"line\">@Configuration</span><br><span class=\"line\">public class KafkaTopicConfiguration implements InitializingBean &#123;</span><br><span class=\"line\">    @Autowired</span><br><span class=\"line\">    private KafkaConfig kafkaconfig;</span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public void afterPropertiesSet() throws Exception &#123;</span><br><span class=\"line\">        String[] topicArr = kafkaconfig.split(&quot;,&quot;);</span><br><span class=\"line\">        int i = 1;</span><br><span class=\"line\">        for(String topic : topicArr)&#123;</span><br><span class=\"line\">            String topicName = &quot;topicName&quot;+i;</span><br><span class=\"line\">            System.setProperty(topicName, topic);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>相比方法一，可行。但是未知topic数量呢。GG。</p>\n<h3 id=\"3-不用注解\"><a href=\"#3-不用注解\" class=\"headerlink\" title=\"3.不用注解\"></a>3.不用注解</h3><p>&emsp;百度找到几个老哥的动态获取并创建topic的方法<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">https://www.cnblogs.com/gaoyawei/p/7723974.html</span><br><span class=\"line\">https://www.cnblogs.com/huxi2b/p/7040617.html</span><br><span class=\"line\">https://blog.csdn.net/qq_27232757/article/details/78970830</span><br></pre></td></tr></table></figure></p>\n<p>写了几版，各种各样的问题，还是我太菜。就想再看看有没有别的简单点的解决办法，没有了再回来搞这个。</p>\n<h3 id=\"4-正则匹配topic\"><a href=\"#4-正则匹配topic\" class=\"headerlink\" title=\"4.正则匹配topic\"></a>4.正则匹配topic</h3><p>&emsp;这期间又找到一个使用正则匹配topic的。直接贴<a href=\"https://www.jianshu.com/p/4c422a6a6c7a\" target=\"_blank\" rel=\"noopener\">链接</a>。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@KafkaListener(topicPattern = &quot;showcase.*&quot;)</span><br><span class=\"line\">这里使用正则匹配topic，其中【*】之前得加上【.】才能匹配到。</span><br></pre></td></tr></table></figure></p>\n<p>中间模仿写了一版使用正则匹配的，其实也可以糊弄实现需求，除了topic取名的时候一定得规范以外，还得考虑到如果不想用某个topic了又得想怎么去避免他。<br>这种方法不太严谨，继续踩坑吧。</p>\n<h2 id=\"二、问题解决\"><a href=\"#二、问题解决\" class=\"headerlink\" title=\"二、问题解决\"></a>二、问题解决</h2><p>&emsp;用蹩脚的英语google了一下，嗯?好多老哥们也是用的以上差不多的方法。然而最后在某个老哥github的<a href=\"https://github.com/spring-projects/spring-kafka/issues/361\" target=\"_blank\" rel=\"noopener\">issues</a>中看到了解决办法。老哥的需求跟我差不多，感谢大佬,贴上最终问题解决方案。  </p>\n<h3 id=\"1-kafka消费者监听配置\"><a href=\"#1-kafka消费者监听配置\" class=\"headerlink\" title=\"1.kafka消费者监听配置\"></a>1.kafka消费者监听配置</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">还是注解的形式</span><br><span class=\"line\">@KafkaListener(topics = &quot;#&#123;&apos;$&#123;kafka.listener_topics&#125;&apos;.split(&apos;,&apos;)&#125;&quot;)</span><br></pre></td></tr></table></figure>\n<p>读取yml文件中kafka.listener_topics的参数，然后根据“,”去split,得到一个topics数组。<br>这么做就可以根据配置文件动态的去监听topic。</p>\n<h3 id=\"2-yml配置文件\"><a href=\"#2-yml配置文件\" class=\"headerlink\" title=\"2.yml配置文件\"></a>2.yml配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">只列出topic相关部分（mqTypes是我用来判断使用哪个topic发送的）</span><br><span class=\"line\">    kafka:</span><br><span class=\"line\">      listener_topics: kafka-topic-a2b,kafka-topic-c2b</span><br><span class=\"line\">      consume:</span><br><span class=\"line\">        topic:</span><br><span class=\"line\">          - name: kafka-topic-a2b</span><br><span class=\"line\">            partitions: 12</span><br><span class=\"line\">            replication_factor: 2</span><br><span class=\"line\">          - name: kafka-topic-c2b</span><br><span class=\"line\">            partitions: 12</span><br><span class=\"line\">            replication_factor: 2</span><br><span class=\"line\">      product:</span><br><span class=\"line\">        topic:</span><br><span class=\"line\">          - name: kafka-topic-b2a</span><br><span class=\"line\">            partitions: 12</span><br><span class=\"line\">            replication_factor: 2</span><br><span class=\"line\">            mqTypes: type1</span><br><span class=\"line\">          - name: kafka-topic-b2c</span><br><span class=\"line\">            partitions: 12</span><br><span class=\"line\">            replication_factor: 2</span><br><span class=\"line\">            mqTypes: type1</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-yml参数解析\"><a href=\"#3-yml参数解析\" class=\"headerlink\" title=\"3.yml参数解析\"></a>3.yml参数解析</h3><p>这里我将kafka的topic相关加载到bean中处理。<br>创建KafkaConsumerBean和KafkaProducerBean分别用来存储yml中生产者和消费者的topic相关参数<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//KafkaConsumerBean</span><br><span class=\"line\">@Component</span><br><span class=\"line\">@ConfigurationProperties(prefix = &quot;kafka.consume&quot;)</span><br><span class=\"line\">public class KafkaConsumerBean &#123;</span><br><span class=\"line\">    private List&lt;Map&lt;String,String&gt;&gt; topic;</span><br><span class=\"line\">    public void setTopic(List&lt;Map&lt;String, String&gt;&gt; topic) &#123;</span><br><span class=\"line\">        this.topic = topic;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    public List&lt;Map&lt;String, String&gt;&gt; getTopic() &#123;</span><br><span class=\"line\">        return topic;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">//KafkaProducerBean</span><br><span class=\"line\">@Component</span><br><span class=\"line\">@ConfigurationProperties(prefix = &quot;kafka.product&quot;)</span><br><span class=\"line\">public class KafkaProducerBean &#123;</span><br><span class=\"line\">    private List&lt;Map&lt;String,String&gt;&gt; topic;</span><br><span class=\"line\">    public void setTopic(List&lt;Map&lt;String, String&gt;&gt; topic) &#123;</span><br><span class=\"line\">        this.topic = topic;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    private Map&lt;String,String&gt; mqType2NameMap = new HashMap&lt;String,String&gt;();</span><br><span class=\"line\">    public List&lt;Map&lt;String, String&gt;&gt; getTopic() &#123;</span><br><span class=\"line\">        return topic;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public String getTopic(String mqType)&#123;</span><br><span class=\"line\">        String name = mqType2NameMap.get(mqType);</span><br><span class=\"line\">        if(name != null)&#123;</span><br><span class=\"line\">            return name;</span><br><span class=\"line\">        &#125;else&#123;</span><br><span class=\"line\">            for(Map&lt;String,String&gt; topicProperty : topic)&#123;</span><br><span class=\"line\">                if (topicProperty.get(&quot;mqTypes&quot;).indexOf(mqType) &gt;= 0)&#123;</span><br><span class=\"line\">                    name = topicProperty.get(&quot;name&quot;);</span><br><span class=\"line\">                    mqType2NameMap.put(mqType,name);</span><br><span class=\"line\">                    return name;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            return null;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-创建topic\"><a href=\"#4-创建topic\" class=\"headerlink\" title=\"4.创建topic\"></a>4.创建topic</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&lt;Map&lt;String,String&gt;&gt; producerTopicList = kafkaProducerBean.getTopic();</span><br><span class=\"line\">for (Map&lt;String,String&gt; topicProperty : producerTopicList)&#123;</span><br><span class=\"line\">    KafkaClient.createTopic(topicProperty.get(&quot;name&quot;),Integer.parseInt(topicProperty.get(&quot;partitions&quot;)),Integer.parseInt(topicProperty.get(&quot;replication_factor&quot;)));</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">List&lt;Map&lt;String,String&gt;&gt; consumerTopicList = kafkaConsumerBean.getTopic();</span><br><span class=\"line\">for (Map&lt;String,String&gt; topicProperty : consumerTopicList)&#123;</span><br><span class=\"line\">    KafkaClient.createTopic(topicProperty.get(&quot;name&quot;),Integer.parseInt(topicProperty.get(&quot;partitions&quot;)),Integer.parseInt(topicProperty.get(&quot;replication_factor&quot;)));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"三、总结\"><a href=\"#三、总结\" class=\"headerlink\" title=\"三、总结\"></a>三、总结</h2><p>&emsp;上面解决问题的方法关键在于<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@KafkaListener(topics = &quot;#&#123;&apos;$&#123;kafka.listener_topics&#125;&apos;.split(&apos;,&apos;)&#125;&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>@KafkaListener这个注解会去读取spring的yml配置文件中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kafka:</span><br><span class=\"line\">      listener_topics: kafka-topic-a2b,kafka-topic-c2b</span><br></pre></td></tr></table></figure></p>\n<p>这块listener_topics配置信息，然后通过’,’分割成topic数组，KafkaListener注解中的 topics 参数，本身就是个数组，如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//</span><br><span class=\"line\">// Source code recreated from a .class file by IntelliJ IDEA</span><br><span class=\"line\">// (powered by Fernflower decompiler)</span><br><span class=\"line\">//</span><br><span class=\"line\"></span><br><span class=\"line\">package org.springframework.kafka.annotation;</span><br><span class=\"line\"></span><br><span class=\"line\">import java.lang.annotation.Documented;</span><br><span class=\"line\">import java.lang.annotation.ElementType;</span><br><span class=\"line\">import java.lang.annotation.Repeatable;</span><br><span class=\"line\">import java.lang.annotation.Retention;</span><br><span class=\"line\">import java.lang.annotation.RetentionPolicy;</span><br><span class=\"line\">import java.lang.annotation.Target;</span><br><span class=\"line\">import org.springframework.messaging.handler.annotation.MessageMapping;</span><br><span class=\"line\"></span><br><span class=\"line\">@Target(&#123;ElementType.TYPE, ElementType.METHOD, ElementType.ANNOTATION_TYPE&#125;)</span><br><span class=\"line\">@Retention(RetentionPolicy.RUNTIME)</span><br><span class=\"line\">@MessageMapping</span><br><span class=\"line\">@Documented</span><br><span class=\"line\">@Repeatable(KafkaListeners.class)</span><br><span class=\"line\">public @interface KafkaListener &#123;</span><br><span class=\"line\">    String id() default &quot;&quot;;</span><br><span class=\"line\"></span><br><span class=\"line\">    String containerFactory() default &quot;&quot;;</span><br><span class=\"line\"></span><br><span class=\"line\">    String[] topics() default &#123;&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    String topicPattern() default &quot;&quot;;</span><br><span class=\"line\"></span><br><span class=\"line\">    TopicPartition[] topicPartitions() default &#123;&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    String group() default &quot;&quot;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;结合我之前的kafka文章，应该是可以拼出一套成型的。</p>"},{"title":"log4j与hadoop的简单结合","abbrlink":"b3d37a88","date":"2018-12-25T07:00:48.000Z","_content":"&emsp;最近使用了一种数据存储的方法，就是使用log4j的logback将数据进行保存，然后将数据上传到hive表中，进行相关的数据分析操作。\n<!--more-->\n## 一、配置说明\n&emsp;不多比比，感谢大佬。[logback的使用和logback.xml详解](http://www.cnblogs.com/warking/p/5710303.html)。这篇博客写的比较详细，关于logbak的相关配置文件说明。\n## 二、提取需要的信息\n先在业务逻辑层中提取关键信息。\n这里我是简单定义一个字符串数组，将信息保存。如果有别的需求，可以自行更改提取方法。\n```\npublic static String[] getLogMessage(String a,String b,String c,String d)\n    {\n        return new String[]{a,b,c,d};\n    }\n```\n## 三、编写单独的日志打印类\n```\npublic class TestBhLogger\n{\n    private static final Logger log = LoggerFactory.getLogger(TestBhLogger.class);\n    public static void log(String[] array)\n    {\n        if ((array == null) || (array.length == 0)) {\n            return;\n        }\n        StringBuilder sb = new StringBuilder();\n        for (String str : array) {\n            sb.append(str).append(\"\\t\");\n        }\n        if (sb.length() >= 1)\n        {\n            String l = sb.substring(0, sb.length() - 1);\n            System.out.println(l);\n            log.info(l);\n        }\n    }\n}\n```\n## 四、logback-spring.xml文件配置\n### 1.将日志信息输出到控制台标准输出流\n```\n<appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\">\n    <encoder>\n        <pattern>%d{MM/dd HH:mm:ss.SSS} %-5level %logger{5} [%thread] - %msg%n</pattern>\n    </encoder>\n</appender>\n```\n### 2.配置编写打印日志类的日志回滚策略\n1)指定要打印日志的类及日志级别  \n2)将日志输出到定义目录的日志文件中  \n3)定义日志回滚策略及日志文件格式  \n```\n<appender name=\"test-log\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n    <File>目录/logs/test/event.txt</File>\n    <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n        <fileNamePattern>目录/logs/test/event-%d{yyyyMMddHH}.txt</fileNamePattern>\n    </rollingPolicy>\n    <layout class=\"ch.qos.logback.classic.PatternLayout\">\n        <Pattern>%msg%n</Pattern>\n    </layout>\n    <filter class=\"ch.qos.logback.classic.filter.LevelFilter\">\n        <level>INFO</level>\n        <onMatch>ACCEPT</onMatch>\n        <onMismatch>DENY</onMismatch>\n    </filter>\n</appender>\n<logger name=\"com.test.log.TestBhLogger\" level=\"INFO\" >\n    <appender-ref ref=\"test-log\"/>\n</logger>\n```\n### 3.完整的配置文件\n```\n<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n<configuration scan=\"false\" scanPeriod=\"60 seconds\" debug=\"false\">\n    <timestamp key=\"day\" datePattern=\"yyyyMMdd\"/>\n    <timestamp key=\"hour\" datePattern=\"yyyyMMddHH\"/>\n\n    <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <encoder>\n            <pattern>%d{MM/dd HH:mm:ss.SSS} %-5level %logger{5} [%thread] - %msg%n</pattern>\n        </encoder>\n    </appender>\n\n    <appender name=\"operatorLog\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n        <File>目录/logs/operator.log</File>\n        <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n            <fileNamePattern>目录/logs/operator-%d{yyyyMMddHH}.log</fileNamePattern>\n        </rollingPolicy>\n        <encoder>\n            <pattern>%d{MM/dd HH:mm:ss.SSS} %-5level %logger{5} [%thread] - %msg%n</pattern>\n        </encoder>\n        <filter class=\"ch.qos.logback.classic.filter.LevelFilter\">\n            <level>INFO</level>\n            <onMatch>ACCEPT</onMatch>\n            <onMismatch>DENY</onMismatch>\n        </filter>\n    </appender>\n\t\n\t<appender name=\"test-log\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n\t\t<File>目录/logs/test/event.txt</File>\n\t\t<rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n\t\t\t<fileNamePattern>目录/logs/test/event-%d{yyyyMMddHH}.txt</fileNamePattern>\n\t\t</rollingPolicy>\n\t\t<layout class=\"ch.qos.logback.classic.PatternLayout\">\n\t\t\t<Pattern>%msg%n</Pattern>\n\t\t</layout>\n\t\t<filter class=\"ch.qos.logback.classic.filter.LevelFilter\">\n\t\t\t<level>INFO</level>\n\t\t\t<onMatch>ACCEPT</onMatch>\n\t\t\t<onMismatch>DENY</onMismatch>\n\t\t</filter>\n\t</appender>\n\t<logger name=\"com.test.log.TestBhLogger\" level=\"INFO\" >\n\t\t<appender-ref ref=\"test-log\"/>\n\t</logger>\n\n\n    <root level=\"info\">\n        <appender-ref ref=\"STDOUT\"/>\n        <appender-ref ref=\"operatorLog\"/>\n    </root>\n\n</configuration>\n```\n## 五、编写激活程序\n由于日志回滚需要打印日志去激活，故编写一个根据需要日志回滚的时间间隔的定时激活程序。  \n这里我直接采用了Sping自带的定时任务注解@EnableScheduling\n```\n@Configuration\n@EnableScheduling\npublic class AliveTask\n{\n    private static final Logger log = LoggerFactory.getLogger(AliveTask.class);\n    SimpleDateFormat dataFormat_yyyyMMddHH = new SimpleDateFormat(\"yyyyMMddHH\");\n\n    @Scheduled(cron=\"0 10 0-23 * * ?\")\n    public void scheduler()\n    {\n        //大概就是这么个意思，具体代码根据不同需求与逻辑更改\n        String[] arr = getLogMessage(a,b,c,d);\n\t\tTestBhLogger.log(arr);\n    }\n}\n```\n## 六、加载日志文件到hive表\n根据不同需求获取需要的时间格式，加载到相应的hive表的相应分区中  \nHiveUtil工具类网上一堆，就不细写了\n```\npublic void loadData(){\n    try\n    {\n        Calendar cal = Calendar.getInstance();\n        cal.add(11, -1);\n        String hourId = this.dataFormat_yyyyMMddHH.format(cal.getTime());\n        String dayId = hourId.substring(0, 8);\n        String hivesql = \"LOAD DATA LOCAL INPATH '\" + Main.BASE_PATH + \"/logs/test/*\" + hourId + \"*' INTO TABLE \" + Config.getString(\"hive.database\") + \".test_bh PARTITION(day_id='\" + dayId + \"',hour_id='\" + hourId + \"')\";\n        HiveUtil.exec(hivesql);\n    }\n    catch (Exception e)\n    {\n        log.error(\"上传数据失败\" + e.getMessage(), e);\n    }\n}\n```\n---\n&emsp;至此就将需要的数据存储到hive表中了，接下来就是根据需求进行数据分析了。log4j真的强大。\n","source":"_posts/log4j与hadoop的简单结合.md","raw":"---\ntitle: log4j与hadoop的简单结合\ntags:\n  - 日志处理\n  - log4j\nabbrlink: b3d37a88\ndate: 2018-12-25 15:00:48\n---\n&emsp;最近使用了一种数据存储的方法，就是使用log4j的logback将数据进行保存，然后将数据上传到hive表中，进行相关的数据分析操作。\n<!--more-->\n## 一、配置说明\n&emsp;不多比比，感谢大佬。[logback的使用和logback.xml详解](http://www.cnblogs.com/warking/p/5710303.html)。这篇博客写的比较详细，关于logbak的相关配置文件说明。\n## 二、提取需要的信息\n先在业务逻辑层中提取关键信息。\n这里我是简单定义一个字符串数组，将信息保存。如果有别的需求，可以自行更改提取方法。\n```\npublic static String[] getLogMessage(String a,String b,String c,String d)\n    {\n        return new String[]{a,b,c,d};\n    }\n```\n## 三、编写单独的日志打印类\n```\npublic class TestBhLogger\n{\n    private static final Logger log = LoggerFactory.getLogger(TestBhLogger.class);\n    public static void log(String[] array)\n    {\n        if ((array == null) || (array.length == 0)) {\n            return;\n        }\n        StringBuilder sb = new StringBuilder();\n        for (String str : array) {\n            sb.append(str).append(\"\\t\");\n        }\n        if (sb.length() >= 1)\n        {\n            String l = sb.substring(0, sb.length() - 1);\n            System.out.println(l);\n            log.info(l);\n        }\n    }\n}\n```\n## 四、logback-spring.xml文件配置\n### 1.将日志信息输出到控制台标准输出流\n```\n<appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\">\n    <encoder>\n        <pattern>%d{MM/dd HH:mm:ss.SSS} %-5level %logger{5} [%thread] - %msg%n</pattern>\n    </encoder>\n</appender>\n```\n### 2.配置编写打印日志类的日志回滚策略\n1)指定要打印日志的类及日志级别  \n2)将日志输出到定义目录的日志文件中  \n3)定义日志回滚策略及日志文件格式  \n```\n<appender name=\"test-log\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n    <File>目录/logs/test/event.txt</File>\n    <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n        <fileNamePattern>目录/logs/test/event-%d{yyyyMMddHH}.txt</fileNamePattern>\n    </rollingPolicy>\n    <layout class=\"ch.qos.logback.classic.PatternLayout\">\n        <Pattern>%msg%n</Pattern>\n    </layout>\n    <filter class=\"ch.qos.logback.classic.filter.LevelFilter\">\n        <level>INFO</level>\n        <onMatch>ACCEPT</onMatch>\n        <onMismatch>DENY</onMismatch>\n    </filter>\n</appender>\n<logger name=\"com.test.log.TestBhLogger\" level=\"INFO\" >\n    <appender-ref ref=\"test-log\"/>\n</logger>\n```\n### 3.完整的配置文件\n```\n<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n<configuration scan=\"false\" scanPeriod=\"60 seconds\" debug=\"false\">\n    <timestamp key=\"day\" datePattern=\"yyyyMMdd\"/>\n    <timestamp key=\"hour\" datePattern=\"yyyyMMddHH\"/>\n\n    <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <encoder>\n            <pattern>%d{MM/dd HH:mm:ss.SSS} %-5level %logger{5} [%thread] - %msg%n</pattern>\n        </encoder>\n    </appender>\n\n    <appender name=\"operatorLog\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n        <File>目录/logs/operator.log</File>\n        <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n            <fileNamePattern>目录/logs/operator-%d{yyyyMMddHH}.log</fileNamePattern>\n        </rollingPolicy>\n        <encoder>\n            <pattern>%d{MM/dd HH:mm:ss.SSS} %-5level %logger{5} [%thread] - %msg%n</pattern>\n        </encoder>\n        <filter class=\"ch.qos.logback.classic.filter.LevelFilter\">\n            <level>INFO</level>\n            <onMatch>ACCEPT</onMatch>\n            <onMismatch>DENY</onMismatch>\n        </filter>\n    </appender>\n\t\n\t<appender name=\"test-log\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n\t\t<File>目录/logs/test/event.txt</File>\n\t\t<rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n\t\t\t<fileNamePattern>目录/logs/test/event-%d{yyyyMMddHH}.txt</fileNamePattern>\n\t\t</rollingPolicy>\n\t\t<layout class=\"ch.qos.logback.classic.PatternLayout\">\n\t\t\t<Pattern>%msg%n</Pattern>\n\t\t</layout>\n\t\t<filter class=\"ch.qos.logback.classic.filter.LevelFilter\">\n\t\t\t<level>INFO</level>\n\t\t\t<onMatch>ACCEPT</onMatch>\n\t\t\t<onMismatch>DENY</onMismatch>\n\t\t</filter>\n\t</appender>\n\t<logger name=\"com.test.log.TestBhLogger\" level=\"INFO\" >\n\t\t<appender-ref ref=\"test-log\"/>\n\t</logger>\n\n\n    <root level=\"info\">\n        <appender-ref ref=\"STDOUT\"/>\n        <appender-ref ref=\"operatorLog\"/>\n    </root>\n\n</configuration>\n```\n## 五、编写激活程序\n由于日志回滚需要打印日志去激活，故编写一个根据需要日志回滚的时间间隔的定时激活程序。  \n这里我直接采用了Sping自带的定时任务注解@EnableScheduling\n```\n@Configuration\n@EnableScheduling\npublic class AliveTask\n{\n    private static final Logger log = LoggerFactory.getLogger(AliveTask.class);\n    SimpleDateFormat dataFormat_yyyyMMddHH = new SimpleDateFormat(\"yyyyMMddHH\");\n\n    @Scheduled(cron=\"0 10 0-23 * * ?\")\n    public void scheduler()\n    {\n        //大概就是这么个意思，具体代码根据不同需求与逻辑更改\n        String[] arr = getLogMessage(a,b,c,d);\n\t\tTestBhLogger.log(arr);\n    }\n}\n```\n## 六、加载日志文件到hive表\n根据不同需求获取需要的时间格式，加载到相应的hive表的相应分区中  \nHiveUtil工具类网上一堆，就不细写了\n```\npublic void loadData(){\n    try\n    {\n        Calendar cal = Calendar.getInstance();\n        cal.add(11, -1);\n        String hourId = this.dataFormat_yyyyMMddHH.format(cal.getTime());\n        String dayId = hourId.substring(0, 8);\n        String hivesql = \"LOAD DATA LOCAL INPATH '\" + Main.BASE_PATH + \"/logs/test/*\" + hourId + \"*' INTO TABLE \" + Config.getString(\"hive.database\") + \".test_bh PARTITION(day_id='\" + dayId + \"',hour_id='\" + hourId + \"')\";\n        HiveUtil.exec(hivesql);\n    }\n    catch (Exception e)\n    {\n        log.error(\"上传数据失败\" + e.getMessage(), e);\n    }\n}\n```\n---\n&emsp;至此就将需要的数据存储到hive表中了，接下来就是根据需求进行数据分析了。log4j真的强大。\n","slug":"log4j与hadoop的简单结合","published":1,"updated":"2019-09-18T13:11:36.917Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0q92d0j000c7guorghwah14","content":"<p>&emsp;最近使用了一种数据存储的方法，就是使用log4j的logback将数据进行保存，然后将数据上传到hive表中，进行相关的数据分析操作。<br><a id=\"more\"></a></p>\n<h2 id=\"一、配置说明\"><a href=\"#一、配置说明\" class=\"headerlink\" title=\"一、配置说明\"></a>一、配置说明</h2><p>&emsp;不多比比，感谢大佬。<a href=\"http://www.cnblogs.com/warking/p/5710303.html\" target=\"_blank\" rel=\"noopener\">logback的使用和logback.xml详解</a>。这篇博客写的比较详细，关于logbak的相关配置文件说明。</p>\n<h2 id=\"二、提取需要的信息\"><a href=\"#二、提取需要的信息\" class=\"headerlink\" title=\"二、提取需要的信息\"></a>二、提取需要的信息</h2><p>先在业务逻辑层中提取关键信息。<br>这里我是简单定义一个字符串数组，将信息保存。如果有别的需求，可以自行更改提取方法。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static String[] getLogMessage(String a,String b,String c,String d)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        return new String[]&#123;a,b,c,d&#125;;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"三、编写单独的日志打印类\"><a href=\"#三、编写单独的日志打印类\" class=\"headerlink\" title=\"三、编写单独的日志打印类\"></a>三、编写单独的日志打印类</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class TestBhLogger</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    private static final Logger log = LoggerFactory.getLogger(TestBhLogger.class);</span><br><span class=\"line\">    public static void log(String[] array)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        if ((array == null) || (array.length == 0)) &#123;</span><br><span class=\"line\">            return;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        StringBuilder sb = new StringBuilder();</span><br><span class=\"line\">        for (String str : array) &#123;</span><br><span class=\"line\">            sb.append(str).append(&quot;\\t&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if (sb.length() &gt;= 1)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            String l = sb.substring(0, sb.length() - 1);</span><br><span class=\"line\">            System.out.println(l);</span><br><span class=\"line\">            log.info(l);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"四、logback-spring-xml文件配置\"><a href=\"#四、logback-spring-xml文件配置\" class=\"headerlink\" title=\"四、logback-spring.xml文件配置\"></a>四、logback-spring.xml文件配置</h2><h3 id=\"1-将日志信息输出到控制台标准输出流\"><a href=\"#1-将日志信息输出到控制台标准输出流\" class=\"headerlink\" title=\"1.将日志信息输出到控制台标准输出流\"></a>1.将日志信息输出到控制台标准输出流</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;</span><br><span class=\"line\">    &lt;encoder&gt;</span><br><span class=\"line\">        &lt;pattern&gt;%d&#123;MM/dd HH:mm:ss.SSS&#125; %-5level %logger&#123;5&#125; [%thread] - %msg%n&lt;/pattern&gt;</span><br><span class=\"line\">    &lt;/encoder&gt;</span><br><span class=\"line\">&lt;/appender&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-配置编写打印日志类的日志回滚策略\"><a href=\"#2-配置编写打印日志类的日志回滚策略\" class=\"headerlink\" title=\"2.配置编写打印日志类的日志回滚策略\"></a>2.配置编写打印日志类的日志回滚策略</h3><p>1)指定要打印日志的类及日志级别<br>2)将日志输出到定义目录的日志文件中<br>3)定义日志回滚策略及日志文件格式<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;appender name=&quot;test-log&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;</span><br><span class=\"line\">    &lt;File&gt;目录/logs/test/event.txt&lt;/File&gt;</span><br><span class=\"line\">    &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;</span><br><span class=\"line\">        &lt;fileNamePattern&gt;目录/logs/test/event-%d&#123;yyyyMMddHH&#125;.txt&lt;/fileNamePattern&gt;</span><br><span class=\"line\">    &lt;/rollingPolicy&gt;</span><br><span class=\"line\">    &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt;</span><br><span class=\"line\">        &lt;Pattern&gt;%msg%n&lt;/Pattern&gt;</span><br><span class=\"line\">    &lt;/layout&gt;</span><br><span class=\"line\">    &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;</span><br><span class=\"line\">        &lt;level&gt;INFO&lt;/level&gt;</span><br><span class=\"line\">        &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;</span><br><span class=\"line\">        &lt;onMismatch&gt;DENY&lt;/onMismatch&gt;</span><br><span class=\"line\">    &lt;/filter&gt;</span><br><span class=\"line\">&lt;/appender&gt;</span><br><span class=\"line\">&lt;logger name=&quot;com.test.log.TestBhLogger&quot; level=&quot;INFO&quot; &gt;</span><br><span class=\"line\">    &lt;appender-ref ref=&quot;test-log&quot;/&gt;</span><br><span class=\"line\">&lt;/logger&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-完整的配置文件\"><a href=\"#3-完整的配置文件\" class=\"headerlink\" title=\"3.完整的配置文件\"></a>3.完整的配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt;</span><br><span class=\"line\">&lt;configuration scan=&quot;false&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;false&quot;&gt;</span><br><span class=\"line\">    &lt;timestamp key=&quot;day&quot; datePattern=&quot;yyyyMMdd&quot;/&gt;</span><br><span class=\"line\">    &lt;timestamp key=&quot;hour&quot; datePattern=&quot;yyyyMMddHH&quot;/&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;</span><br><span class=\"line\">        &lt;encoder&gt;</span><br><span class=\"line\">            &lt;pattern&gt;%d&#123;MM/dd HH:mm:ss.SSS&#125; %-5level %logger&#123;5&#125; [%thread] - %msg%n&lt;/pattern&gt;</span><br><span class=\"line\">        &lt;/encoder&gt;</span><br><span class=\"line\">    &lt;/appender&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;appender name=&quot;operatorLog&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;</span><br><span class=\"line\">        &lt;File&gt;目录/logs/operator.log&lt;/File&gt;</span><br><span class=\"line\">        &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;</span><br><span class=\"line\">            &lt;fileNamePattern&gt;目录/logs/operator-%d&#123;yyyyMMddHH&#125;.log&lt;/fileNamePattern&gt;</span><br><span class=\"line\">        &lt;/rollingPolicy&gt;</span><br><span class=\"line\">        &lt;encoder&gt;</span><br><span class=\"line\">            &lt;pattern&gt;%d&#123;MM/dd HH:mm:ss.SSS&#125; %-5level %logger&#123;5&#125; [%thread] - %msg%n&lt;/pattern&gt;</span><br><span class=\"line\">        &lt;/encoder&gt;</span><br><span class=\"line\">        &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;</span><br><span class=\"line\">            &lt;level&gt;INFO&lt;/level&gt;</span><br><span class=\"line\">            &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;</span><br><span class=\"line\">            &lt;onMismatch&gt;DENY&lt;/onMismatch&gt;</span><br><span class=\"line\">        &lt;/filter&gt;</span><br><span class=\"line\">    &lt;/appender&gt;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t&lt;appender name=&quot;test-log&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;</span><br><span class=\"line\">\t\t&lt;File&gt;目录/logs/test/event.txt&lt;/File&gt;</span><br><span class=\"line\">\t\t&lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;</span><br><span class=\"line\">\t\t\t&lt;fileNamePattern&gt;目录/logs/test/event-%d&#123;yyyyMMddHH&#125;.txt&lt;/fileNamePattern&gt;</span><br><span class=\"line\">\t\t&lt;/rollingPolicy&gt;</span><br><span class=\"line\">\t\t&lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt;</span><br><span class=\"line\">\t\t\t&lt;Pattern&gt;%msg%n&lt;/Pattern&gt;</span><br><span class=\"line\">\t\t&lt;/layout&gt;</span><br><span class=\"line\">\t\t&lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;</span><br><span class=\"line\">\t\t\t&lt;level&gt;INFO&lt;/level&gt;</span><br><span class=\"line\">\t\t\t&lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;</span><br><span class=\"line\">\t\t\t&lt;onMismatch&gt;DENY&lt;/onMismatch&gt;</span><br><span class=\"line\">\t\t&lt;/filter&gt;</span><br><span class=\"line\">\t&lt;/appender&gt;</span><br><span class=\"line\">\t&lt;logger name=&quot;com.test.log.TestBhLogger&quot; level=&quot;INFO&quot; &gt;</span><br><span class=\"line\">\t\t&lt;appender-ref ref=&quot;test-log&quot;/&gt;</span><br><span class=\"line\">\t&lt;/logger&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;root level=&quot;info&quot;&gt;</span><br><span class=\"line\">        &lt;appender-ref ref=&quot;STDOUT&quot;/&gt;</span><br><span class=\"line\">        &lt;appender-ref ref=&quot;operatorLog&quot;/&gt;</span><br><span class=\"line\">    &lt;/root&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"五、编写激活程序\"><a href=\"#五、编写激活程序\" class=\"headerlink\" title=\"五、编写激活程序\"></a>五、编写激活程序</h2><p>由于日志回滚需要打印日志去激活，故编写一个根据需要日志回滚的时间间隔的定时激活程序。<br>这里我直接采用了Sping自带的定时任务注解@EnableScheduling<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Configuration</span><br><span class=\"line\">@EnableScheduling</span><br><span class=\"line\">public class AliveTask</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    private static final Logger log = LoggerFactory.getLogger(AliveTask.class);</span><br><span class=\"line\">    SimpleDateFormat dataFormat_yyyyMMddHH = new SimpleDateFormat(&quot;yyyyMMddHH&quot;);</span><br><span class=\"line\"></span><br><span class=\"line\">    @Scheduled(cron=&quot;0 10 0-23 * * ?&quot;)</span><br><span class=\"line\">    public void scheduler()</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        //大概就是这么个意思，具体代码根据不同需求与逻辑更改</span><br><span class=\"line\">        String[] arr = getLogMessage(a,b,c,d);</span><br><span class=\"line\">\t\tTestBhLogger.log(arr);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"六、加载日志文件到hive表\"><a href=\"#六、加载日志文件到hive表\" class=\"headerlink\" title=\"六、加载日志文件到hive表\"></a>六、加载日志文件到hive表</h2><p>根据不同需求获取需要的时间格式，加载到相应的hive表的相应分区中<br>HiveUtil工具类网上一堆，就不细写了<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void loadData()&#123;</span><br><span class=\"line\">    try</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        Calendar cal = Calendar.getInstance();</span><br><span class=\"line\">        cal.add(11, -1);</span><br><span class=\"line\">        String hourId = this.dataFormat_yyyyMMddHH.format(cal.getTime());</span><br><span class=\"line\">        String dayId = hourId.substring(0, 8);</span><br><span class=\"line\">        String hivesql = &quot;LOAD DATA LOCAL INPATH &apos;&quot; + Main.BASE_PATH + &quot;/logs/test/*&quot; + hourId + &quot;*&apos; INTO TABLE &quot; + Config.getString(&quot;hive.database&quot;) + &quot;.test_bh PARTITION(day_id=&apos;&quot; + dayId + &quot;&apos;,hour_id=&apos;&quot; + hourId + &quot;&apos;)&quot;;</span><br><span class=\"line\">        HiveUtil.exec(hivesql);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    catch (Exception e)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        log.error(&quot;上传数据失败&quot; + e.getMessage(), e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>&emsp;至此就将需要的数据存储到hive表中了，接下来就是根据需求进行数据分析了。log4j真的强大。</p>\n","site":{"data":{}},"excerpt":"<p>&emsp;最近使用了一种数据存储的方法，就是使用log4j的logback将数据进行保存，然后将数据上传到hive表中，进行相关的数据分析操作。<br>","more":"</p>\n<h2 id=\"一、配置说明\"><a href=\"#一、配置说明\" class=\"headerlink\" title=\"一、配置说明\"></a>一、配置说明</h2><p>&emsp;不多比比，感谢大佬。<a href=\"http://www.cnblogs.com/warking/p/5710303.html\" target=\"_blank\" rel=\"noopener\">logback的使用和logback.xml详解</a>。这篇博客写的比较详细，关于logbak的相关配置文件说明。</p>\n<h2 id=\"二、提取需要的信息\"><a href=\"#二、提取需要的信息\" class=\"headerlink\" title=\"二、提取需要的信息\"></a>二、提取需要的信息</h2><p>先在业务逻辑层中提取关键信息。<br>这里我是简单定义一个字符串数组，将信息保存。如果有别的需求，可以自行更改提取方法。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static String[] getLogMessage(String a,String b,String c,String d)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        return new String[]&#123;a,b,c,d&#125;;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"三、编写单独的日志打印类\"><a href=\"#三、编写单独的日志打印类\" class=\"headerlink\" title=\"三、编写单独的日志打印类\"></a>三、编写单独的日志打印类</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class TestBhLogger</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    private static final Logger log = LoggerFactory.getLogger(TestBhLogger.class);</span><br><span class=\"line\">    public static void log(String[] array)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        if ((array == null) || (array.length == 0)) &#123;</span><br><span class=\"line\">            return;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        StringBuilder sb = new StringBuilder();</span><br><span class=\"line\">        for (String str : array) &#123;</span><br><span class=\"line\">            sb.append(str).append(&quot;\\t&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if (sb.length() &gt;= 1)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            String l = sb.substring(0, sb.length() - 1);</span><br><span class=\"line\">            System.out.println(l);</span><br><span class=\"line\">            log.info(l);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"四、logback-spring-xml文件配置\"><a href=\"#四、logback-spring-xml文件配置\" class=\"headerlink\" title=\"四、logback-spring.xml文件配置\"></a>四、logback-spring.xml文件配置</h2><h3 id=\"1-将日志信息输出到控制台标准输出流\"><a href=\"#1-将日志信息输出到控制台标准输出流\" class=\"headerlink\" title=\"1.将日志信息输出到控制台标准输出流\"></a>1.将日志信息输出到控制台标准输出流</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;</span><br><span class=\"line\">    &lt;encoder&gt;</span><br><span class=\"line\">        &lt;pattern&gt;%d&#123;MM/dd HH:mm:ss.SSS&#125; %-5level %logger&#123;5&#125; [%thread] - %msg%n&lt;/pattern&gt;</span><br><span class=\"line\">    &lt;/encoder&gt;</span><br><span class=\"line\">&lt;/appender&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-配置编写打印日志类的日志回滚策略\"><a href=\"#2-配置编写打印日志类的日志回滚策略\" class=\"headerlink\" title=\"2.配置编写打印日志类的日志回滚策略\"></a>2.配置编写打印日志类的日志回滚策略</h3><p>1)指定要打印日志的类及日志级别<br>2)将日志输出到定义目录的日志文件中<br>3)定义日志回滚策略及日志文件格式<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;appender name=&quot;test-log&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;</span><br><span class=\"line\">    &lt;File&gt;目录/logs/test/event.txt&lt;/File&gt;</span><br><span class=\"line\">    &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;</span><br><span class=\"line\">        &lt;fileNamePattern&gt;目录/logs/test/event-%d&#123;yyyyMMddHH&#125;.txt&lt;/fileNamePattern&gt;</span><br><span class=\"line\">    &lt;/rollingPolicy&gt;</span><br><span class=\"line\">    &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt;</span><br><span class=\"line\">        &lt;Pattern&gt;%msg%n&lt;/Pattern&gt;</span><br><span class=\"line\">    &lt;/layout&gt;</span><br><span class=\"line\">    &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;</span><br><span class=\"line\">        &lt;level&gt;INFO&lt;/level&gt;</span><br><span class=\"line\">        &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;</span><br><span class=\"line\">        &lt;onMismatch&gt;DENY&lt;/onMismatch&gt;</span><br><span class=\"line\">    &lt;/filter&gt;</span><br><span class=\"line\">&lt;/appender&gt;</span><br><span class=\"line\">&lt;logger name=&quot;com.test.log.TestBhLogger&quot; level=&quot;INFO&quot; &gt;</span><br><span class=\"line\">    &lt;appender-ref ref=&quot;test-log&quot;/&gt;</span><br><span class=\"line\">&lt;/logger&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-完整的配置文件\"><a href=\"#3-完整的配置文件\" class=\"headerlink\" title=\"3.完整的配置文件\"></a>3.完整的配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt;</span><br><span class=\"line\">&lt;configuration scan=&quot;false&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;false&quot;&gt;</span><br><span class=\"line\">    &lt;timestamp key=&quot;day&quot; datePattern=&quot;yyyyMMdd&quot;/&gt;</span><br><span class=\"line\">    &lt;timestamp key=&quot;hour&quot; datePattern=&quot;yyyyMMddHH&quot;/&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;</span><br><span class=\"line\">        &lt;encoder&gt;</span><br><span class=\"line\">            &lt;pattern&gt;%d&#123;MM/dd HH:mm:ss.SSS&#125; %-5level %logger&#123;5&#125; [%thread] - %msg%n&lt;/pattern&gt;</span><br><span class=\"line\">        &lt;/encoder&gt;</span><br><span class=\"line\">    &lt;/appender&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;appender name=&quot;operatorLog&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;</span><br><span class=\"line\">        &lt;File&gt;目录/logs/operator.log&lt;/File&gt;</span><br><span class=\"line\">        &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;</span><br><span class=\"line\">            &lt;fileNamePattern&gt;目录/logs/operator-%d&#123;yyyyMMddHH&#125;.log&lt;/fileNamePattern&gt;</span><br><span class=\"line\">        &lt;/rollingPolicy&gt;</span><br><span class=\"line\">        &lt;encoder&gt;</span><br><span class=\"line\">            &lt;pattern&gt;%d&#123;MM/dd HH:mm:ss.SSS&#125; %-5level %logger&#123;5&#125; [%thread] - %msg%n&lt;/pattern&gt;</span><br><span class=\"line\">        &lt;/encoder&gt;</span><br><span class=\"line\">        &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;</span><br><span class=\"line\">            &lt;level&gt;INFO&lt;/level&gt;</span><br><span class=\"line\">            &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;</span><br><span class=\"line\">            &lt;onMismatch&gt;DENY&lt;/onMismatch&gt;</span><br><span class=\"line\">        &lt;/filter&gt;</span><br><span class=\"line\">    &lt;/appender&gt;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t&lt;appender name=&quot;test-log&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;</span><br><span class=\"line\">\t\t&lt;File&gt;目录/logs/test/event.txt&lt;/File&gt;</span><br><span class=\"line\">\t\t&lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;</span><br><span class=\"line\">\t\t\t&lt;fileNamePattern&gt;目录/logs/test/event-%d&#123;yyyyMMddHH&#125;.txt&lt;/fileNamePattern&gt;</span><br><span class=\"line\">\t\t&lt;/rollingPolicy&gt;</span><br><span class=\"line\">\t\t&lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt;</span><br><span class=\"line\">\t\t\t&lt;Pattern&gt;%msg%n&lt;/Pattern&gt;</span><br><span class=\"line\">\t\t&lt;/layout&gt;</span><br><span class=\"line\">\t\t&lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;</span><br><span class=\"line\">\t\t\t&lt;level&gt;INFO&lt;/level&gt;</span><br><span class=\"line\">\t\t\t&lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;</span><br><span class=\"line\">\t\t\t&lt;onMismatch&gt;DENY&lt;/onMismatch&gt;</span><br><span class=\"line\">\t\t&lt;/filter&gt;</span><br><span class=\"line\">\t&lt;/appender&gt;</span><br><span class=\"line\">\t&lt;logger name=&quot;com.test.log.TestBhLogger&quot; level=&quot;INFO&quot; &gt;</span><br><span class=\"line\">\t\t&lt;appender-ref ref=&quot;test-log&quot;/&gt;</span><br><span class=\"line\">\t&lt;/logger&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;root level=&quot;info&quot;&gt;</span><br><span class=\"line\">        &lt;appender-ref ref=&quot;STDOUT&quot;/&gt;</span><br><span class=\"line\">        &lt;appender-ref ref=&quot;operatorLog&quot;/&gt;</span><br><span class=\"line\">    &lt;/root&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"五、编写激活程序\"><a href=\"#五、编写激活程序\" class=\"headerlink\" title=\"五、编写激活程序\"></a>五、编写激活程序</h2><p>由于日志回滚需要打印日志去激活，故编写一个根据需要日志回滚的时间间隔的定时激活程序。<br>这里我直接采用了Sping自带的定时任务注解@EnableScheduling<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Configuration</span><br><span class=\"line\">@EnableScheduling</span><br><span class=\"line\">public class AliveTask</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    private static final Logger log = LoggerFactory.getLogger(AliveTask.class);</span><br><span class=\"line\">    SimpleDateFormat dataFormat_yyyyMMddHH = new SimpleDateFormat(&quot;yyyyMMddHH&quot;);</span><br><span class=\"line\"></span><br><span class=\"line\">    @Scheduled(cron=&quot;0 10 0-23 * * ?&quot;)</span><br><span class=\"line\">    public void scheduler()</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        //大概就是这么个意思，具体代码根据不同需求与逻辑更改</span><br><span class=\"line\">        String[] arr = getLogMessage(a,b,c,d);</span><br><span class=\"line\">\t\tTestBhLogger.log(arr);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"六、加载日志文件到hive表\"><a href=\"#六、加载日志文件到hive表\" class=\"headerlink\" title=\"六、加载日志文件到hive表\"></a>六、加载日志文件到hive表</h2><p>根据不同需求获取需要的时间格式，加载到相应的hive表的相应分区中<br>HiveUtil工具类网上一堆，就不细写了<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void loadData()&#123;</span><br><span class=\"line\">    try</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        Calendar cal = Calendar.getInstance();</span><br><span class=\"line\">        cal.add(11, -1);</span><br><span class=\"line\">        String hourId = this.dataFormat_yyyyMMddHH.format(cal.getTime());</span><br><span class=\"line\">        String dayId = hourId.substring(0, 8);</span><br><span class=\"line\">        String hivesql = &quot;LOAD DATA LOCAL INPATH &apos;&quot; + Main.BASE_PATH + &quot;/logs/test/*&quot; + hourId + &quot;*&apos; INTO TABLE &quot; + Config.getString(&quot;hive.database&quot;) + &quot;.test_bh PARTITION(day_id=&apos;&quot; + dayId + &quot;&apos;,hour_id=&apos;&quot; + hourId + &quot;&apos;)&quot;;</span><br><span class=\"line\">        HiveUtil.exec(hivesql);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    catch (Exception e)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        log.error(&quot;上传数据失败&quot; + e.getMessage(), e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>&emsp;至此就将需要的数据存储到hive表中了，接下来就是根据需求进行数据分析了。log4j真的强大。</p>"},{"title":"shell中获取hdfs文件路径参数","abbrlink":"ad3ef435","date":"2019-03-28T12:55:57.000Z","_content":"\n&emsp;算是个简单的工具吧。需求是这样的，有套脚本是不定期跑的累积表，所以需要知道上次跑到了哪天。累积表有个day_id分区，所以直接看表分区是最后的day_id就行。\n<!--more-->\n不多比比直接上代码\n```\n#!/bin/bash\nhdfs_path=$1\n#获取hdfs最后一个时间分区时间参数脚本\n#注意分区在第几层改第二个print的参数\nlast_data_date=`hadoop fs -ls  $hdfs_path | awk '{print $8}' |  awk -F'/' '{print $8}' | tail -n 1`\n#echo $last_data_date \nlast_date=${last_data_date##*=}\necho $last_date \n```\n获取其他信息同理。需要加别的条件就加grep。","source":"_posts/shell中获取hdfs文件路径参数.md","raw":"---\ntitle: shell中获取hdfs文件路径参数\ntags:\n  - linux\n  - shell\nabbrlink: ad3ef435\ndate: 2019-03-28 20:55:57\n---\n\n&emsp;算是个简单的工具吧。需求是这样的，有套脚本是不定期跑的累积表，所以需要知道上次跑到了哪天。累积表有个day_id分区，所以直接看表分区是最后的day_id就行。\n<!--more-->\n不多比比直接上代码\n```\n#!/bin/bash\nhdfs_path=$1\n#获取hdfs最后一个时间分区时间参数脚本\n#注意分区在第几层改第二个print的参数\nlast_data_date=`hadoop fs -ls  $hdfs_path | awk '{print $8}' |  awk -F'/' '{print $8}' | tail -n 1`\n#echo $last_data_date \nlast_date=${last_data_date##*=}\necho $last_date \n```\n获取其他信息同理。需要加别的条件就加grep。","slug":"shell中获取hdfs文件路径参数","published":1,"updated":"2019-09-18T13:11:36.918Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0q92d0q000d7guocmjrmdy6","content":"<p>&emsp;算是个简单的工具吧。需求是这样的，有套脚本是不定期跑的累积表，所以需要知道上次跑到了哪天。累积表有个day_id分区，所以直接看表分区是最后的day_id就行。<br><a id=\"more\"></a><br>不多比比直接上代码<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">hdfs_path=$1</span><br><span class=\"line\">#获取hdfs最后一个时间分区时间参数脚本</span><br><span class=\"line\">#注意分区在第几层改第二个print的参数</span><br><span class=\"line\">last_data_date=`hadoop fs -ls  $hdfs_path | awk &apos;&#123;print $8&#125;&apos; |  awk -F&apos;/&apos; &apos;&#123;print $8&#125;&apos; | tail -n 1`</span><br><span class=\"line\">#echo $last_data_date </span><br><span class=\"line\">last_date=$&#123;last_data_date##*=&#125;</span><br><span class=\"line\">echo $last_date</span><br></pre></td></tr></table></figure></p>\n<p>获取其他信息同理。需要加别的条件就加grep。</p>\n","site":{"data":{}},"excerpt":"<p>&emsp;算是个简单的工具吧。需求是这样的，有套脚本是不定期跑的累积表，所以需要知道上次跑到了哪天。累积表有个day_id分区，所以直接看表分区是最后的day_id就行。<br>","more":"<br>不多比比直接上代码<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">hdfs_path=$1</span><br><span class=\"line\">#获取hdfs最后一个时间分区时间参数脚本</span><br><span class=\"line\">#注意分区在第几层改第二个print的参数</span><br><span class=\"line\">last_data_date=`hadoop fs -ls  $hdfs_path | awk &apos;&#123;print $8&#125;&apos; |  awk -F&apos;/&apos; &apos;&#123;print $8&#125;&apos; | tail -n 1`</span><br><span class=\"line\">#echo $last_data_date </span><br><span class=\"line\">last_date=$&#123;last_data_date##*=&#125;</span><br><span class=\"line\">echo $last_date</span><br></pre></td></tr></table></figure></p>\n<p>获取其他信息同理。需要加别的条件就加grep。</p>"},{"title":"Kafka及Spring&Kafka整合","abbrlink":"85675a3e","date":"2018-09-13T00:05:26.000Z","_content":"&emsp;由于某项目的消息队列使用了Spring整合Kafka，开发中我需要使用kafka客户端模拟生产者和消费者。简单了解了一下Kafka，扫盲贴，先标记一下，日后再深入学习。  \n<!--more-->\n## 一、Kafka简介\n### 1.1 简介\n&emsp; kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。      \n&emsp;在大数据系统中，常常会碰到一个问题，整个大数据是由各个子系统组成，数据需要在各个子系统中高性能，低延迟的不停流转。传统的企业消息系统并不是非常适合大规模的数据处理。为了已在同时搞定在线应用（消息）和离线应用（数据文件，日志）Kafka就出现了。  \n&emsp;简单点概括一下：Kafka是一个分布式的，可划分的，高性能，低延迟的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。\n### 1.2 特点\n    * 高吞吐量\n    * 可进行持久化操作\n    * 分布式\n### 1.3 组件\n&emsp;Topic，Broker，Partition，Message，Producer，Consumer,Zookpeer\n#### 1.3.1 名词解释\n    服务：\n    Topic：主题，Kafka处理的消息的不同分类。\n    Broker：消息代理，Kafka集群中的一个kafka服务节点称为一个broker，主要存储消息数据。存在硬盘中。每个topic都是有分区的。\n    Partition：Topic物理上的分组，一个topic在broker中被分为1个或者多个partition，分区在创建topic的时候指定。\n    Message：消息，是通信的基本单位，每个消息都属于一个partition\n    服务相关：\n    Producer：消息和数据的生产者，向Kafka的一个topic发布消息。\n    Consumer：消息和数据的消费者，定于topic并处理其发布的消息。\n    Zookeeper：协调kafka的正常运行。\n### 1.4 应用场景\n构建实时的流数据管道，可靠地获取系统和应用程序之间的数据。  \n构建实时流的应用程序，对数据流进行转换或反应。  \n\n## 二、Kafka搭建\n### 2.1 安装\n&emsp;教程很多，就不写了。 \n### 2.2 配置\n&emsp;配置文件放在kafka下config下\n\n    * consumer.properites 消费者配置\n    * producer.properties 生产者配置\n    * server.properties kafka服务器的配置\n        broker.id 申明当前kafka服务器在集群中的唯一ID，需配置为integer,并且集群中的每一个kafka服务器的id都应是唯一的\n        listeners 申明此kafka服务器需要监听的端口号，如果是在本机上跑虚拟机运行可以不用配置本项，默认会使用localhost的地址，如果是在远程服务器上运行则必须配置，例如：\n                  listeners=PLAINTEXT:// 192.168.180.128:9092。并确保服务器的9092端口能够访问\n        zookeeper.connect 申明kafka所连接的zookeeper的地址 ，需配置为zookeeper的地址\n\n&emsp;上面配置文件中listeners的配置尤其注意，刚开始整的时候，没注意自己编写producer和cusmer时报错，如下：\n\n        Connection to node -1 could not be established. Broker may not be available.\n   \n&emsp;就是因为配置文件中的PLAINTEXT跟我请求的内容不同。\n\n&emsp;具体配置教程很多，也不写了。\n\n## 三、Kafka操作\n### 3.1 Topic操作\n#### 3.1.1 创建Topic\n    kafka-topics.sh --create --topic hbase --zookeeper ip1:port --partitions 3 --replication-factor 1\n    创建topic过程的问题，replication-factor个数不能超过broker的个数\n    创建topic后，可以在../data/kafka目录查看到分区的目录\n#### 3.1.2 查看Topic列表\n    kafka-topics.sh --list --zookeeper ip:port\n#### 3.1.3 查看某一个具体的Topic\n    kafka-topics.sh --describe xxx --zookeeper ip:port\n#### 3.1.4 修改Topic\n    kafka-topics.sh --alter --topic topic-test --zookeeper ip:port --partitions 3\n    不能修改replication-factor，以及只能对partition个数进行增加，不能减少\n#### 3.1.5 删除Topic\n    kafka-topics.sh --delete --topic topic-test --zookeeper ip:port\n    彻底删除一个topic，需要在server.properties中配置delete.topic.enable=true，否则只是标记删除\n    配置完成之后，需要重启kafka服务。\n### 3.2 生产者操作\n    sh kafka-console-producer.sh --broker-list ip1:port,ip2:port,ip3:port --sync --topic kafka-topic-test\n    生产数据的时候需要指定：当前数据流向哪个broker，以及哪一个topic\n### 3.3 消费者操作\n    sh kafka-console-consumer.sh --zookeeper ip1:port,ip2:port,ip3:port --topic kafka-topic-test --from-beginning\n    --from-begining 获取最新以及历史数据\n    \n    黑白名单（暂时未用到）\n    --blacklist 后面跟需要过滤的topic的列表，使用\",\"隔开，意思是除了列表中的topic之外，都能接收其它topic的数据\n    --whitelist 后面跟需要过滤的topic的列表，使用\",\"隔开，意思是除了列表中的topic之外，都不能接收其它topic的数据\n\n## 四、Springboot整合Kafka\n这个只是个人使用的简单的测试环境搭建，可能有很多地方有问题，以后深入学习时再检查。\n### 4.1 整合\n&emsp;springboot集成kafka的默认配置都在org.springframework.boot.autoconfigure.kafka包里面。直接使用即可。flag=深入学习kafka。\n### 4.2 pom.xml配置\n    <dependency>\n       <!--引入spring和kafka整合的jar-->\n    \t\t\t<groupId>org.springframework.cloud</groupId>\n    \t\t\t<artifactId>spring-cloud-starter-stream-kafka</artifactId>\n    \t\t\t<exclusions>\n    \t\t\t\t<exclusion>\n    \t\t\t\t\t<groupId>org.apache.kafka</groupId>\n    \t\t\t\t\t<artifactId>kafka_2.11</artifactId>\n    \t\t\t\t</exclusion>\n    \t\t\t\t<exclusion>\n    \t\t\t\t\t<groupId>org.apache.kafka</groupId>\n    \t\t\t\t\t<artifactId>kafka-clients</artifactId>\n    \t\t\t\t</exclusion>\n    \t\t\t\t<exclusion>\n    \t\t\t\t\t<groupId>org.springframework.kafka</groupId>\n    \t\t\t\t\t<artifactId>spring-kafka</artifactId>\n    \t\t\t\t</exclusion>\n    \t\t\t</exclusions>\n    \t\t</dependency>\n    \t\t<dependency>\n    \t\t\t<groupId>org.springframework.cloud</groupId>\n    \t\t\t<artifactId>spring-cloud-starter-hystrix</artifactId>\n    \t\t</dependency>\n    \t\t<dependency>\n    \t\t\t<groupId>org.apache.kafka</groupId>\n    \t\t\t<artifactId>kafka_2.11</artifactId>\n    \t\t\t<version>1.0.1</version>\n    \t\t</dependency>\n    \t\t<dependency>\n    \t\t    <groupId>org.springframework.cloud</groupId>\n    \t\t    <artifactId>spring-cloud-task-core</artifactId>\n    \t\t</dependency>\n    \t\t<dependency>\n    \t\t\t<groupId>org.springframework.kafka</groupId>\n    \t\t\t<artifactId>spring-kafka</artifactId>\n    \t\t\t<version>1.3.5.RELEASE</version><!--$NO-MVN-MAN-VER$-->\n    \t\t\t</dependency>\n    \t\t<dependency>\n### 4.3 Producer配置\n    @Configuration\n    @EnableKafka\n    public class KafkaProducer {\n    \n        public Map<String, Object> producerConfigs() {\n            Map<String, Object> props = new HashMap<>();\n            props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);\n            props.put(ProducerConfig.RETRIES_CONFIG, KafkaConfig.PRODUCER_RETRIES);\n            props.put(ProducerConfig.BATCH_SIZE_CONFIG, KafkaConfig.PRODUCER_BATCH_SIZE);\n            props.put(ProducerConfig.LINGER_MS_CONFIG, KafkaConfig.PRODUCER_LINGER_MS);\n            props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, KafkaConfig.PRODUCER_BUFFER_MEMORY);\n            props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n            props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n            props.put(\"advertised.host.name\",KafkaConfig.BOOTSTRAP_SERVERS);\n            props.put(ConsumerConfig.GROUP_ID_CONFIG, \"1\");\n    \n            System.out.println(\"KafkaConfig.BOOTSTRAP_SERVERS:\"+KafkaConfig.BOOTSTRAP_SERVERS);\n            return props;\n        }\n    \n        /** 获取工厂 */\n        public ProducerFactory<String, String> producerFactory() {\n            return new DefaultKafkaProducerFactory<>(producerConfigs());\n        }\n    \n        /** 注册实例 */\n        @Bean\n        public KafkaTemplate<String, String> kafkaTemplate() {\n            return new KafkaTemplate<>(producerFactory());\n        }\n    \n    }\n### 4.4 使用生产者\n     @Autowired\n     private KafkaTemplate<String, String> kafkaTemplate;\n     kafkaTemplate.send(\"kafka-topic-test\", \"helloWorld\");\n### 4.5 Consumer配置\n    @Configuration\n    @EnableKafka\n    public class KafkaConsumer {\n        private final static Logger log = LoggerFactory.getLogger(KafkaConsumer .class);\n  \n        @KafkaListener(topics = {\"kafka-topic-test\"})\n        public void consume(ConsumerRecord<?, ?> record) {\n            String topic = record.topic();\n            String value = record.value().toString();\n    \n            System.out.println(\"partitions:\"+record.partition()+\",\"+\"offset:\"+record.offset()+\",value=\"+value);\n            MqConsumerRunnable runnable = new MqConsumerRunnable(topic,value);\n            executor.execute(runnable);\n        }\n    \n        public Map<String, Object> consumerConfigs() {\n            Map<String, Object> props = new HashMap<>();\n            props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);\n            props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);\n            props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, \"100\");\n            props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n            props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n            props.put(ConsumerConfig.GROUP_ID_CONFIG, \"1\");\n            props.put(\"auto.offset.reset\", \"latest\");// 一般配置earliest 或者latest 值\n    \n            return props;\n        }\n    \n        /** 获取工厂 */\n        public ConsumerFactory<String, String> consumerFactory() {\n            return new DefaultKafkaConsumerFactory<>(consumerConfigs());\n        }\n    \n        /** 获取实例 */\n        @Bean\n        public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, String>> kafkaListenerContainerFactory() {\n            ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();\n            factory.setConsumerFactory(consumerFactory());\n            factory.setConcurrency(3);\n            factory.getContainerProperties().setPollTimeout(3000);\n            return factory;\n        }\n    \n    }\n### 4.6 使用消费者\n    public class KafkaMessageListener implements MessageListener<String, String> {\n    \n        private static Logger LOG = LoggerFactory.getLogger(KafkaMessageListener.class);\n        @Autowired\n        private AppProperties appProperties;\n    \n        @Override\n        public void onMessage(ConsumerRecord<String, String> data) {\n            LOG.info(\"消费消息topic：{} value {}\", data.topic(), data.value());\n            String topic = data.topic();\n            String content = data.value();\n            //可同时监听多个topic，根据不同topic处理不同的业务\n            if (topic.equals(\"topica\")) {           \n                LOG.info(\"###############topic:{} value:{}\" ,topic,content);\n            } else if (topic.equals(\"topicb\")) {\n             LOG.info(\"###############topic:{} value:{}\" ,topic,content);\n            } \n        }\n    }\n### 4.7 注意\n    kafkaTemplate.send(\"kafka-topic-test\", \"helloWorld\");\n    @KafkaListener(topics = {\"kafka-topic-test\"})\n    topic需要对应\n#### 4.8 使用\n&emsp;本地运行以后，到kafka服务器上可以进行消费者和生产者的模拟发送与接收信息。\n\n## 五、总结\n&emsp;上述方法进行模拟测试，可以测试，但是总感觉问题很大，却又找不出问题，这个后期再说吧，先凑合用。  \n&emsp;有关Kafka的具体学习，后期补上。\n\n## 六、相关链接\n感谢各位大佬：\n- [kafka 基础知识梳理](https://www.cnblogs.com/yangxiaoyi/p/7359236.html)\n- [kafka实战](https://www.cnblogs.com/hei12138/p/7805475.html)\n- [kafka笔记整理](http://blog.51cto.com/xpleaf/2090847)\n- [kafka介绍](https://www.cnblogs.com/yepei/p/6197236.html)","source":"_posts/Kafka及Spring-Kafka整合.md","raw":"---\ntitle: Kafka及Spring&Kafka整合\ntags:\n  - kafka\nabbrlink: 85675a3e\ndate: 2018-09-13 08:05:26\n---\n&emsp;由于某项目的消息队列使用了Spring整合Kafka，开发中我需要使用kafka客户端模拟生产者和消费者。简单了解了一下Kafka，扫盲贴，先标记一下，日后再深入学习。  \n<!--more-->\n## 一、Kafka简介\n### 1.1 简介\n&emsp; kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。      \n&emsp;在大数据系统中，常常会碰到一个问题，整个大数据是由各个子系统组成，数据需要在各个子系统中高性能，低延迟的不停流转。传统的企业消息系统并不是非常适合大规模的数据处理。为了已在同时搞定在线应用（消息）和离线应用（数据文件，日志）Kafka就出现了。  \n&emsp;简单点概括一下：Kafka是一个分布式的，可划分的，高性能，低延迟的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。\n### 1.2 特点\n    * 高吞吐量\n    * 可进行持久化操作\n    * 分布式\n### 1.3 组件\n&emsp;Topic，Broker，Partition，Message，Producer，Consumer,Zookpeer\n#### 1.3.1 名词解释\n    服务：\n    Topic：主题，Kafka处理的消息的不同分类。\n    Broker：消息代理，Kafka集群中的一个kafka服务节点称为一个broker，主要存储消息数据。存在硬盘中。每个topic都是有分区的。\n    Partition：Topic物理上的分组，一个topic在broker中被分为1个或者多个partition，分区在创建topic的时候指定。\n    Message：消息，是通信的基本单位，每个消息都属于一个partition\n    服务相关：\n    Producer：消息和数据的生产者，向Kafka的一个topic发布消息。\n    Consumer：消息和数据的消费者，定于topic并处理其发布的消息。\n    Zookeeper：协调kafka的正常运行。\n### 1.4 应用场景\n构建实时的流数据管道，可靠地获取系统和应用程序之间的数据。  \n构建实时流的应用程序，对数据流进行转换或反应。  \n\n## 二、Kafka搭建\n### 2.1 安装\n&emsp;教程很多，就不写了。 \n### 2.2 配置\n&emsp;配置文件放在kafka下config下\n\n    * consumer.properites 消费者配置\n    * producer.properties 生产者配置\n    * server.properties kafka服务器的配置\n        broker.id 申明当前kafka服务器在集群中的唯一ID，需配置为integer,并且集群中的每一个kafka服务器的id都应是唯一的\n        listeners 申明此kafka服务器需要监听的端口号，如果是在本机上跑虚拟机运行可以不用配置本项，默认会使用localhost的地址，如果是在远程服务器上运行则必须配置，例如：\n                  listeners=PLAINTEXT:// 192.168.180.128:9092。并确保服务器的9092端口能够访问\n        zookeeper.connect 申明kafka所连接的zookeeper的地址 ，需配置为zookeeper的地址\n\n&emsp;上面配置文件中listeners的配置尤其注意，刚开始整的时候，没注意自己编写producer和cusmer时报错，如下：\n\n        Connection to node -1 could not be established. Broker may not be available.\n   \n&emsp;就是因为配置文件中的PLAINTEXT跟我请求的内容不同。\n\n&emsp;具体配置教程很多，也不写了。\n\n## 三、Kafka操作\n### 3.1 Topic操作\n#### 3.1.1 创建Topic\n    kafka-topics.sh --create --topic hbase --zookeeper ip1:port --partitions 3 --replication-factor 1\n    创建topic过程的问题，replication-factor个数不能超过broker的个数\n    创建topic后，可以在../data/kafka目录查看到分区的目录\n#### 3.1.2 查看Topic列表\n    kafka-topics.sh --list --zookeeper ip:port\n#### 3.1.3 查看某一个具体的Topic\n    kafka-topics.sh --describe xxx --zookeeper ip:port\n#### 3.1.4 修改Topic\n    kafka-topics.sh --alter --topic topic-test --zookeeper ip:port --partitions 3\n    不能修改replication-factor，以及只能对partition个数进行增加，不能减少\n#### 3.1.5 删除Topic\n    kafka-topics.sh --delete --topic topic-test --zookeeper ip:port\n    彻底删除一个topic，需要在server.properties中配置delete.topic.enable=true，否则只是标记删除\n    配置完成之后，需要重启kafka服务。\n### 3.2 生产者操作\n    sh kafka-console-producer.sh --broker-list ip1:port,ip2:port,ip3:port --sync --topic kafka-topic-test\n    生产数据的时候需要指定：当前数据流向哪个broker，以及哪一个topic\n### 3.3 消费者操作\n    sh kafka-console-consumer.sh --zookeeper ip1:port,ip2:port,ip3:port --topic kafka-topic-test --from-beginning\n    --from-begining 获取最新以及历史数据\n    \n    黑白名单（暂时未用到）\n    --blacklist 后面跟需要过滤的topic的列表，使用\",\"隔开，意思是除了列表中的topic之外，都能接收其它topic的数据\n    --whitelist 后面跟需要过滤的topic的列表，使用\",\"隔开，意思是除了列表中的topic之外，都不能接收其它topic的数据\n\n## 四、Springboot整合Kafka\n这个只是个人使用的简单的测试环境搭建，可能有很多地方有问题，以后深入学习时再检查。\n### 4.1 整合\n&emsp;springboot集成kafka的默认配置都在org.springframework.boot.autoconfigure.kafka包里面。直接使用即可。flag=深入学习kafka。\n### 4.2 pom.xml配置\n    <dependency>\n       <!--引入spring和kafka整合的jar-->\n    \t\t\t<groupId>org.springframework.cloud</groupId>\n    \t\t\t<artifactId>spring-cloud-starter-stream-kafka</artifactId>\n    \t\t\t<exclusions>\n    \t\t\t\t<exclusion>\n    \t\t\t\t\t<groupId>org.apache.kafka</groupId>\n    \t\t\t\t\t<artifactId>kafka_2.11</artifactId>\n    \t\t\t\t</exclusion>\n    \t\t\t\t<exclusion>\n    \t\t\t\t\t<groupId>org.apache.kafka</groupId>\n    \t\t\t\t\t<artifactId>kafka-clients</artifactId>\n    \t\t\t\t</exclusion>\n    \t\t\t\t<exclusion>\n    \t\t\t\t\t<groupId>org.springframework.kafka</groupId>\n    \t\t\t\t\t<artifactId>spring-kafka</artifactId>\n    \t\t\t\t</exclusion>\n    \t\t\t</exclusions>\n    \t\t</dependency>\n    \t\t<dependency>\n    \t\t\t<groupId>org.springframework.cloud</groupId>\n    \t\t\t<artifactId>spring-cloud-starter-hystrix</artifactId>\n    \t\t</dependency>\n    \t\t<dependency>\n    \t\t\t<groupId>org.apache.kafka</groupId>\n    \t\t\t<artifactId>kafka_2.11</artifactId>\n    \t\t\t<version>1.0.1</version>\n    \t\t</dependency>\n    \t\t<dependency>\n    \t\t    <groupId>org.springframework.cloud</groupId>\n    \t\t    <artifactId>spring-cloud-task-core</artifactId>\n    \t\t</dependency>\n    \t\t<dependency>\n    \t\t\t<groupId>org.springframework.kafka</groupId>\n    \t\t\t<artifactId>spring-kafka</artifactId>\n    \t\t\t<version>1.3.5.RELEASE</version><!--$NO-MVN-MAN-VER$-->\n    \t\t\t</dependency>\n    \t\t<dependency>\n### 4.3 Producer配置\n    @Configuration\n    @EnableKafka\n    public class KafkaProducer {\n    \n        public Map<String, Object> producerConfigs() {\n            Map<String, Object> props = new HashMap<>();\n            props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);\n            props.put(ProducerConfig.RETRIES_CONFIG, KafkaConfig.PRODUCER_RETRIES);\n            props.put(ProducerConfig.BATCH_SIZE_CONFIG, KafkaConfig.PRODUCER_BATCH_SIZE);\n            props.put(ProducerConfig.LINGER_MS_CONFIG, KafkaConfig.PRODUCER_LINGER_MS);\n            props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, KafkaConfig.PRODUCER_BUFFER_MEMORY);\n            props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n            props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n            props.put(\"advertised.host.name\",KafkaConfig.BOOTSTRAP_SERVERS);\n            props.put(ConsumerConfig.GROUP_ID_CONFIG, \"1\");\n    \n            System.out.println(\"KafkaConfig.BOOTSTRAP_SERVERS:\"+KafkaConfig.BOOTSTRAP_SERVERS);\n            return props;\n        }\n    \n        /** 获取工厂 */\n        public ProducerFactory<String, String> producerFactory() {\n            return new DefaultKafkaProducerFactory<>(producerConfigs());\n        }\n    \n        /** 注册实例 */\n        @Bean\n        public KafkaTemplate<String, String> kafkaTemplate() {\n            return new KafkaTemplate<>(producerFactory());\n        }\n    \n    }\n### 4.4 使用生产者\n     @Autowired\n     private KafkaTemplate<String, String> kafkaTemplate;\n     kafkaTemplate.send(\"kafka-topic-test\", \"helloWorld\");\n### 4.5 Consumer配置\n    @Configuration\n    @EnableKafka\n    public class KafkaConsumer {\n        private final static Logger log = LoggerFactory.getLogger(KafkaConsumer .class);\n  \n        @KafkaListener(topics = {\"kafka-topic-test\"})\n        public void consume(ConsumerRecord<?, ?> record) {\n            String topic = record.topic();\n            String value = record.value().toString();\n    \n            System.out.println(\"partitions:\"+record.partition()+\",\"+\"offset:\"+record.offset()+\",value=\"+value);\n            MqConsumerRunnable runnable = new MqConsumerRunnable(topic,value);\n            executor.execute(runnable);\n        }\n    \n        public Map<String, Object> consumerConfigs() {\n            Map<String, Object> props = new HashMap<>();\n            props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);\n            props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);\n            props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, \"100\");\n            props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n            props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n            props.put(ConsumerConfig.GROUP_ID_CONFIG, \"1\");\n            props.put(\"auto.offset.reset\", \"latest\");// 一般配置earliest 或者latest 值\n    \n            return props;\n        }\n    \n        /** 获取工厂 */\n        public ConsumerFactory<String, String> consumerFactory() {\n            return new DefaultKafkaConsumerFactory<>(consumerConfigs());\n        }\n    \n        /** 获取实例 */\n        @Bean\n        public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, String>> kafkaListenerContainerFactory() {\n            ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();\n            factory.setConsumerFactory(consumerFactory());\n            factory.setConcurrency(3);\n            factory.getContainerProperties().setPollTimeout(3000);\n            return factory;\n        }\n    \n    }\n### 4.6 使用消费者\n    public class KafkaMessageListener implements MessageListener<String, String> {\n    \n        private static Logger LOG = LoggerFactory.getLogger(KafkaMessageListener.class);\n        @Autowired\n        private AppProperties appProperties;\n    \n        @Override\n        public void onMessage(ConsumerRecord<String, String> data) {\n            LOG.info(\"消费消息topic：{} value {}\", data.topic(), data.value());\n            String topic = data.topic();\n            String content = data.value();\n            //可同时监听多个topic，根据不同topic处理不同的业务\n            if (topic.equals(\"topica\")) {           \n                LOG.info(\"###############topic:{} value:{}\" ,topic,content);\n            } else if (topic.equals(\"topicb\")) {\n             LOG.info(\"###############topic:{} value:{}\" ,topic,content);\n            } \n        }\n    }\n### 4.7 注意\n    kafkaTemplate.send(\"kafka-topic-test\", \"helloWorld\");\n    @KafkaListener(topics = {\"kafka-topic-test\"})\n    topic需要对应\n#### 4.8 使用\n&emsp;本地运行以后，到kafka服务器上可以进行消费者和生产者的模拟发送与接收信息。\n\n## 五、总结\n&emsp;上述方法进行模拟测试，可以测试，但是总感觉问题很大，却又找不出问题，这个后期再说吧，先凑合用。  \n&emsp;有关Kafka的具体学习，后期补上。\n\n## 六、相关链接\n感谢各位大佬：\n- [kafka 基础知识梳理](https://www.cnblogs.com/yangxiaoyi/p/7359236.html)\n- [kafka实战](https://www.cnblogs.com/hei12138/p/7805475.html)\n- [kafka笔记整理](http://blog.51cto.com/xpleaf/2090847)\n- [kafka介绍](https://www.cnblogs.com/yepei/p/6197236.html)","slug":"Kafka及Spring-Kafka整合","published":1,"updated":"2019-09-18T13:11:36.919Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0q92d1f00107guotky9r4l0","content":"<p>&emsp;由于某项目的消息队列使用了Spring整合Kafka，开发中我需要使用kafka客户端模拟生产者和消费者。简单了解了一下Kafka，扫盲贴，先标记一下，日后再深入学习。<br><a id=\"more\"></a></p>\n<h2 id=\"一、Kafka简介\"><a href=\"#一、Kafka简介\" class=\"headerlink\" title=\"一、Kafka简介\"></a>一、Kafka简介</h2><h3 id=\"1-1-简介\"><a href=\"#1-1-简介\" class=\"headerlink\" title=\"1.1 简介\"></a>1.1 简介</h3><p>&emsp; kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。<br>&emsp;在大数据系统中，常常会碰到一个问题，整个大数据是由各个子系统组成，数据需要在各个子系统中高性能，低延迟的不停流转。传统的企业消息系统并不是非常适合大规模的数据处理。为了已在同时搞定在线应用（消息）和离线应用（数据文件，日志）Kafka就出现了。<br>&emsp;简单点概括一下：Kafka是一个分布式的，可划分的，高性能，低延迟的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。</p>\n<h3 id=\"1-2-特点\"><a href=\"#1-2-特点\" class=\"headerlink\" title=\"1.2 特点\"></a>1.2 特点</h3><pre><code>* 高吞吐量\n* 可进行持久化操作\n* 分布式\n</code></pre><h3 id=\"1-3-组件\"><a href=\"#1-3-组件\" class=\"headerlink\" title=\"1.3 组件\"></a>1.3 组件</h3><p>&emsp;Topic，Broker，Partition，Message，Producer，Consumer,Zookpeer</p>\n<h4 id=\"1-3-1-名词解释\"><a href=\"#1-3-1-名词解释\" class=\"headerlink\" title=\"1.3.1 名词解释\"></a>1.3.1 名词解释</h4><pre><code>服务：\nTopic：主题，Kafka处理的消息的不同分类。\nBroker：消息代理，Kafka集群中的一个kafka服务节点称为一个broker，主要存储消息数据。存在硬盘中。每个topic都是有分区的。\nPartition：Topic物理上的分组，一个topic在broker中被分为1个或者多个partition，分区在创建topic的时候指定。\nMessage：消息，是通信的基本单位，每个消息都属于一个partition\n服务相关：\nProducer：消息和数据的生产者，向Kafka的一个topic发布消息。\nConsumer：消息和数据的消费者，定于topic并处理其发布的消息。\nZookeeper：协调kafka的正常运行。\n</code></pre><h3 id=\"1-4-应用场景\"><a href=\"#1-4-应用场景\" class=\"headerlink\" title=\"1.4 应用场景\"></a>1.4 应用场景</h3><p>构建实时的流数据管道，可靠地获取系统和应用程序之间的数据。<br>构建实时流的应用程序，对数据流进行转换或反应。  </p>\n<h2 id=\"二、Kafka搭建\"><a href=\"#二、Kafka搭建\" class=\"headerlink\" title=\"二、Kafka搭建\"></a>二、Kafka搭建</h2><h3 id=\"2-1-安装\"><a href=\"#2-1-安装\" class=\"headerlink\" title=\"2.1 安装\"></a>2.1 安装</h3><p>&emsp;教程很多，就不写了。 </p>\n<h3 id=\"2-2-配置\"><a href=\"#2-2-配置\" class=\"headerlink\" title=\"2.2 配置\"></a>2.2 配置</h3><p>&emsp;配置文件放在kafka下config下</p>\n<pre><code>* consumer.properites 消费者配置\n* producer.properties 生产者配置\n* server.properties kafka服务器的配置\n    broker.id 申明当前kafka服务器在集群中的唯一ID，需配置为integer,并且集群中的每一个kafka服务器的id都应是唯一的\n    listeners 申明此kafka服务器需要监听的端口号，如果是在本机上跑虚拟机运行可以不用配置本项，默认会使用localhost的地址，如果是在远程服务器上运行则必须配置，例如：\n              listeners=PLAINTEXT:// 192.168.180.128:9092。并确保服务器的9092端口能够访问\n    zookeeper.connect 申明kafka所连接的zookeeper的地址 ，需配置为zookeeper的地址\n</code></pre><p>&emsp;上面配置文件中listeners的配置尤其注意，刚开始整的时候，没注意自己编写producer和cusmer时报错，如下：</p>\n<pre><code>Connection to node -1 could not be established. Broker may not be available.\n</code></pre><p>&emsp;就是因为配置文件中的PLAINTEXT跟我请求的内容不同。</p>\n<p>&emsp;具体配置教程很多，也不写了。</p>\n<h2 id=\"三、Kafka操作\"><a href=\"#三、Kafka操作\" class=\"headerlink\" title=\"三、Kafka操作\"></a>三、Kafka操作</h2><h3 id=\"3-1-Topic操作\"><a href=\"#3-1-Topic操作\" class=\"headerlink\" title=\"3.1 Topic操作\"></a>3.1 Topic操作</h3><h4 id=\"3-1-1-创建Topic\"><a href=\"#3-1-1-创建Topic\" class=\"headerlink\" title=\"3.1.1 创建Topic\"></a>3.1.1 创建Topic</h4><pre><code>kafka-topics.sh --create --topic hbase --zookeeper ip1:port --partitions 3 --replication-factor 1\n创建topic过程的问题，replication-factor个数不能超过broker的个数\n创建topic后，可以在../data/kafka目录查看到分区的目录\n</code></pre><h4 id=\"3-1-2-查看Topic列表\"><a href=\"#3-1-2-查看Topic列表\" class=\"headerlink\" title=\"3.1.2 查看Topic列表\"></a>3.1.2 查看Topic列表</h4><pre><code>kafka-topics.sh --list --zookeeper ip:port\n</code></pre><h4 id=\"3-1-3-查看某一个具体的Topic\"><a href=\"#3-1-3-查看某一个具体的Topic\" class=\"headerlink\" title=\"3.1.3 查看某一个具体的Topic\"></a>3.1.3 查看某一个具体的Topic</h4><pre><code>kafka-topics.sh --describe xxx --zookeeper ip:port\n</code></pre><h4 id=\"3-1-4-修改Topic\"><a href=\"#3-1-4-修改Topic\" class=\"headerlink\" title=\"3.1.4 修改Topic\"></a>3.1.4 修改Topic</h4><pre><code>kafka-topics.sh --alter --topic topic-test --zookeeper ip:port --partitions 3\n不能修改replication-factor，以及只能对partition个数进行增加，不能减少\n</code></pre><h4 id=\"3-1-5-删除Topic\"><a href=\"#3-1-5-删除Topic\" class=\"headerlink\" title=\"3.1.5 删除Topic\"></a>3.1.5 删除Topic</h4><pre><code>kafka-topics.sh --delete --topic topic-test --zookeeper ip:port\n彻底删除一个topic，需要在server.properties中配置delete.topic.enable=true，否则只是标记删除\n配置完成之后，需要重启kafka服务。\n</code></pre><h3 id=\"3-2-生产者操作\"><a href=\"#3-2-生产者操作\" class=\"headerlink\" title=\"3.2 生产者操作\"></a>3.2 生产者操作</h3><pre><code>sh kafka-console-producer.sh --broker-list ip1:port,ip2:port,ip3:port --sync --topic kafka-topic-test\n生产数据的时候需要指定：当前数据流向哪个broker，以及哪一个topic\n</code></pre><h3 id=\"3-3-消费者操作\"><a href=\"#3-3-消费者操作\" class=\"headerlink\" title=\"3.3 消费者操作\"></a>3.3 消费者操作</h3><pre><code>sh kafka-console-consumer.sh --zookeeper ip1:port,ip2:port,ip3:port --topic kafka-topic-test --from-beginning\n--from-begining 获取最新以及历史数据\n\n黑白名单（暂时未用到）\n--blacklist 后面跟需要过滤的topic的列表，使用&quot;,&quot;隔开，意思是除了列表中的topic之外，都能接收其它topic的数据\n--whitelist 后面跟需要过滤的topic的列表，使用&quot;,&quot;隔开，意思是除了列表中的topic之外，都不能接收其它topic的数据\n</code></pre><h2 id=\"四、Springboot整合Kafka\"><a href=\"#四、Springboot整合Kafka\" class=\"headerlink\" title=\"四、Springboot整合Kafka\"></a>四、Springboot整合Kafka</h2><p>这个只是个人使用的简单的测试环境搭建，可能有很多地方有问题，以后深入学习时再检查。</p>\n<h3 id=\"4-1-整合\"><a href=\"#4-1-整合\" class=\"headerlink\" title=\"4.1 整合\"></a>4.1 整合</h3><p>&emsp;springboot集成kafka的默认配置都在org.springframework.boot.autoconfigure.kafka包里面。直接使用即可。flag=深入学习kafka。</p>\n<h3 id=\"4-2-pom-xml配置\"><a href=\"#4-2-pom-xml配置\" class=\"headerlink\" title=\"4.2 pom.xml配置\"></a>4.2 pom.xml配置</h3><pre><code>&lt;dependency&gt;\n   &lt;!--引入spring和kafka整合的jar--&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-stream-kafka&lt;/artifactId&gt;\n            &lt;exclusions&gt;\n                &lt;exclusion&gt;\n                    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n                    &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;\n                &lt;/exclusion&gt;\n                &lt;exclusion&gt;\n                    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n                    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;\n                &lt;/exclusion&gt;\n                &lt;exclusion&gt;\n                    &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;\n                    &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;\n                &lt;/exclusion&gt;\n            &lt;/exclusions&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n            &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;\n            &lt;version&gt;1.0.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-task-core&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;\n            &lt;version&gt;1.3.5.RELEASE&lt;/version&gt;&lt;!--$NO-MVN-MAN-VER$--&gt;\n            &lt;/dependency&gt;\n        &lt;dependency&gt;\n</code></pre><h3 id=\"4-3-Producer配置\"><a href=\"#4-3-Producer配置\" class=\"headerlink\" title=\"4.3 Producer配置\"></a>4.3 Producer配置</h3><pre><code>@Configuration\n@EnableKafka\npublic class KafkaProducer {\n\n    public Map&lt;String, Object&gt; producerConfigs() {\n        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);\n        props.put(ProducerConfig.RETRIES_CONFIG, KafkaConfig.PRODUCER_RETRIES);\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, KafkaConfig.PRODUCER_BATCH_SIZE);\n        props.put(ProducerConfig.LINGER_MS_CONFIG, KafkaConfig.PRODUCER_LINGER_MS);\n        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, KafkaConfig.PRODUCER_BUFFER_MEMORY);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(&quot;advertised.host.name&quot;,KafkaConfig.BOOTSTRAP_SERVERS);\n        props.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;1&quot;);\n\n        System.out.println(&quot;KafkaConfig.BOOTSTRAP_SERVERS:&quot;+KafkaConfig.BOOTSTRAP_SERVERS);\n        return props;\n    }\n\n    /** 获取工厂 */\n    public ProducerFactory&lt;String, String&gt; producerFactory() {\n        return new DefaultKafkaProducerFactory&lt;&gt;(producerConfigs());\n    }\n\n    /** 注册实例 */\n    @Bean\n    public KafkaTemplate&lt;String, String&gt; kafkaTemplate() {\n        return new KafkaTemplate&lt;&gt;(producerFactory());\n    }\n\n}\n</code></pre><h3 id=\"4-4-使用生产者\"><a href=\"#4-4-使用生产者\" class=\"headerlink\" title=\"4.4 使用生产者\"></a>4.4 使用生产者</h3><pre><code>@Autowired\nprivate KafkaTemplate&lt;String, String&gt; kafkaTemplate;\nkafkaTemplate.send(&quot;kafka-topic-test&quot;, &quot;helloWorld&quot;);\n</code></pre><h3 id=\"4-5-Consumer配置\"><a href=\"#4-5-Consumer配置\" class=\"headerlink\" title=\"4.5 Consumer配置\"></a>4.5 Consumer配置</h3><pre><code>@Configuration\n@EnableKafka\npublic class KafkaConsumer {\n    private final static Logger log = LoggerFactory.getLogger(KafkaConsumer .class);\n\n    @KafkaListener(topics = {&quot;kafka-topic-test&quot;})\n    public void consume(ConsumerRecord&lt;?, ?&gt; record) {\n        String topic = record.topic();\n        String value = record.value().toString();\n\n        System.out.println(&quot;partitions:&quot;+record.partition()+&quot;,&quot;+&quot;offset:&quot;+record.offset()+&quot;,value=&quot;+value);\n        MqConsumerRunnable runnable = new MqConsumerRunnable(topic,value);\n        executor.execute(runnable);\n    }\n\n    public Map&lt;String, Object&gt; consumerConfigs() {\n        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);\n        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, &quot;100&quot;);\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;1&quot;);\n        props.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);// 一般配置earliest 或者latest 值\n\n        return props;\n    }\n\n    /** 获取工厂 */\n    public ConsumerFactory&lt;String, String&gt; consumerFactory() {\n        return new DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs());\n    }\n\n    /** 获取实例 */\n    @Bean\n    public KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;String, String&gt;&gt; kafkaListenerContainerFactory() {\n        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();\n        factory.setConsumerFactory(consumerFactory());\n        factory.setConcurrency(3);\n        factory.getContainerProperties().setPollTimeout(3000);\n        return factory;\n    }\n\n}\n</code></pre><h3 id=\"4-6-使用消费者\"><a href=\"#4-6-使用消费者\" class=\"headerlink\" title=\"4.6 使用消费者\"></a>4.6 使用消费者</h3><pre><code>public class KafkaMessageListener implements MessageListener&lt;String, String&gt; {\n\n    private static Logger LOG = LoggerFactory.getLogger(KafkaMessageListener.class);\n    @Autowired\n    private AppProperties appProperties;\n\n    @Override\n    public void onMessage(ConsumerRecord&lt;String, String&gt; data) {\n        LOG.info(&quot;消费消息topic：{} value {}&quot;, data.topic(), data.value());\n        String topic = data.topic();\n        String content = data.value();\n        //可同时监听多个topic，根据不同topic处理不同的业务\n        if (topic.equals(&quot;topica&quot;)) {           \n            LOG.info(&quot;###############topic:{} value:{}&quot; ,topic,content);\n        } else if (topic.equals(&quot;topicb&quot;)) {\n         LOG.info(&quot;###############topic:{} value:{}&quot; ,topic,content);\n        } \n    }\n}\n</code></pre><h3 id=\"4-7-注意\"><a href=\"#4-7-注意\" class=\"headerlink\" title=\"4.7 注意\"></a>4.7 注意</h3><pre><code>kafkaTemplate.send(&quot;kafka-topic-test&quot;, &quot;helloWorld&quot;);\n@KafkaListener(topics = {&quot;kafka-topic-test&quot;})\ntopic需要对应\n</code></pre><h4 id=\"4-8-使用\"><a href=\"#4-8-使用\" class=\"headerlink\" title=\"4.8 使用\"></a>4.8 使用</h4><p>&emsp;本地运行以后，到kafka服务器上可以进行消费者和生产者的模拟发送与接收信息。</p>\n<h2 id=\"五、总结\"><a href=\"#五、总结\" class=\"headerlink\" title=\"五、总结\"></a>五、总结</h2><p>&emsp;上述方法进行模拟测试，可以测试，但是总感觉问题很大，却又找不出问题，这个后期再说吧，先凑合用。<br>&emsp;有关Kafka的具体学习，后期补上。</p>\n<h2 id=\"六、相关链接\"><a href=\"#六、相关链接\" class=\"headerlink\" title=\"六、相关链接\"></a>六、相关链接</h2><p>感谢各位大佬：</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/yangxiaoyi/p/7359236.html\" target=\"_blank\" rel=\"noopener\">kafka 基础知识梳理</a></li>\n<li><a href=\"https://www.cnblogs.com/hei12138/p/7805475.html\" target=\"_blank\" rel=\"noopener\">kafka实战</a></li>\n<li><a href=\"http://blog.51cto.com/xpleaf/2090847\" target=\"_blank\" rel=\"noopener\">kafka笔记整理</a></li>\n<li><a href=\"https://www.cnblogs.com/yepei/p/6197236.html\" target=\"_blank\" rel=\"noopener\">kafka介绍</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>&emsp;由于某项目的消息队列使用了Spring整合Kafka，开发中我需要使用kafka客户端模拟生产者和消费者。简单了解了一下Kafka，扫盲贴，先标记一下，日后再深入学习。<br>","more":"</p>\n<h2 id=\"一、Kafka简介\"><a href=\"#一、Kafka简介\" class=\"headerlink\" title=\"一、Kafka简介\"></a>一、Kafka简介</h2><h3 id=\"1-1-简介\"><a href=\"#1-1-简介\" class=\"headerlink\" title=\"1.1 简介\"></a>1.1 简介</h3><p>&emsp; kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。<br>&emsp;在大数据系统中，常常会碰到一个问题，整个大数据是由各个子系统组成，数据需要在各个子系统中高性能，低延迟的不停流转。传统的企业消息系统并不是非常适合大规模的数据处理。为了已在同时搞定在线应用（消息）和离线应用（数据文件，日志）Kafka就出现了。<br>&emsp;简单点概括一下：Kafka是一个分布式的，可划分的，高性能，低延迟的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。</p>\n<h3 id=\"1-2-特点\"><a href=\"#1-2-特点\" class=\"headerlink\" title=\"1.2 特点\"></a>1.2 特点</h3><pre><code>* 高吞吐量\n* 可进行持久化操作\n* 分布式\n</code></pre><h3 id=\"1-3-组件\"><a href=\"#1-3-组件\" class=\"headerlink\" title=\"1.3 组件\"></a>1.3 组件</h3><p>&emsp;Topic，Broker，Partition，Message，Producer，Consumer,Zookpeer</p>\n<h4 id=\"1-3-1-名词解释\"><a href=\"#1-3-1-名词解释\" class=\"headerlink\" title=\"1.3.1 名词解释\"></a>1.3.1 名词解释</h4><pre><code>服务：\nTopic：主题，Kafka处理的消息的不同分类。\nBroker：消息代理，Kafka集群中的一个kafka服务节点称为一个broker，主要存储消息数据。存在硬盘中。每个topic都是有分区的。\nPartition：Topic物理上的分组，一个topic在broker中被分为1个或者多个partition，分区在创建topic的时候指定。\nMessage：消息，是通信的基本单位，每个消息都属于一个partition\n服务相关：\nProducer：消息和数据的生产者，向Kafka的一个topic发布消息。\nConsumer：消息和数据的消费者，定于topic并处理其发布的消息。\nZookeeper：协调kafka的正常运行。\n</code></pre><h3 id=\"1-4-应用场景\"><a href=\"#1-4-应用场景\" class=\"headerlink\" title=\"1.4 应用场景\"></a>1.4 应用场景</h3><p>构建实时的流数据管道，可靠地获取系统和应用程序之间的数据。<br>构建实时流的应用程序，对数据流进行转换或反应。  </p>\n<h2 id=\"二、Kafka搭建\"><a href=\"#二、Kafka搭建\" class=\"headerlink\" title=\"二、Kafka搭建\"></a>二、Kafka搭建</h2><h3 id=\"2-1-安装\"><a href=\"#2-1-安装\" class=\"headerlink\" title=\"2.1 安装\"></a>2.1 安装</h3><p>&emsp;教程很多，就不写了。 </p>\n<h3 id=\"2-2-配置\"><a href=\"#2-2-配置\" class=\"headerlink\" title=\"2.2 配置\"></a>2.2 配置</h3><p>&emsp;配置文件放在kafka下config下</p>\n<pre><code>* consumer.properites 消费者配置\n* producer.properties 生产者配置\n* server.properties kafka服务器的配置\n    broker.id 申明当前kafka服务器在集群中的唯一ID，需配置为integer,并且集群中的每一个kafka服务器的id都应是唯一的\n    listeners 申明此kafka服务器需要监听的端口号，如果是在本机上跑虚拟机运行可以不用配置本项，默认会使用localhost的地址，如果是在远程服务器上运行则必须配置，例如：\n              listeners=PLAINTEXT:// 192.168.180.128:9092。并确保服务器的9092端口能够访问\n    zookeeper.connect 申明kafka所连接的zookeeper的地址 ，需配置为zookeeper的地址\n</code></pre><p>&emsp;上面配置文件中listeners的配置尤其注意，刚开始整的时候，没注意自己编写producer和cusmer时报错，如下：</p>\n<pre><code>Connection to node -1 could not be established. Broker may not be available.\n</code></pre><p>&emsp;就是因为配置文件中的PLAINTEXT跟我请求的内容不同。</p>\n<p>&emsp;具体配置教程很多，也不写了。</p>\n<h2 id=\"三、Kafka操作\"><a href=\"#三、Kafka操作\" class=\"headerlink\" title=\"三、Kafka操作\"></a>三、Kafka操作</h2><h3 id=\"3-1-Topic操作\"><a href=\"#3-1-Topic操作\" class=\"headerlink\" title=\"3.1 Topic操作\"></a>3.1 Topic操作</h3><h4 id=\"3-1-1-创建Topic\"><a href=\"#3-1-1-创建Topic\" class=\"headerlink\" title=\"3.1.1 创建Topic\"></a>3.1.1 创建Topic</h4><pre><code>kafka-topics.sh --create --topic hbase --zookeeper ip1:port --partitions 3 --replication-factor 1\n创建topic过程的问题，replication-factor个数不能超过broker的个数\n创建topic后，可以在../data/kafka目录查看到分区的目录\n</code></pre><h4 id=\"3-1-2-查看Topic列表\"><a href=\"#3-1-2-查看Topic列表\" class=\"headerlink\" title=\"3.1.2 查看Topic列表\"></a>3.1.2 查看Topic列表</h4><pre><code>kafka-topics.sh --list --zookeeper ip:port\n</code></pre><h4 id=\"3-1-3-查看某一个具体的Topic\"><a href=\"#3-1-3-查看某一个具体的Topic\" class=\"headerlink\" title=\"3.1.3 查看某一个具体的Topic\"></a>3.1.3 查看某一个具体的Topic</h4><pre><code>kafka-topics.sh --describe xxx --zookeeper ip:port\n</code></pre><h4 id=\"3-1-4-修改Topic\"><a href=\"#3-1-4-修改Topic\" class=\"headerlink\" title=\"3.1.4 修改Topic\"></a>3.1.4 修改Topic</h4><pre><code>kafka-topics.sh --alter --topic topic-test --zookeeper ip:port --partitions 3\n不能修改replication-factor，以及只能对partition个数进行增加，不能减少\n</code></pre><h4 id=\"3-1-5-删除Topic\"><a href=\"#3-1-5-删除Topic\" class=\"headerlink\" title=\"3.1.5 删除Topic\"></a>3.1.5 删除Topic</h4><pre><code>kafka-topics.sh --delete --topic topic-test --zookeeper ip:port\n彻底删除一个topic，需要在server.properties中配置delete.topic.enable=true，否则只是标记删除\n配置完成之后，需要重启kafka服务。\n</code></pre><h3 id=\"3-2-生产者操作\"><a href=\"#3-2-生产者操作\" class=\"headerlink\" title=\"3.2 生产者操作\"></a>3.2 生产者操作</h3><pre><code>sh kafka-console-producer.sh --broker-list ip1:port,ip2:port,ip3:port --sync --topic kafka-topic-test\n生产数据的时候需要指定：当前数据流向哪个broker，以及哪一个topic\n</code></pre><h3 id=\"3-3-消费者操作\"><a href=\"#3-3-消费者操作\" class=\"headerlink\" title=\"3.3 消费者操作\"></a>3.3 消费者操作</h3><pre><code>sh kafka-console-consumer.sh --zookeeper ip1:port,ip2:port,ip3:port --topic kafka-topic-test --from-beginning\n--from-begining 获取最新以及历史数据\n\n黑白名单（暂时未用到）\n--blacklist 后面跟需要过滤的topic的列表，使用&quot;,&quot;隔开，意思是除了列表中的topic之外，都能接收其它topic的数据\n--whitelist 后面跟需要过滤的topic的列表，使用&quot;,&quot;隔开，意思是除了列表中的topic之外，都不能接收其它topic的数据\n</code></pre><h2 id=\"四、Springboot整合Kafka\"><a href=\"#四、Springboot整合Kafka\" class=\"headerlink\" title=\"四、Springboot整合Kafka\"></a>四、Springboot整合Kafka</h2><p>这个只是个人使用的简单的测试环境搭建，可能有很多地方有问题，以后深入学习时再检查。</p>\n<h3 id=\"4-1-整合\"><a href=\"#4-1-整合\" class=\"headerlink\" title=\"4.1 整合\"></a>4.1 整合</h3><p>&emsp;springboot集成kafka的默认配置都在org.springframework.boot.autoconfigure.kafka包里面。直接使用即可。flag=深入学习kafka。</p>\n<h3 id=\"4-2-pom-xml配置\"><a href=\"#4-2-pom-xml配置\" class=\"headerlink\" title=\"4.2 pom.xml配置\"></a>4.2 pom.xml配置</h3><pre><code>&lt;dependency&gt;\n   &lt;!--引入spring和kafka整合的jar--&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-stream-kafka&lt;/artifactId&gt;\n            &lt;exclusions&gt;\n                &lt;exclusion&gt;\n                    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n                    &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;\n                &lt;/exclusion&gt;\n                &lt;exclusion&gt;\n                    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n                    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;\n                &lt;/exclusion&gt;\n                &lt;exclusion&gt;\n                    &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;\n                    &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;\n                &lt;/exclusion&gt;\n            &lt;/exclusions&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n            &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;\n            &lt;version&gt;1.0.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-task-core&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;\n            &lt;version&gt;1.3.5.RELEASE&lt;/version&gt;&lt;!--$NO-MVN-MAN-VER$--&gt;\n            &lt;/dependency&gt;\n        &lt;dependency&gt;\n</code></pre><h3 id=\"4-3-Producer配置\"><a href=\"#4-3-Producer配置\" class=\"headerlink\" title=\"4.3 Producer配置\"></a>4.3 Producer配置</h3><pre><code>@Configuration\n@EnableKafka\npublic class KafkaProducer {\n\n    public Map&lt;String, Object&gt; producerConfigs() {\n        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);\n        props.put(ProducerConfig.RETRIES_CONFIG, KafkaConfig.PRODUCER_RETRIES);\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, KafkaConfig.PRODUCER_BATCH_SIZE);\n        props.put(ProducerConfig.LINGER_MS_CONFIG, KafkaConfig.PRODUCER_LINGER_MS);\n        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, KafkaConfig.PRODUCER_BUFFER_MEMORY);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(&quot;advertised.host.name&quot;,KafkaConfig.BOOTSTRAP_SERVERS);\n        props.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;1&quot;);\n\n        System.out.println(&quot;KafkaConfig.BOOTSTRAP_SERVERS:&quot;+KafkaConfig.BOOTSTRAP_SERVERS);\n        return props;\n    }\n\n    /** 获取工厂 */\n    public ProducerFactory&lt;String, String&gt; producerFactory() {\n        return new DefaultKafkaProducerFactory&lt;&gt;(producerConfigs());\n    }\n\n    /** 注册实例 */\n    @Bean\n    public KafkaTemplate&lt;String, String&gt; kafkaTemplate() {\n        return new KafkaTemplate&lt;&gt;(producerFactory());\n    }\n\n}\n</code></pre><h3 id=\"4-4-使用生产者\"><a href=\"#4-4-使用生产者\" class=\"headerlink\" title=\"4.4 使用生产者\"></a>4.4 使用生产者</h3><pre><code>@Autowired\nprivate KafkaTemplate&lt;String, String&gt; kafkaTemplate;\nkafkaTemplate.send(&quot;kafka-topic-test&quot;, &quot;helloWorld&quot;);\n</code></pre><h3 id=\"4-5-Consumer配置\"><a href=\"#4-5-Consumer配置\" class=\"headerlink\" title=\"4.5 Consumer配置\"></a>4.5 Consumer配置</h3><pre><code>@Configuration\n@EnableKafka\npublic class KafkaConsumer {\n    private final static Logger log = LoggerFactory.getLogger(KafkaConsumer .class);\n\n    @KafkaListener(topics = {&quot;kafka-topic-test&quot;})\n    public void consume(ConsumerRecord&lt;?, ?&gt; record) {\n        String topic = record.topic();\n        String value = record.value().toString();\n\n        System.out.println(&quot;partitions:&quot;+record.partition()+&quot;,&quot;+&quot;offset:&quot;+record.offset()+&quot;,value=&quot;+value);\n        MqConsumerRunnable runnable = new MqConsumerRunnable(topic,value);\n        executor.execute(runnable);\n    }\n\n    public Map&lt;String, Object&gt; consumerConfigs() {\n        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);\n        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, &quot;100&quot;);\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;1&quot;);\n        props.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);// 一般配置earliest 或者latest 值\n\n        return props;\n    }\n\n    /** 获取工厂 */\n    public ConsumerFactory&lt;String, String&gt; consumerFactory() {\n        return new DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs());\n    }\n\n    /** 获取实例 */\n    @Bean\n    public KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;String, String&gt;&gt; kafkaListenerContainerFactory() {\n        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();\n        factory.setConsumerFactory(consumerFactory());\n        factory.setConcurrency(3);\n        factory.getContainerProperties().setPollTimeout(3000);\n        return factory;\n    }\n\n}\n</code></pre><h3 id=\"4-6-使用消费者\"><a href=\"#4-6-使用消费者\" class=\"headerlink\" title=\"4.6 使用消费者\"></a>4.6 使用消费者</h3><pre><code>public class KafkaMessageListener implements MessageListener&lt;String, String&gt; {\n\n    private static Logger LOG = LoggerFactory.getLogger(KafkaMessageListener.class);\n    @Autowired\n    private AppProperties appProperties;\n\n    @Override\n    public void onMessage(ConsumerRecord&lt;String, String&gt; data) {\n        LOG.info(&quot;消费消息topic：{} value {}&quot;, data.topic(), data.value());\n        String topic = data.topic();\n        String content = data.value();\n        //可同时监听多个topic，根据不同topic处理不同的业务\n        if (topic.equals(&quot;topica&quot;)) {           \n            LOG.info(&quot;###############topic:{} value:{}&quot; ,topic,content);\n        } else if (topic.equals(&quot;topicb&quot;)) {\n         LOG.info(&quot;###############topic:{} value:{}&quot; ,topic,content);\n        } \n    }\n}\n</code></pre><h3 id=\"4-7-注意\"><a href=\"#4-7-注意\" class=\"headerlink\" title=\"4.7 注意\"></a>4.7 注意</h3><pre><code>kafkaTemplate.send(&quot;kafka-topic-test&quot;, &quot;helloWorld&quot;);\n@KafkaListener(topics = {&quot;kafka-topic-test&quot;})\ntopic需要对应\n</code></pre><h4 id=\"4-8-使用\"><a href=\"#4-8-使用\" class=\"headerlink\" title=\"4.8 使用\"></a>4.8 使用</h4><p>&emsp;本地运行以后，到kafka服务器上可以进行消费者和生产者的模拟发送与接收信息。</p>\n<h2 id=\"五、总结\"><a href=\"#五、总结\" class=\"headerlink\" title=\"五、总结\"></a>五、总结</h2><p>&emsp;上述方法进行模拟测试，可以测试，但是总感觉问题很大，却又找不出问题，这个后期再说吧，先凑合用。<br>&emsp;有关Kafka的具体学习，后期补上。</p>\n<h2 id=\"六、相关链接\"><a href=\"#六、相关链接\" class=\"headerlink\" title=\"六、相关链接\"></a>六、相关链接</h2><p>感谢各位大佬：</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/yangxiaoyi/p/7359236.html\" target=\"_blank\" rel=\"noopener\">kafka 基础知识梳理</a></li>\n<li><a href=\"https://www.cnblogs.com/hei12138/p/7805475.html\" target=\"_blank\" rel=\"noopener\">kafka实战</a></li>\n<li><a href=\"http://blog.51cto.com/xpleaf/2090847\" target=\"_blank\" rel=\"noopener\">kafka笔记整理</a></li>\n<li><a href=\"https://www.cnblogs.com/yepei/p/6197236.html\" target=\"_blank\" rel=\"noopener\">kafka介绍</a></li>\n</ul>"},{"title":"kafka一个服务配置到不同topic的不同group","abbrlink":"b033fd85","date":"2019-02-27T04:45:33.000Z","_content":"&emsp;标题比较长，实在想不出什么好的描述。大概要解决的问题就是，同一个服务同时监听多个topic，且在每个topic中的group都不相同，具体看问题描述吧。\n<!--more-->\n## 一、问题背景\n&emsp;前几天部署了一套系统，每个服务都搭建了多个节点，而且是没有主从关系的节点。每个服务中有很多东西是放到缓存中的，配置多节点之后，相同服务的不同节点出现了缓存不一致的问题。  \n\n## 二、问题描述\n&emsp;刚开始想出一种解决方案，监听同一个topic1，每个节点分到一个group中，这样每次生产者生产消息后，kafka会将消息分发到所有group中，消息中带一个消息类型字段（mq_type）。  \n各个节点由于处于不同group中都会消费此消息，然后根据mq_type判断是否该处理此消息。  \n&emsp;然而，pass。原因:由于此系统（系统B）中的服务1还与系统A有消费与生产消息的关系，都放到一个topic下数据不规范。而且如果多个服务1同时消费消息，会进行读表改表操作，还得做处理。\n\n&emsp;emmm，又想出了一种解决方案，系统B中每个节点还是分到不同的group中，当某个服务1消费到系统A发送的消息，需要刷新缓存时，该节点对所有节点通过系统B内部的消息队列topic2进行广播，各个服务接收到消费消息后根据消息类型进行缓存的更新。  \n具体系统图如下：\n![系统图](https://github.com/olicity-wong/Resource/blob/master/system.png?raw=true)\n[图片备用链接](https://github.com/olicity-wong/Resource/blob/master/system.png?raw=true)  \n&emsp;**ps：以上区分两个topic是为了规范来自不同的渠道的数据走不同的topic，如果没有这种要求完全没有必要做如下这种操作，可以直接通过group和消息内容去做区分**\n&emsp;如上图，系统A通过topic1向系统B中的服务1发送消息，系统B中服务1和服务2以及他们的其他节点在系统B中通过topic2发送消息。  \n&emsp;可以看出，系统B中的服务1扮演了三个角色：系统A发送消息的消费者，系统B内部消息的生产者和消费者。可以得出如下问题：\n```\n对于服务1，需要将其配置为监听两个topic，分别监听topic1和topic2\n系统A向系统B发送消息时，服务1以及他的其他节点处于topic1的同一个group下，即只有一个服务1节点会去消费系统A发来的消息\n系统B内部之间发送消息时，每个服务和节点都处于topic2的不同group下\n```\n&emsp;说到这里，其实就清楚很多了。其实就是想让服务1-1和他的其他节点在topic1中都处于group-A2B中，服务1-1在topic2中处于group-service1-1中，服务1-2在topic2中处于group-service1-2中。\n\n## 三、需求实现\n### 3.1 代码基础\n&emsp;kafka的基础代码请参照我的以下两篇博客,本次修改都是基于这些代码的基础上改造的  \n&emsp;[Kafka及Spring&Kafka整合](https://olicity-wong.github.io/post/85675a3e.html)  \n&emsp;[kafka动态配置topic](https://olicity-wong.github.io/post/bf5e9970.html)\n### 3.2 生产者\n&emsp;kafka生产者发送消息时，会向该topic下的所有group发送消息，而每个group只会有一个消费者进行消费。所以生产者不用进行更改。\n### 3.3 消费者\n#### 3.3.1 消费者的配置\n&emsp;以下消费者以服务1-1为例，其他节点服务同理。  \n&emsp;由于同一个服务要扮演两个消费者，所以我们需要不同的配置文件用来生成不同的消费者\n```\n//首先是获取公共配置方法\n    public Map<String, Object> getCommonPropertis(){\n        Map<String, Object> props = new HashMap<>();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);\n        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, \"100\");\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(\"auto.offset.reset\", \"latest\");// 一般配置earliest 或者latest 值\n        return props;\n    }\n\n//然后不同的用来生成不同消费者的工厂\n    //topic1的消费者\n    public ConsumerFactory<String, String> consumerFactoryA2B() {\n        Map<String, Object> properties = getCommonPropertis();\n        //所在group\n        properties.put(ConsumerConfig.GROUP_ID_CONFIG, \"group-A2B\");\n        return new DefaultKafkaConsumerFactory<String, String>(properties);\n    }\n    \n    \n    //系统B内topic2的每个服务的group我这里用服务名+ip+端口命名\n    String GROUP_NAME = \"service1-1-\"+serviceInfoUtil.getIpAddress()+\"-\"+serviceInfoUtil.getLocalPort();\n    \n    //topic2的消费者\n    public ConsumerFactory<String, String> consumerFactoryB2B(){\n            Map<String, Object> properties = getCommonPropertis();\n            //所在group\n            properties.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_NAME);\n            return new DefaultKafkaConsumerFactory<String, String>(properties);\n    }\n    \n//再通过不同的配置工厂生成实例bean\n    //topic1的消费者bean\n    @Bean\n    public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, String>> kafkaListenerContainerFactoryA2B() {\n        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();\n        factory.setConsumerFactory(consumerFactoryA2B());//通过不同工厂获取实例\n        factory.setConcurrency(3);\n        factory.getContainerProperties().setPollTimeout(3000);\n        return factory;\n    }  \n    \n    //topic2的消费者bean\n    @Bean\n    public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, String>> kafkaListenerContainerFactoryB2B() {\n        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();\n        factory.setConsumerFactory(consumerFactoryB2B());//通过不同工厂获取实例\n        factory.setConcurrency(3);\n        factory.getContainerProperties().setPollTimeout(3000);\n        return factory;\n    }  \n    \n```\n#### 3.3.2 消费者的使用  \n&emsp;以上消费者的配置就算完成了，接下来就可以直接使用了。\n```\n     /**\n     * 监听B2B所有消息\n     * @param record\n     */\n    @KafkaListener(topics = \"#{'${kafka.B2B.listener_topics}'.split(',')}\",containerFactory = \"kafkaListenerContainerFactoryB2B\")\n    public void B2Bconsume(ConsumerRecord<?, ?> record){\n        recordDeal(record);\n    }\n\n    /**\n     * 监听A2B的所有消息\n     * @param record\n     */\n    @KafkaListener(topics = \"#{'${kafka.A2B.listener_topics}'.split(',')}\",containerFactory = \"kafkaListenerContainerFactoryA2B\")\n    public void A2Bconsume(ConsumerRecord<?, ?> record) {\n        recordDeal(record);\n    }\n    \n//containerFactory = \"kafkaListenerContainerFactoryA2B\"  主要就是这个containerFactory参数，用它控制是哪个实例\n```\n#### 3.3.3 获取服务启动的ip和端口类\n```\n@Configuration\npublic class ServiceInfoUtil {\n    public static String getIpAddress() throws UnknownHostException {\n        InetAddress address = InetAddress.getLocalHost();\n        return address.getHostAddress();\n    }\n    public static String getLocalPort() throws MalformedObjectNameException {\n        MBeanServer beanServer = ManagementFactory.getPlatformMBeanServer();\n        Set<ObjectName> objectNames = beanServer.queryNames(new ObjectName(\"*:type=Connector,*\"),\n                Query.match(Query.attr(\"protocol\"), Query.value(\"HTTP/1.1\")));\n        String port = objectNames.iterator().next().getKeyProperty(\"port\");\n        return port;\n    }\n}    \n```\n#### 3.3.4 最后\n&emsp;这样修改后启动时，通过配置文件中的kafka.A2B.listener_topics去判断这个消费者该监听哪个topic，通过containerFactory = \"kafkaListenerContainerFactoryA2B\"判断这个消费者在这个topic中属于哪个group。\n然后发送消息测试，成了。\n\n## 四、感谢大佬\n这几个大佬的对于kafka的group的讲解比较好：  \n[KAFKA 多个消费者同一个GROUPID，只有一个能收到消息的原因](http://www.mrslee.cn/archives/54)  \n[Kafka消费组(consumer group)](http://www.cnblogs.com/huxi2b/p/6223228.html)  \n[springboot 集成kafka 实现多个customer不同group](https://blog.csdn.net/caijiapeng0102/article/details/80765923)  \n","source":"_posts/kafka一个服务配置到不同topic的不同group.md","raw":"---\ntitle: kafka一个服务配置到不同topic的不同group\ntags:\n  - kafka\nabbrlink: b033fd85\ndate: 2019-02-27 12:45:33\n---\n&emsp;标题比较长，实在想不出什么好的描述。大概要解决的问题就是，同一个服务同时监听多个topic，且在每个topic中的group都不相同，具体看问题描述吧。\n<!--more-->\n## 一、问题背景\n&emsp;前几天部署了一套系统，每个服务都搭建了多个节点，而且是没有主从关系的节点。每个服务中有很多东西是放到缓存中的，配置多节点之后，相同服务的不同节点出现了缓存不一致的问题。  \n\n## 二、问题描述\n&emsp;刚开始想出一种解决方案，监听同一个topic1，每个节点分到一个group中，这样每次生产者生产消息后，kafka会将消息分发到所有group中，消息中带一个消息类型字段（mq_type）。  \n各个节点由于处于不同group中都会消费此消息，然后根据mq_type判断是否该处理此消息。  \n&emsp;然而，pass。原因:由于此系统（系统B）中的服务1还与系统A有消费与生产消息的关系，都放到一个topic下数据不规范。而且如果多个服务1同时消费消息，会进行读表改表操作，还得做处理。\n\n&emsp;emmm，又想出了一种解决方案，系统B中每个节点还是分到不同的group中，当某个服务1消费到系统A发送的消息，需要刷新缓存时，该节点对所有节点通过系统B内部的消息队列topic2进行广播，各个服务接收到消费消息后根据消息类型进行缓存的更新。  \n具体系统图如下：\n![系统图](https://github.com/olicity-wong/Resource/blob/master/system.png?raw=true)\n[图片备用链接](https://github.com/olicity-wong/Resource/blob/master/system.png?raw=true)  \n&emsp;**ps：以上区分两个topic是为了规范来自不同的渠道的数据走不同的topic，如果没有这种要求完全没有必要做如下这种操作，可以直接通过group和消息内容去做区分**\n&emsp;如上图，系统A通过topic1向系统B中的服务1发送消息，系统B中服务1和服务2以及他们的其他节点在系统B中通过topic2发送消息。  \n&emsp;可以看出，系统B中的服务1扮演了三个角色：系统A发送消息的消费者，系统B内部消息的生产者和消费者。可以得出如下问题：\n```\n对于服务1，需要将其配置为监听两个topic，分别监听topic1和topic2\n系统A向系统B发送消息时，服务1以及他的其他节点处于topic1的同一个group下，即只有一个服务1节点会去消费系统A发来的消息\n系统B内部之间发送消息时，每个服务和节点都处于topic2的不同group下\n```\n&emsp;说到这里，其实就清楚很多了。其实就是想让服务1-1和他的其他节点在topic1中都处于group-A2B中，服务1-1在topic2中处于group-service1-1中，服务1-2在topic2中处于group-service1-2中。\n\n## 三、需求实现\n### 3.1 代码基础\n&emsp;kafka的基础代码请参照我的以下两篇博客,本次修改都是基于这些代码的基础上改造的  \n&emsp;[Kafka及Spring&Kafka整合](https://olicity-wong.github.io/post/85675a3e.html)  \n&emsp;[kafka动态配置topic](https://olicity-wong.github.io/post/bf5e9970.html)\n### 3.2 生产者\n&emsp;kafka生产者发送消息时，会向该topic下的所有group发送消息，而每个group只会有一个消费者进行消费。所以生产者不用进行更改。\n### 3.3 消费者\n#### 3.3.1 消费者的配置\n&emsp;以下消费者以服务1-1为例，其他节点服务同理。  \n&emsp;由于同一个服务要扮演两个消费者，所以我们需要不同的配置文件用来生成不同的消费者\n```\n//首先是获取公共配置方法\n    public Map<String, Object> getCommonPropertis(){\n        Map<String, Object> props = new HashMap<>();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);\n        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, \"100\");\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(\"auto.offset.reset\", \"latest\");// 一般配置earliest 或者latest 值\n        return props;\n    }\n\n//然后不同的用来生成不同消费者的工厂\n    //topic1的消费者\n    public ConsumerFactory<String, String> consumerFactoryA2B() {\n        Map<String, Object> properties = getCommonPropertis();\n        //所在group\n        properties.put(ConsumerConfig.GROUP_ID_CONFIG, \"group-A2B\");\n        return new DefaultKafkaConsumerFactory<String, String>(properties);\n    }\n    \n    \n    //系统B内topic2的每个服务的group我这里用服务名+ip+端口命名\n    String GROUP_NAME = \"service1-1-\"+serviceInfoUtil.getIpAddress()+\"-\"+serviceInfoUtil.getLocalPort();\n    \n    //topic2的消费者\n    public ConsumerFactory<String, String> consumerFactoryB2B(){\n            Map<String, Object> properties = getCommonPropertis();\n            //所在group\n            properties.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_NAME);\n            return new DefaultKafkaConsumerFactory<String, String>(properties);\n    }\n    \n//再通过不同的配置工厂生成实例bean\n    //topic1的消费者bean\n    @Bean\n    public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, String>> kafkaListenerContainerFactoryA2B() {\n        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();\n        factory.setConsumerFactory(consumerFactoryA2B());//通过不同工厂获取实例\n        factory.setConcurrency(3);\n        factory.getContainerProperties().setPollTimeout(3000);\n        return factory;\n    }  \n    \n    //topic2的消费者bean\n    @Bean\n    public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, String>> kafkaListenerContainerFactoryB2B() {\n        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();\n        factory.setConsumerFactory(consumerFactoryB2B());//通过不同工厂获取实例\n        factory.setConcurrency(3);\n        factory.getContainerProperties().setPollTimeout(3000);\n        return factory;\n    }  \n    \n```\n#### 3.3.2 消费者的使用  \n&emsp;以上消费者的配置就算完成了，接下来就可以直接使用了。\n```\n     /**\n     * 监听B2B所有消息\n     * @param record\n     */\n    @KafkaListener(topics = \"#{'${kafka.B2B.listener_topics}'.split(',')}\",containerFactory = \"kafkaListenerContainerFactoryB2B\")\n    public void B2Bconsume(ConsumerRecord<?, ?> record){\n        recordDeal(record);\n    }\n\n    /**\n     * 监听A2B的所有消息\n     * @param record\n     */\n    @KafkaListener(topics = \"#{'${kafka.A2B.listener_topics}'.split(',')}\",containerFactory = \"kafkaListenerContainerFactoryA2B\")\n    public void A2Bconsume(ConsumerRecord<?, ?> record) {\n        recordDeal(record);\n    }\n    \n//containerFactory = \"kafkaListenerContainerFactoryA2B\"  主要就是这个containerFactory参数，用它控制是哪个实例\n```\n#### 3.3.3 获取服务启动的ip和端口类\n```\n@Configuration\npublic class ServiceInfoUtil {\n    public static String getIpAddress() throws UnknownHostException {\n        InetAddress address = InetAddress.getLocalHost();\n        return address.getHostAddress();\n    }\n    public static String getLocalPort() throws MalformedObjectNameException {\n        MBeanServer beanServer = ManagementFactory.getPlatformMBeanServer();\n        Set<ObjectName> objectNames = beanServer.queryNames(new ObjectName(\"*:type=Connector,*\"),\n                Query.match(Query.attr(\"protocol\"), Query.value(\"HTTP/1.1\")));\n        String port = objectNames.iterator().next().getKeyProperty(\"port\");\n        return port;\n    }\n}    \n```\n#### 3.3.4 最后\n&emsp;这样修改后启动时，通过配置文件中的kafka.A2B.listener_topics去判断这个消费者该监听哪个topic，通过containerFactory = \"kafkaListenerContainerFactoryA2B\"判断这个消费者在这个topic中属于哪个group。\n然后发送消息测试，成了。\n\n## 四、感谢大佬\n这几个大佬的对于kafka的group的讲解比较好：  \n[KAFKA 多个消费者同一个GROUPID，只有一个能收到消息的原因](http://www.mrslee.cn/archives/54)  \n[Kafka消费组(consumer group)](http://www.cnblogs.com/huxi2b/p/6223228.html)  \n[springboot 集成kafka 实现多个customer不同group](https://blog.csdn.net/caijiapeng0102/article/details/80765923)  \n","slug":"kafka一个服务配置到不同topic的不同group","published":1,"updated":"2019-09-18T13:44:05.974Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0q92d1g00117guo2w2b6omj","content":"<p>&emsp;标题比较长，实在想不出什么好的描述。大概要解决的问题就是，同一个服务同时监听多个topic，且在每个topic中的group都不相同，具体看问题描述吧。<br><a id=\"more\"></a></p>\n<h2 id=\"一、问题背景\"><a href=\"#一、问题背景\" class=\"headerlink\" title=\"一、问题背景\"></a>一、问题背景</h2><p>&emsp;前几天部署了一套系统，每个服务都搭建了多个节点，而且是没有主从关系的节点。每个服务中有很多东西是放到缓存中的，配置多节点之后，相同服务的不同节点出现了缓存不一致的问题。  </p>\n<h2 id=\"二、问题描述\"><a href=\"#二、问题描述\" class=\"headerlink\" title=\"二、问题描述\"></a>二、问题描述</h2><p>&emsp;刚开始想出一种解决方案，监听同一个topic1，每个节点分到一个group中，这样每次生产者生产消息后，kafka会将消息分发到所有group中，消息中带一个消息类型字段（mq_type）。<br>各个节点由于处于不同group中都会消费此消息，然后根据mq_type判断是否该处理此消息。<br>&emsp;然而，pass。原因:由于此系统（系统B）中的服务1还与系统A有消费与生产消息的关系，都放到一个topic下数据不规范。而且如果多个服务1同时消费消息，会进行读表改表操作，还得做处理。</p>\n<p>&emsp;emmm，又想出了一种解决方案，系统B中每个节点还是分到不同的group中，当某个服务1消费到系统A发送的消息，需要刷新缓存时，该节点对所有节点通过系统B内部的消息队列topic2进行广播，各个服务接收到消费消息后根据消息类型进行缓存的更新。<br>具体系统图如下：<br><img src=\"https://github.com/olicity-wong/Resource/blob/master/system.png?raw=true\" alt=\"系统图\"><br><a href=\"https://github.com/olicity-wong/Resource/blob/master/system.png?raw=true\" target=\"_blank\" rel=\"noopener\">图片备用链接</a><br>&emsp;<strong>ps：以上区分两个topic是为了规范来自不同的渠道的数据走不同的topic，如果没有这种要求完全没有必要做如下这种操作，可以直接通过group和消息内容去做区分</strong><br>&emsp;如上图，系统A通过topic1向系统B中的服务1发送消息，系统B中服务1和服务2以及他们的其他节点在系统B中通过topic2发送消息。<br>&emsp;可以看出，系统B中的服务1扮演了三个角色：系统A发送消息的消费者，系统B内部消息的生产者和消费者。可以得出如下问题：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">对于服务1，需要将其配置为监听两个topic，分别监听topic1和topic2</span><br><span class=\"line\">系统A向系统B发送消息时，服务1以及他的其他节点处于topic1的同一个group下，即只有一个服务1节点会去消费系统A发来的消息</span><br><span class=\"line\">系统B内部之间发送消息时，每个服务和节点都处于topic2的不同group下</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;说到这里，其实就清楚很多了。其实就是想让服务1-1和他的其他节点在topic1中都处于group-A2B中，服务1-1在topic2中处于group-service1-1中，服务1-2在topic2中处于group-service1-2中。</p>\n<h2 id=\"三、需求实现\"><a href=\"#三、需求实现\" class=\"headerlink\" title=\"三、需求实现\"></a>三、需求实现</h2><h3 id=\"3-1-代码基础\"><a href=\"#3-1-代码基础\" class=\"headerlink\" title=\"3.1 代码基础\"></a>3.1 代码基础</h3><p>&emsp;kafka的基础代码请参照我的以下两篇博客,本次修改都是基于这些代码的基础上改造的<br>&emsp;<a href=\"https://olicity-wong.github.io/post/85675a3e.html\">Kafka及Spring&amp;Kafka整合</a><br>&emsp;<a href=\"https://olicity-wong.github.io/post/bf5e9970.html\">kafka动态配置topic</a></p>\n<h3 id=\"3-2-生产者\"><a href=\"#3-2-生产者\" class=\"headerlink\" title=\"3.2 生产者\"></a>3.2 生产者</h3><p>&emsp;kafka生产者发送消息时，会向该topic下的所有group发送消息，而每个group只会有一个消费者进行消费。所以生产者不用进行更改。</p>\n<h3 id=\"3-3-消费者\"><a href=\"#3-3-消费者\" class=\"headerlink\" title=\"3.3 消费者\"></a>3.3 消费者</h3><h4 id=\"3-3-1-消费者的配置\"><a href=\"#3-3-1-消费者的配置\" class=\"headerlink\" title=\"3.3.1 消费者的配置\"></a>3.3.1 消费者的配置</h4><p>&emsp;以下消费者以服务1-1为例，其他节点服务同理。<br>&emsp;由于同一个服务要扮演两个消费者，所以我们需要不同的配置文件用来生成不同的消费者<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//首先是获取公共配置方法</span><br><span class=\"line\">    public Map&lt;String, Object&gt; getCommonPropertis()&#123;</span><br><span class=\"line\">        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();</span><br><span class=\"line\">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);</span><br><span class=\"line\">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);</span><br><span class=\"line\">        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, &quot;100&quot;);</span><br><span class=\"line\">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">        props.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);// 一般配置earliest 或者latest 值</span><br><span class=\"line\">        return props;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">//然后不同的用来生成不同消费者的工厂</span><br><span class=\"line\">    //topic1的消费者</span><br><span class=\"line\">    public ConsumerFactory&lt;String, String&gt; consumerFactoryA2B() &#123;</span><br><span class=\"line\">        Map&lt;String, Object&gt; properties = getCommonPropertis();</span><br><span class=\"line\">        //所在group</span><br><span class=\"line\">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;group-A2B&quot;);</span><br><span class=\"line\">        return new DefaultKafkaConsumerFactory&lt;String, String&gt;(properties);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">    //系统B内topic2的每个服务的group我这里用服务名+ip+端口命名</span><br><span class=\"line\">    String GROUP_NAME = &quot;service1-1-&quot;+serviceInfoUtil.getIpAddress()+&quot;-&quot;+serviceInfoUtil.getLocalPort();</span><br><span class=\"line\">    </span><br><span class=\"line\">    //topic2的消费者</span><br><span class=\"line\">    public ConsumerFactory&lt;String, String&gt; consumerFactoryB2B()&#123;</span><br><span class=\"line\">            Map&lt;String, Object&gt; properties = getCommonPropertis();</span><br><span class=\"line\">            //所在group</span><br><span class=\"line\">            properties.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_NAME);</span><br><span class=\"line\">            return new DefaultKafkaConsumerFactory&lt;String, String&gt;(properties);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">//再通过不同的配置工厂生成实例bean</span><br><span class=\"line\">    //topic1的消费者bean</span><br><span class=\"line\">    @Bean</span><br><span class=\"line\">    public KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;String, String&gt;&gt; kafkaListenerContainerFactoryA2B() &#123;</span><br><span class=\"line\">        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();</span><br><span class=\"line\">        factory.setConsumerFactory(consumerFactoryA2B());//通过不同工厂获取实例</span><br><span class=\"line\">        factory.setConcurrency(3);</span><br><span class=\"line\">        factory.getContainerProperties().setPollTimeout(3000);</span><br><span class=\"line\">        return factory;</span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    </span><br><span class=\"line\">    //topic2的消费者bean</span><br><span class=\"line\">    @Bean</span><br><span class=\"line\">    public KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;String, String&gt;&gt; kafkaListenerContainerFactoryB2B() &#123;</span><br><span class=\"line\">        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();</span><br><span class=\"line\">        factory.setConsumerFactory(consumerFactoryB2B());//通过不同工厂获取实例</span><br><span class=\"line\">        factory.setConcurrency(3);</span><br><span class=\"line\">        factory.getContainerProperties().setPollTimeout(3000);</span><br><span class=\"line\">        return factory;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"3-3-2-消费者的使用\"><a href=\"#3-3-2-消费者的使用\" class=\"headerlink\" title=\"3.3.2 消费者的使用\"></a>3.3.2 消费者的使用</h4><p>&emsp;以上消费者的配置就算完成了，接下来就可以直接使用了。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">     /**</span><br><span class=\"line\">     * 监听B2B所有消息</span><br><span class=\"line\">     * @param record</span><br><span class=\"line\">     */</span><br><span class=\"line\">    @KafkaListener(topics = &quot;#&#123;&apos;$&#123;kafka.B2B.listener_topics&#125;&apos;.split(&apos;,&apos;)&#125;&quot;,containerFactory = &quot;kafkaListenerContainerFactoryB2B&quot;)</span><br><span class=\"line\">    public void B2Bconsume(ConsumerRecord&lt;?, ?&gt; record)&#123;</span><br><span class=\"line\">        recordDeal(record);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    /**</span><br><span class=\"line\">     * 监听A2B的所有消息</span><br><span class=\"line\">     * @param record</span><br><span class=\"line\">     */</span><br><span class=\"line\">    @KafkaListener(topics = &quot;#&#123;&apos;$&#123;kafka.A2B.listener_topics&#125;&apos;.split(&apos;,&apos;)&#125;&quot;,containerFactory = &quot;kafkaListenerContainerFactoryA2B&quot;)</span><br><span class=\"line\">    public void A2Bconsume(ConsumerRecord&lt;?, ?&gt; record) &#123;</span><br><span class=\"line\">        recordDeal(record);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">//containerFactory = &quot;kafkaListenerContainerFactoryA2B&quot;  主要就是这个containerFactory参数，用它控制是哪个实例</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"3-3-3-获取服务启动的ip和端口类\"><a href=\"#3-3-3-获取服务启动的ip和端口类\" class=\"headerlink\" title=\"3.3.3 获取服务启动的ip和端口类\"></a>3.3.3 获取服务启动的ip和端口类</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Configuration</span><br><span class=\"line\">public class ServiceInfoUtil &#123;</span><br><span class=\"line\">    public static String getIpAddress() throws UnknownHostException &#123;</span><br><span class=\"line\">        InetAddress address = InetAddress.getLocalHost();</span><br><span class=\"line\">        return address.getHostAddress();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    public static String getLocalPort() throws MalformedObjectNameException &#123;</span><br><span class=\"line\">        MBeanServer beanServer = ManagementFactory.getPlatformMBeanServer();</span><br><span class=\"line\">        Set&lt;ObjectName&gt; objectNames = beanServer.queryNames(new ObjectName(&quot;*:type=Connector,*&quot;),</span><br><span class=\"line\">                Query.match(Query.attr(&quot;protocol&quot;), Query.value(&quot;HTTP/1.1&quot;)));</span><br><span class=\"line\">        String port = objectNames.iterator().next().getKeyProperty(&quot;port&quot;);</span><br><span class=\"line\">        return port;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-3-4-最后\"><a href=\"#3-3-4-最后\" class=\"headerlink\" title=\"3.3.4 最后\"></a>3.3.4 最后</h4><p>&emsp;这样修改后启动时，通过配置文件中的kafka.A2B.listener_topics去判断这个消费者该监听哪个topic，通过containerFactory = “kafkaListenerContainerFactoryA2B”判断这个消费者在这个topic中属于哪个group。<br>然后发送消息测试，成了。</p>\n<h2 id=\"四、感谢大佬\"><a href=\"#四、感谢大佬\" class=\"headerlink\" title=\"四、感谢大佬\"></a>四、感谢大佬</h2><p>这几个大佬的对于kafka的group的讲解比较好：<br><a href=\"http://www.mrslee.cn/archives/54\" target=\"_blank\" rel=\"noopener\">KAFKA 多个消费者同一个GROUPID，只有一个能收到消息的原因</a><br><a href=\"http://www.cnblogs.com/huxi2b/p/6223228.html\" target=\"_blank\" rel=\"noopener\">Kafka消费组(consumer group)</a><br><a href=\"https://blog.csdn.net/caijiapeng0102/article/details/80765923\" target=\"_blank\" rel=\"noopener\">springboot 集成kafka 实现多个customer不同group</a>  </p>\n","site":{"data":{}},"excerpt":"<p>&emsp;标题比较长，实在想不出什么好的描述。大概要解决的问题就是，同一个服务同时监听多个topic，且在每个topic中的group都不相同，具体看问题描述吧。<br>","more":"</p>\n<h2 id=\"一、问题背景\"><a href=\"#一、问题背景\" class=\"headerlink\" title=\"一、问题背景\"></a>一、问题背景</h2><p>&emsp;前几天部署了一套系统，每个服务都搭建了多个节点，而且是没有主从关系的节点。每个服务中有很多东西是放到缓存中的，配置多节点之后，相同服务的不同节点出现了缓存不一致的问题。  </p>\n<h2 id=\"二、问题描述\"><a href=\"#二、问题描述\" class=\"headerlink\" title=\"二、问题描述\"></a>二、问题描述</h2><p>&emsp;刚开始想出一种解决方案，监听同一个topic1，每个节点分到一个group中，这样每次生产者生产消息后，kafka会将消息分发到所有group中，消息中带一个消息类型字段（mq_type）。<br>各个节点由于处于不同group中都会消费此消息，然后根据mq_type判断是否该处理此消息。<br>&emsp;然而，pass。原因:由于此系统（系统B）中的服务1还与系统A有消费与生产消息的关系，都放到一个topic下数据不规范。而且如果多个服务1同时消费消息，会进行读表改表操作，还得做处理。</p>\n<p>&emsp;emmm，又想出了一种解决方案，系统B中每个节点还是分到不同的group中，当某个服务1消费到系统A发送的消息，需要刷新缓存时，该节点对所有节点通过系统B内部的消息队列topic2进行广播，各个服务接收到消费消息后根据消息类型进行缓存的更新。<br>具体系统图如下：<br><img src=\"https://github.com/olicity-wong/Resource/blob/master/system.png?raw=true\" alt=\"系统图\"><br><a href=\"https://github.com/olicity-wong/Resource/blob/master/system.png?raw=true\" target=\"_blank\" rel=\"noopener\">图片备用链接</a><br>&emsp;<strong>ps：以上区分两个topic是为了规范来自不同的渠道的数据走不同的topic，如果没有这种要求完全没有必要做如下这种操作，可以直接通过group和消息内容去做区分</strong><br>&emsp;如上图，系统A通过topic1向系统B中的服务1发送消息，系统B中服务1和服务2以及他们的其他节点在系统B中通过topic2发送消息。<br>&emsp;可以看出，系统B中的服务1扮演了三个角色：系统A发送消息的消费者，系统B内部消息的生产者和消费者。可以得出如下问题：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">对于服务1，需要将其配置为监听两个topic，分别监听topic1和topic2</span><br><span class=\"line\">系统A向系统B发送消息时，服务1以及他的其他节点处于topic1的同一个group下，即只有一个服务1节点会去消费系统A发来的消息</span><br><span class=\"line\">系统B内部之间发送消息时，每个服务和节点都处于topic2的不同group下</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;说到这里，其实就清楚很多了。其实就是想让服务1-1和他的其他节点在topic1中都处于group-A2B中，服务1-1在topic2中处于group-service1-1中，服务1-2在topic2中处于group-service1-2中。</p>\n<h2 id=\"三、需求实现\"><a href=\"#三、需求实现\" class=\"headerlink\" title=\"三、需求实现\"></a>三、需求实现</h2><h3 id=\"3-1-代码基础\"><a href=\"#3-1-代码基础\" class=\"headerlink\" title=\"3.1 代码基础\"></a>3.1 代码基础</h3><p>&emsp;kafka的基础代码请参照我的以下两篇博客,本次修改都是基于这些代码的基础上改造的<br>&emsp;<a href=\"https://olicity-wong.github.io/post/85675a3e.html\">Kafka及Spring&amp;Kafka整合</a><br>&emsp;<a href=\"https://olicity-wong.github.io/post/bf5e9970.html\">kafka动态配置topic</a></p>\n<h3 id=\"3-2-生产者\"><a href=\"#3-2-生产者\" class=\"headerlink\" title=\"3.2 生产者\"></a>3.2 生产者</h3><p>&emsp;kafka生产者发送消息时，会向该topic下的所有group发送消息，而每个group只会有一个消费者进行消费。所以生产者不用进行更改。</p>\n<h3 id=\"3-3-消费者\"><a href=\"#3-3-消费者\" class=\"headerlink\" title=\"3.3 消费者\"></a>3.3 消费者</h3><h4 id=\"3-3-1-消费者的配置\"><a href=\"#3-3-1-消费者的配置\" class=\"headerlink\" title=\"3.3.1 消费者的配置\"></a>3.3.1 消费者的配置</h4><p>&emsp;以下消费者以服务1-1为例，其他节点服务同理。<br>&emsp;由于同一个服务要扮演两个消费者，所以我们需要不同的配置文件用来生成不同的消费者<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//首先是获取公共配置方法</span><br><span class=\"line\">    public Map&lt;String, Object&gt; getCommonPropertis()&#123;</span><br><span class=\"line\">        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();</span><br><span class=\"line\">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);</span><br><span class=\"line\">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);</span><br><span class=\"line\">        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, &quot;100&quot;);</span><br><span class=\"line\">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">        props.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);// 一般配置earliest 或者latest 值</span><br><span class=\"line\">        return props;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">//然后不同的用来生成不同消费者的工厂</span><br><span class=\"line\">    //topic1的消费者</span><br><span class=\"line\">    public ConsumerFactory&lt;String, String&gt; consumerFactoryA2B() &#123;</span><br><span class=\"line\">        Map&lt;String, Object&gt; properties = getCommonPropertis();</span><br><span class=\"line\">        //所在group</span><br><span class=\"line\">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;group-A2B&quot;);</span><br><span class=\"line\">        return new DefaultKafkaConsumerFactory&lt;String, String&gt;(properties);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">    //系统B内topic2的每个服务的group我这里用服务名+ip+端口命名</span><br><span class=\"line\">    String GROUP_NAME = &quot;service1-1-&quot;+serviceInfoUtil.getIpAddress()+&quot;-&quot;+serviceInfoUtil.getLocalPort();</span><br><span class=\"line\">    </span><br><span class=\"line\">    //topic2的消费者</span><br><span class=\"line\">    public ConsumerFactory&lt;String, String&gt; consumerFactoryB2B()&#123;</span><br><span class=\"line\">            Map&lt;String, Object&gt; properties = getCommonPropertis();</span><br><span class=\"line\">            //所在group</span><br><span class=\"line\">            properties.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_NAME);</span><br><span class=\"line\">            return new DefaultKafkaConsumerFactory&lt;String, String&gt;(properties);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">//再通过不同的配置工厂生成实例bean</span><br><span class=\"line\">    //topic1的消费者bean</span><br><span class=\"line\">    @Bean</span><br><span class=\"line\">    public KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;String, String&gt;&gt; kafkaListenerContainerFactoryA2B() &#123;</span><br><span class=\"line\">        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();</span><br><span class=\"line\">        factory.setConsumerFactory(consumerFactoryA2B());//通过不同工厂获取实例</span><br><span class=\"line\">        factory.setConcurrency(3);</span><br><span class=\"line\">        factory.getContainerProperties().setPollTimeout(3000);</span><br><span class=\"line\">        return factory;</span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    </span><br><span class=\"line\">    //topic2的消费者bean</span><br><span class=\"line\">    @Bean</span><br><span class=\"line\">    public KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;String, String&gt;&gt; kafkaListenerContainerFactoryB2B() &#123;</span><br><span class=\"line\">        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();</span><br><span class=\"line\">        factory.setConsumerFactory(consumerFactoryB2B());//通过不同工厂获取实例</span><br><span class=\"line\">        factory.setConcurrency(3);</span><br><span class=\"line\">        factory.getContainerProperties().setPollTimeout(3000);</span><br><span class=\"line\">        return factory;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"3-3-2-消费者的使用\"><a href=\"#3-3-2-消费者的使用\" class=\"headerlink\" title=\"3.3.2 消费者的使用\"></a>3.3.2 消费者的使用</h4><p>&emsp;以上消费者的配置就算完成了，接下来就可以直接使用了。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">     /**</span><br><span class=\"line\">     * 监听B2B所有消息</span><br><span class=\"line\">     * @param record</span><br><span class=\"line\">     */</span><br><span class=\"line\">    @KafkaListener(topics = &quot;#&#123;&apos;$&#123;kafka.B2B.listener_topics&#125;&apos;.split(&apos;,&apos;)&#125;&quot;,containerFactory = &quot;kafkaListenerContainerFactoryB2B&quot;)</span><br><span class=\"line\">    public void B2Bconsume(ConsumerRecord&lt;?, ?&gt; record)&#123;</span><br><span class=\"line\">        recordDeal(record);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    /**</span><br><span class=\"line\">     * 监听A2B的所有消息</span><br><span class=\"line\">     * @param record</span><br><span class=\"line\">     */</span><br><span class=\"line\">    @KafkaListener(topics = &quot;#&#123;&apos;$&#123;kafka.A2B.listener_topics&#125;&apos;.split(&apos;,&apos;)&#125;&quot;,containerFactory = &quot;kafkaListenerContainerFactoryA2B&quot;)</span><br><span class=\"line\">    public void A2Bconsume(ConsumerRecord&lt;?, ?&gt; record) &#123;</span><br><span class=\"line\">        recordDeal(record);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">//containerFactory = &quot;kafkaListenerContainerFactoryA2B&quot;  主要就是这个containerFactory参数，用它控制是哪个实例</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"3-3-3-获取服务启动的ip和端口类\"><a href=\"#3-3-3-获取服务启动的ip和端口类\" class=\"headerlink\" title=\"3.3.3 获取服务启动的ip和端口类\"></a>3.3.3 获取服务启动的ip和端口类</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Configuration</span><br><span class=\"line\">public class ServiceInfoUtil &#123;</span><br><span class=\"line\">    public static String getIpAddress() throws UnknownHostException &#123;</span><br><span class=\"line\">        InetAddress address = InetAddress.getLocalHost();</span><br><span class=\"line\">        return address.getHostAddress();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    public static String getLocalPort() throws MalformedObjectNameException &#123;</span><br><span class=\"line\">        MBeanServer beanServer = ManagementFactory.getPlatformMBeanServer();</span><br><span class=\"line\">        Set&lt;ObjectName&gt; objectNames = beanServer.queryNames(new ObjectName(&quot;*:type=Connector,*&quot;),</span><br><span class=\"line\">                Query.match(Query.attr(&quot;protocol&quot;), Query.value(&quot;HTTP/1.1&quot;)));</span><br><span class=\"line\">        String port = objectNames.iterator().next().getKeyProperty(&quot;port&quot;);</span><br><span class=\"line\">        return port;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-3-4-最后\"><a href=\"#3-3-4-最后\" class=\"headerlink\" title=\"3.3.4 最后\"></a>3.3.4 最后</h4><p>&emsp;这样修改后启动时，通过配置文件中的kafka.A2B.listener_topics去判断这个消费者该监听哪个topic，通过containerFactory = “kafkaListenerContainerFactoryA2B”判断这个消费者在这个topic中属于哪个group。<br>然后发送消息测试，成了。</p>\n<h2 id=\"四、感谢大佬\"><a href=\"#四、感谢大佬\" class=\"headerlink\" title=\"四、感谢大佬\"></a>四、感谢大佬</h2><p>这几个大佬的对于kafka的group的讲解比较好：<br><a href=\"http://www.mrslee.cn/archives/54\" target=\"_blank\" rel=\"noopener\">KAFKA 多个消费者同一个GROUPID，只有一个能收到消息的原因</a><br><a href=\"http://www.cnblogs.com/huxi2b/p/6223228.html\" target=\"_blank\" rel=\"noopener\">Kafka消费组(consumer group)</a><br><a href=\"https://blog.csdn.net/caijiapeng0102/article/details/80765923\" target=\"_blank\" rel=\"noopener\">springboot 集成kafka 实现多个customer不同group</a>  </p>"},{"title":"spring基础学习记录","abbrlink":"db474090","date":"2018-09-11T13:22:50.000Z","_content":"&emsp;最近在做后端的开发，用到了spring，借这个机会给spring捋一遍吧。东西有点多，需要点时间，抽空学。记录(一)主要是写一点spring最基础的IOC和AOP,至于spring的深入学习、各种注解和SpingMVC另开贴。  \n<!--more-->\n## 一、Spring概述\n### 1.简介\n&emsp;Spring是一个基于**控制反转（IOC）**和**面向切面（AOP）**的结构J2EE系统的框架。从简单性、可测试性和松耦合的角度而言，任何Java应用都可以从Spring中受益。  \n&emsp;控制反转(IOC)是一个通用概念，Spring最认同的技术是控制反转的**依赖注入（DI）**模式。简单地说就是拿到的对象的属性，已经被注入好相关值了，直接使用即可。\n&emsp;面向切面编程（AOP），一个程序中跨越多个点的功能被称为横切关注点，这些横切关注点在概念上独立于应用程序的业务逻辑。Spring 框架的 AOP 模块提供了面向方面的程序设计实现，可以定义诸如方法拦截器和切入点等，从而使实现功能的代码彻底的解耦出来。  \n### 2.Spring关键策略\n```\n* 基于POJO的轻量级和最小侵入性编程\n* 通过依赖注入和面向接口实现松耦合\n* 基于切面和惯例进行声明式编程\n* 通过切面和模板减少样板式代码\n```\n### 3.Spring优点\n```\n*方便解耦，简化开发 （高内聚低耦合） \n    Spring就是一个大工厂（容器），可以将所有对象创建和依赖关系维护，交给Spring管理 \n    spring工厂是用于生成bean\n*AOP编程的支持 \n    Spring提供面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等功能\n    声明式事务的支持 \n    只需要通过配置就可以完成对事务的管理，而无需手动编程\n*方便程序的测试 \n    Spring对Junit4支持，可以通过注解方便的测试Spring程序\n*方便集成各种优秀框架 \n    Spring不排斥各种优秀的开源框架，其内部提供了对各种优秀框架（如：Struts、Hibernate、MyBatis、Quartz等）的直接支持\n*降低JavaEE API的使用难度 \n    Spring 对JavaEE开发中非常难用的一些API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低\n```\n## 二、控制反转（IOC）\n### 1.概述\n&emsp;代码之间的**耦合性**具有两面性。耦合是必须的，但应小心谨慎的管理。  \n&emsp;DI会将所以来的关系自动交给目标对象，而不是让对象自己去获取依赖。–**松耦合**\n### 2.获取对象的方式进行比较\n传统方式：\n&emsp;通过new关键字主动创建一个对象。  \nIOC方式：\n&emsp;对象的生命周期由Spring来管理，直接从Spring那里去获取一个对象。 IOC是反转控制 (Inversion Of Control)的缩写，就像控制权从本来在自己手里，交给了Spring。  \n### 3.装配\n&emsp;创建应用组件之间协作的行为通常称为装配。常用XML装配方式。Spring配置文件。\n```\n*使用构造器进行配置\n    <bean> 定义JavaBean，配置需要创建的对象\n    id/name ：用于之后从spring容器获得实例时使用的\n    class ：需要创建实例的全限定类名\n\n\n    <bean id=\"c\" class=\"pojo.Category\">\n        <property name=\"name\" value=\"category 1\" /> <!--category对象-->\n    </bean>\n\n    <!--创建product对象的时候注入了一个category对象，使用ref-->\n    <bean name=\"p\" class=\"pojo.Product\"> <!--product类中添加category对象的set&get方法-->\n        <property name=\"name\" value=\"product1\" />   <!--product对象-->\n        <property name=\"category\" ref=\"c\" />    <!--为product对象注入category对象-->\n    </bean>\n\n*使用注解的方式进行配置\n    xml中添加  <context:annotation-config/> 注释掉注入category\n        *@Autowired注解方法\n            在Product.java的category属性前加上@Autowired注解\n            @Autowired\n            private Category category;\n            或者\n            @Autowired\n            public void setCategory(Category category) \n        *@Resource\n            @Resource(name=\"c\")\n            private Category category;\n\n*对bean进行注解配置\n    直接xml中添加  <context:component-scan base-package=\"pojo\"/>\n    就是告知spring，所有的bean都在pojo包下\n    通过@Component注解\n        在类前加入  @Component(\"c\")  表明此类是bean\n        因为配置从applicationContext.xml中移出来了，所以属性初始化放在属性声明上进行了。\n        private String name=\"product 1\";\n        private String name=\"category 1\";\n        同时@Autowired也需要在Product.java的category属性前加上\n\n*Spring javaConfig 后期补充\n```\n[样例源码参照](https://github.com/olicity-wong/Spring/tree/master/spring)  \n## 三、面向切面（AOP）\n### 1.概述\n&emsp;AOP 即 Aspect Oriented Program 面向切面编程。在面向切面编程的思想里面，把功能分为**核心业务功能**和**周边功能**。  \n&emsp;核心业务，比如登陆，增加数据，删除数据都叫核心业务。  \n&emsp;周边功能，比如性能统计，日志，事务管理等等。  \n&emsp;周边功能在Spring的面向切面编程AOP思想里，即被定义为**切面**  \n&emsp;在面向切面编程AOP的思想里面，核心业务功能和切面功能分别独立进行开发，然后把切面功能和核心业务功能 “编织” 在一起，这种能够选择性的，低耦合的把切面和核心业务功能结合在一起的编程思想，就叫AOP。  \n### 2.AOP核心概念\n```\n1、横切关注点\n对哪些方法进行拦截，拦截后怎么处理，这些关注点称之为横切关注点\n2、切面（aspect）\n类是对物体特征的抽象，切面就是对横切关注点的抽象\n3、连接点（joinpoint）\n被拦截到的点，因为Spring只支持方法类型的连接点，所以在Spring中连接点指的就是被拦截到的方法，实际上连接点还可以是字段或者构造器\n4、切入点（pointcut）\n对连接点进行拦截的定义\n5、通知（advice）\n所谓通知指的就是指拦截到连接点之后要执行的代码，通知分为前置、后置、异常、最终、环绕通知五类\n6、目标对象\n代理的目标对象\n7、织入（weave）\n将切面应用到目标对象并导致代理对象创建的过程\n8、引入（introduction）\n在不修改代码的前提下，引入可以在运行期为类动态地添加一些方法或字段\n```\n### 3.Spring对AOP的支持\n&emsp;Spring中AOP代理由Spring的IOC容器负责生成、管理，其依赖关系也由IOC容器负责管理。因此，AOP代理可以直接使用容器中的其它bean实例作为目标，这种关系可由IOC容器的依赖注入提供。Spring创建代理的规则为：  \n```\n1、默认使用Java动态代理来创建AOP代理，这样就可以为任何接口实例创建代理了\n2、当需要代理的类不是代理接口的时候，Spring会切换为使用CGLIB代理，也可强制使用CGLIB\n```\n### 4.AOP编程\n```\n1、定义普通业务组件\n2、定义切入点，一个切入点可能横切多个业务组件 \n3、定义增强处理，增强处理就是在AOP框架为普通业务组件织入的处理动作\n```\n### 5.简单例子熟悉AOP\n```\nAOP配置文件\n<!--声明业务对象-->\n    <bean name=\"s\" class=\"service.ProductService\">\n    </bean>\n    <!--声明日志切面-->\n    <bean id=\"loggerAspect\" class=\"aspect.LoggerAspect\"/>\n    <aop:config>\n        <!--指定核心业务功能-->\n        <aop:pointcut id=\"loggerCutpoint\"\n                      expression=\n                              \"execution(* service.ProductService.*(..)) \"/>    <!--* 返回任意类型,   包名以 service.ProductService 开头的类的任意方法,   (..) 参数是任意数量和类型-->\n        <!--指定辅助业务功能-->\n        <aop:aspect id=\"logAspect\" ref=\"loggerAspect\">\n            <aop:around pointcut-ref=\"loggerCutpoint\" method=\"log\"/>\n        </aop:aspect>\n    </aop:config>\n```\n### 6.注解方式AOP\n```\n1.注解注解配置业务类,使用@Component(\"s\")\n2.注解配置切面\n    @Aspect 注解表示这是一个切面\n    @Component 表示这是一个bean,由Spring进行管理\n    @Around(value = \"execution(* service.ProductService.*(..))\") 表示对service.ProductService 这个类中的所有方法进行切面操作\n3.配置applicationContext.xml\n    <!--扫描这俩包，定位业务类和切面类-->\n    <context:component-scan base-package=\"aspect\"/>\n    <context:component-scan base-package=\"service\"/>\n    <!--找到被注解了的切面类，进行切面配置-->\n    <aop:aspectj-autoproxy/>\n```\n[样例源码参照](https://github.com/olicity-wong/Spring)\n## 四、总结说明\n&emsp;这个帖子主要就是简单的介绍了一下spring的IOC和AOP思想以及简单的例子。暂时写这么多，spring东西比较多，下一阶段应该是再看一下Spring各种注解的使用还有SpringMVC。","source":"_posts/spring基础学习记录.md","raw":"---\ntitle: spring基础学习记录\ntags:\n  - java\n  - spring\nabbrlink: db474090\ndate: 2018-09-11 21:22:50\n---\n&emsp;最近在做后端的开发，用到了spring，借这个机会给spring捋一遍吧。东西有点多，需要点时间，抽空学。记录(一)主要是写一点spring最基础的IOC和AOP,至于spring的深入学习、各种注解和SpingMVC另开贴。  \n<!--more-->\n## 一、Spring概述\n### 1.简介\n&emsp;Spring是一个基于**控制反转（IOC）**和**面向切面（AOP）**的结构J2EE系统的框架。从简单性、可测试性和松耦合的角度而言，任何Java应用都可以从Spring中受益。  \n&emsp;控制反转(IOC)是一个通用概念，Spring最认同的技术是控制反转的**依赖注入（DI）**模式。简单地说就是拿到的对象的属性，已经被注入好相关值了，直接使用即可。\n&emsp;面向切面编程（AOP），一个程序中跨越多个点的功能被称为横切关注点，这些横切关注点在概念上独立于应用程序的业务逻辑。Spring 框架的 AOP 模块提供了面向方面的程序设计实现，可以定义诸如方法拦截器和切入点等，从而使实现功能的代码彻底的解耦出来。  \n### 2.Spring关键策略\n```\n* 基于POJO的轻量级和最小侵入性编程\n* 通过依赖注入和面向接口实现松耦合\n* 基于切面和惯例进行声明式编程\n* 通过切面和模板减少样板式代码\n```\n### 3.Spring优点\n```\n*方便解耦，简化开发 （高内聚低耦合） \n    Spring就是一个大工厂（容器），可以将所有对象创建和依赖关系维护，交给Spring管理 \n    spring工厂是用于生成bean\n*AOP编程的支持 \n    Spring提供面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等功能\n    声明式事务的支持 \n    只需要通过配置就可以完成对事务的管理，而无需手动编程\n*方便程序的测试 \n    Spring对Junit4支持，可以通过注解方便的测试Spring程序\n*方便集成各种优秀框架 \n    Spring不排斥各种优秀的开源框架，其内部提供了对各种优秀框架（如：Struts、Hibernate、MyBatis、Quartz等）的直接支持\n*降低JavaEE API的使用难度 \n    Spring 对JavaEE开发中非常难用的一些API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低\n```\n## 二、控制反转（IOC）\n### 1.概述\n&emsp;代码之间的**耦合性**具有两面性。耦合是必须的，但应小心谨慎的管理。  \n&emsp;DI会将所以来的关系自动交给目标对象，而不是让对象自己去获取依赖。–**松耦合**\n### 2.获取对象的方式进行比较\n传统方式：\n&emsp;通过new关键字主动创建一个对象。  \nIOC方式：\n&emsp;对象的生命周期由Spring来管理，直接从Spring那里去获取一个对象。 IOC是反转控制 (Inversion Of Control)的缩写，就像控制权从本来在自己手里，交给了Spring。  \n### 3.装配\n&emsp;创建应用组件之间协作的行为通常称为装配。常用XML装配方式。Spring配置文件。\n```\n*使用构造器进行配置\n    <bean> 定义JavaBean，配置需要创建的对象\n    id/name ：用于之后从spring容器获得实例时使用的\n    class ：需要创建实例的全限定类名\n\n\n    <bean id=\"c\" class=\"pojo.Category\">\n        <property name=\"name\" value=\"category 1\" /> <!--category对象-->\n    </bean>\n\n    <!--创建product对象的时候注入了一个category对象，使用ref-->\n    <bean name=\"p\" class=\"pojo.Product\"> <!--product类中添加category对象的set&get方法-->\n        <property name=\"name\" value=\"product1\" />   <!--product对象-->\n        <property name=\"category\" ref=\"c\" />    <!--为product对象注入category对象-->\n    </bean>\n\n*使用注解的方式进行配置\n    xml中添加  <context:annotation-config/> 注释掉注入category\n        *@Autowired注解方法\n            在Product.java的category属性前加上@Autowired注解\n            @Autowired\n            private Category category;\n            或者\n            @Autowired\n            public void setCategory(Category category) \n        *@Resource\n            @Resource(name=\"c\")\n            private Category category;\n\n*对bean进行注解配置\n    直接xml中添加  <context:component-scan base-package=\"pojo\"/>\n    就是告知spring，所有的bean都在pojo包下\n    通过@Component注解\n        在类前加入  @Component(\"c\")  表明此类是bean\n        因为配置从applicationContext.xml中移出来了，所以属性初始化放在属性声明上进行了。\n        private String name=\"product 1\";\n        private String name=\"category 1\";\n        同时@Autowired也需要在Product.java的category属性前加上\n\n*Spring javaConfig 后期补充\n```\n[样例源码参照](https://github.com/olicity-wong/Spring/tree/master/spring)  \n## 三、面向切面（AOP）\n### 1.概述\n&emsp;AOP 即 Aspect Oriented Program 面向切面编程。在面向切面编程的思想里面，把功能分为**核心业务功能**和**周边功能**。  \n&emsp;核心业务，比如登陆，增加数据，删除数据都叫核心业务。  \n&emsp;周边功能，比如性能统计，日志，事务管理等等。  \n&emsp;周边功能在Spring的面向切面编程AOP思想里，即被定义为**切面**  \n&emsp;在面向切面编程AOP的思想里面，核心业务功能和切面功能分别独立进行开发，然后把切面功能和核心业务功能 “编织” 在一起，这种能够选择性的，低耦合的把切面和核心业务功能结合在一起的编程思想，就叫AOP。  \n### 2.AOP核心概念\n```\n1、横切关注点\n对哪些方法进行拦截，拦截后怎么处理，这些关注点称之为横切关注点\n2、切面（aspect）\n类是对物体特征的抽象，切面就是对横切关注点的抽象\n3、连接点（joinpoint）\n被拦截到的点，因为Spring只支持方法类型的连接点，所以在Spring中连接点指的就是被拦截到的方法，实际上连接点还可以是字段或者构造器\n4、切入点（pointcut）\n对连接点进行拦截的定义\n5、通知（advice）\n所谓通知指的就是指拦截到连接点之后要执行的代码，通知分为前置、后置、异常、最终、环绕通知五类\n6、目标对象\n代理的目标对象\n7、织入（weave）\n将切面应用到目标对象并导致代理对象创建的过程\n8、引入（introduction）\n在不修改代码的前提下，引入可以在运行期为类动态地添加一些方法或字段\n```\n### 3.Spring对AOP的支持\n&emsp;Spring中AOP代理由Spring的IOC容器负责生成、管理，其依赖关系也由IOC容器负责管理。因此，AOP代理可以直接使用容器中的其它bean实例作为目标，这种关系可由IOC容器的依赖注入提供。Spring创建代理的规则为：  \n```\n1、默认使用Java动态代理来创建AOP代理，这样就可以为任何接口实例创建代理了\n2、当需要代理的类不是代理接口的时候，Spring会切换为使用CGLIB代理，也可强制使用CGLIB\n```\n### 4.AOP编程\n```\n1、定义普通业务组件\n2、定义切入点，一个切入点可能横切多个业务组件 \n3、定义增强处理，增强处理就是在AOP框架为普通业务组件织入的处理动作\n```\n### 5.简单例子熟悉AOP\n```\nAOP配置文件\n<!--声明业务对象-->\n    <bean name=\"s\" class=\"service.ProductService\">\n    </bean>\n    <!--声明日志切面-->\n    <bean id=\"loggerAspect\" class=\"aspect.LoggerAspect\"/>\n    <aop:config>\n        <!--指定核心业务功能-->\n        <aop:pointcut id=\"loggerCutpoint\"\n                      expression=\n                              \"execution(* service.ProductService.*(..)) \"/>    <!--* 返回任意类型,   包名以 service.ProductService 开头的类的任意方法,   (..) 参数是任意数量和类型-->\n        <!--指定辅助业务功能-->\n        <aop:aspect id=\"logAspect\" ref=\"loggerAspect\">\n            <aop:around pointcut-ref=\"loggerCutpoint\" method=\"log\"/>\n        </aop:aspect>\n    </aop:config>\n```\n### 6.注解方式AOP\n```\n1.注解注解配置业务类,使用@Component(\"s\")\n2.注解配置切面\n    @Aspect 注解表示这是一个切面\n    @Component 表示这是一个bean,由Spring进行管理\n    @Around(value = \"execution(* service.ProductService.*(..))\") 表示对service.ProductService 这个类中的所有方法进行切面操作\n3.配置applicationContext.xml\n    <!--扫描这俩包，定位业务类和切面类-->\n    <context:component-scan base-package=\"aspect\"/>\n    <context:component-scan base-package=\"service\"/>\n    <!--找到被注解了的切面类，进行切面配置-->\n    <aop:aspectj-autoproxy/>\n```\n[样例源码参照](https://github.com/olicity-wong/Spring)\n## 四、总结说明\n&emsp;这个帖子主要就是简单的介绍了一下spring的IOC和AOP思想以及简单的例子。暂时写这么多，spring东西比较多，下一阶段应该是再看一下Spring各种注解的使用还有SpringMVC。","slug":"spring基础学习记录","published":1,"updated":"2019-09-18T13:40:44.063Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0q92d1i00137guo8pb45roz","content":"<p>&emsp;最近在做后端的开发，用到了spring，借这个机会给spring捋一遍吧。东西有点多，需要点时间，抽空学。记录(一)主要是写一点spring最基础的IOC和AOP,至于spring的深入学习、各种注解和SpingMVC另开贴。<br><a id=\"more\"></a></p>\n<h2 id=\"一、Spring概述\"><a href=\"#一、Spring概述\" class=\"headerlink\" title=\"一、Spring概述\"></a>一、Spring概述</h2><h3 id=\"1-简介\"><a href=\"#1-简介\" class=\"headerlink\" title=\"1.简介\"></a>1.简介</h3><p>&emsp;Spring是一个基于<strong>控制反转（IOC）</strong>和<strong>面向切面（AOP）</strong>的结构J2EE系统的框架。从简单性、可测试性和松耦合的角度而言，任何Java应用都可以从Spring中受益。<br>&emsp;控制反转(IOC)是一个通用概念，Spring最认同的技术是控制反转的<strong>依赖注入（DI）</strong>模式。简单地说就是拿到的对象的属性，已经被注入好相关值了，直接使用即可。<br>&emsp;面向切面编程（AOP），一个程序中跨越多个点的功能被称为横切关注点，这些横切关注点在概念上独立于应用程序的业务逻辑。Spring 框架的 AOP 模块提供了面向方面的程序设计实现，可以定义诸如方法拦截器和切入点等，从而使实现功能的代码彻底的解耦出来。  </p>\n<h3 id=\"2-Spring关键策略\"><a href=\"#2-Spring关键策略\" class=\"headerlink\" title=\"2.Spring关键策略\"></a>2.Spring关键策略</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">* 基于POJO的轻量级和最小侵入性编程</span><br><span class=\"line\">* 通过依赖注入和面向接口实现松耦合</span><br><span class=\"line\">* 基于切面和惯例进行声明式编程</span><br><span class=\"line\">* 通过切面和模板减少样板式代码</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Spring优点\"><a href=\"#3-Spring优点\" class=\"headerlink\" title=\"3.Spring优点\"></a>3.Spring优点</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">*方便解耦，简化开发 （高内聚低耦合） </span><br><span class=\"line\">    Spring就是一个大工厂（容器），可以将所有对象创建和依赖关系维护，交给Spring管理 </span><br><span class=\"line\">    spring工厂是用于生成bean</span><br><span class=\"line\">*AOP编程的支持 </span><br><span class=\"line\">    Spring提供面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等功能</span><br><span class=\"line\">    声明式事务的支持 </span><br><span class=\"line\">    只需要通过配置就可以完成对事务的管理，而无需手动编程</span><br><span class=\"line\">*方便程序的测试 </span><br><span class=\"line\">    Spring对Junit4支持，可以通过注解方便的测试Spring程序</span><br><span class=\"line\">*方便集成各种优秀框架 </span><br><span class=\"line\">    Spring不排斥各种优秀的开源框架，其内部提供了对各种优秀框架（如：Struts、Hibernate、MyBatis、Quartz等）的直接支持</span><br><span class=\"line\">*降低JavaEE API的使用难度 </span><br><span class=\"line\">    Spring 对JavaEE开发中非常难用的一些API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低</span><br></pre></td></tr></table></figure>\n<h2 id=\"二、控制反转（IOC）\"><a href=\"#二、控制反转（IOC）\" class=\"headerlink\" title=\"二、控制反转（IOC）\"></a>二、控制反转（IOC）</h2><h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1.概述\"></a>1.概述</h3><p>&emsp;代码之间的<strong>耦合性</strong>具有两面性。耦合是必须的，但应小心谨慎的管理。<br>&emsp;DI会将所以来的关系自动交给目标对象，而不是让对象自己去获取依赖。–<strong>松耦合</strong></p>\n<h3 id=\"2-获取对象的方式进行比较\"><a href=\"#2-获取对象的方式进行比较\" class=\"headerlink\" title=\"2.获取对象的方式进行比较\"></a>2.获取对象的方式进行比较</h3><p>传统方式：<br>&emsp;通过new关键字主动创建一个对象。<br>IOC方式：<br>&emsp;对象的生命周期由Spring来管理，直接从Spring那里去获取一个对象。 IOC是反转控制 (Inversion Of Control)的缩写，就像控制权从本来在自己手里，交给了Spring。  </p>\n<h3 id=\"3-装配\"><a href=\"#3-装配\" class=\"headerlink\" title=\"3.装配\"></a>3.装配</h3><p>&emsp;创建应用组件之间协作的行为通常称为装配。常用XML装配方式。Spring配置文件。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">*使用构造器进行配置</span><br><span class=\"line\">    &lt;bean&gt; 定义JavaBean，配置需要创建的对象</span><br><span class=\"line\">    id/name ：用于之后从spring容器获得实例时使用的</span><br><span class=\"line\">    class ：需要创建实例的全限定类名</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;bean id=&quot;c&quot; class=&quot;pojo.Category&quot;&gt;</span><br><span class=\"line\">        &lt;property name=&quot;name&quot; value=&quot;category 1&quot; /&gt; &lt;!--category对象--&gt;</span><br><span class=\"line\">    &lt;/bean&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;!--创建product对象的时候注入了一个category对象，使用ref--&gt;</span><br><span class=\"line\">    &lt;bean name=&quot;p&quot; class=&quot;pojo.Product&quot;&gt; &lt;!--product类中添加category对象的set&amp;get方法--&gt;</span><br><span class=\"line\">        &lt;property name=&quot;name&quot; value=&quot;product1&quot; /&gt;   &lt;!--product对象--&gt;</span><br><span class=\"line\">        &lt;property name=&quot;category&quot; ref=&quot;c&quot; /&gt;    &lt;!--为product对象注入category对象--&gt;</span><br><span class=\"line\">    &lt;/bean&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">*使用注解的方式进行配置</span><br><span class=\"line\">    xml中添加  &lt;context:annotation-config/&gt; 注释掉注入category</span><br><span class=\"line\">        *@Autowired注解方法</span><br><span class=\"line\">            在Product.java的category属性前加上@Autowired注解</span><br><span class=\"line\">            @Autowired</span><br><span class=\"line\">            private Category category;</span><br><span class=\"line\">            或者</span><br><span class=\"line\">            @Autowired</span><br><span class=\"line\">            public void setCategory(Category category) </span><br><span class=\"line\">        *@Resource</span><br><span class=\"line\">            @Resource(name=&quot;c&quot;)</span><br><span class=\"line\">            private Category category;</span><br><span class=\"line\"></span><br><span class=\"line\">*对bean进行注解配置</span><br><span class=\"line\">    直接xml中添加  &lt;context:component-scan base-package=&quot;pojo&quot;/&gt;</span><br><span class=\"line\">    就是告知spring，所有的bean都在pojo包下</span><br><span class=\"line\">    通过@Component注解</span><br><span class=\"line\">        在类前加入  @Component(&quot;c&quot;)  表明此类是bean</span><br><span class=\"line\">        因为配置从applicationContext.xml中移出来了，所以属性初始化放在属性声明上进行了。</span><br><span class=\"line\">        private String name=&quot;product 1&quot;;</span><br><span class=\"line\">        private String name=&quot;category 1&quot;;</span><br><span class=\"line\">        同时@Autowired也需要在Product.java的category属性前加上</span><br><span class=\"line\"></span><br><span class=\"line\">*Spring javaConfig 后期补充</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/olicity-wong/Spring/tree/master/spring\" target=\"_blank\" rel=\"noopener\">样例源码参照</a>  </p>\n<h2 id=\"三、面向切面（AOP）\"><a href=\"#三、面向切面（AOP）\" class=\"headerlink\" title=\"三、面向切面（AOP）\"></a>三、面向切面（AOP）</h2><h3 id=\"1-概述-1\"><a href=\"#1-概述-1\" class=\"headerlink\" title=\"1.概述\"></a>1.概述</h3><p>&emsp;AOP 即 Aspect Oriented Program 面向切面编程。在面向切面编程的思想里面，把功能分为<strong>核心业务功能</strong>和<strong>周边功能</strong>。<br>&emsp;核心业务，比如登陆，增加数据，删除数据都叫核心业务。<br>&emsp;周边功能，比如性能统计，日志，事务管理等等。<br>&emsp;周边功能在Spring的面向切面编程AOP思想里，即被定义为<strong>切面</strong><br>&emsp;在面向切面编程AOP的思想里面，核心业务功能和切面功能分别独立进行开发，然后把切面功能和核心业务功能 “编织” 在一起，这种能够选择性的，低耦合的把切面和核心业务功能结合在一起的编程思想，就叫AOP。  </p>\n<h3 id=\"2-AOP核心概念\"><a href=\"#2-AOP核心概念\" class=\"headerlink\" title=\"2.AOP核心概念\"></a>2.AOP核心概念</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1、横切关注点</span><br><span class=\"line\">对哪些方法进行拦截，拦截后怎么处理，这些关注点称之为横切关注点</span><br><span class=\"line\">2、切面（aspect）</span><br><span class=\"line\">类是对物体特征的抽象，切面就是对横切关注点的抽象</span><br><span class=\"line\">3、连接点（joinpoint）</span><br><span class=\"line\">被拦截到的点，因为Spring只支持方法类型的连接点，所以在Spring中连接点指的就是被拦截到的方法，实际上连接点还可以是字段或者构造器</span><br><span class=\"line\">4、切入点（pointcut）</span><br><span class=\"line\">对连接点进行拦截的定义</span><br><span class=\"line\">5、通知（advice）</span><br><span class=\"line\">所谓通知指的就是指拦截到连接点之后要执行的代码，通知分为前置、后置、异常、最终、环绕通知五类</span><br><span class=\"line\">6、目标对象</span><br><span class=\"line\">代理的目标对象</span><br><span class=\"line\">7、织入（weave）</span><br><span class=\"line\">将切面应用到目标对象并导致代理对象创建的过程</span><br><span class=\"line\">8、引入（introduction）</span><br><span class=\"line\">在不修改代码的前提下，引入可以在运行期为类动态地添加一些方法或字段</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Spring对AOP的支持\"><a href=\"#3-Spring对AOP的支持\" class=\"headerlink\" title=\"3.Spring对AOP的支持\"></a>3.Spring对AOP的支持</h3><p>&emsp;Spring中AOP代理由Spring的IOC容器负责生成、管理，其依赖关系也由IOC容器负责管理。因此，AOP代理可以直接使用容器中的其它bean实例作为目标，这种关系可由IOC容器的依赖注入提供。Spring创建代理的规则为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1、默认使用Java动态代理来创建AOP代理，这样就可以为任何接口实例创建代理了</span><br><span class=\"line\">2、当需要代理的类不是代理接口的时候，Spring会切换为使用CGLIB代理，也可强制使用CGLIB</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-AOP编程\"><a href=\"#4-AOP编程\" class=\"headerlink\" title=\"4.AOP编程\"></a>4.AOP编程</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1、定义普通业务组件</span><br><span class=\"line\">2、定义切入点，一个切入点可能横切多个业务组件 </span><br><span class=\"line\">3、定义增强处理，增强处理就是在AOP框架为普通业务组件织入的处理动作</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-简单例子熟悉AOP\"><a href=\"#5-简单例子熟悉AOP\" class=\"headerlink\" title=\"5.简单例子熟悉AOP\"></a>5.简单例子熟悉AOP</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AOP配置文件</span><br><span class=\"line\">&lt;!--声明业务对象--&gt;</span><br><span class=\"line\">    &lt;bean name=&quot;s&quot; class=&quot;service.ProductService&quot;&gt;</span><br><span class=\"line\">    &lt;/bean&gt;</span><br><span class=\"line\">    &lt;!--声明日志切面--&gt;</span><br><span class=\"line\">    &lt;bean id=&quot;loggerAspect&quot; class=&quot;aspect.LoggerAspect&quot;/&gt;</span><br><span class=\"line\">    &lt;aop:config&gt;</span><br><span class=\"line\">        &lt;!--指定核心业务功能--&gt;</span><br><span class=\"line\">        &lt;aop:pointcut id=&quot;loggerCutpoint&quot;</span><br><span class=\"line\">                      expression=</span><br><span class=\"line\">                              &quot;execution(* service.ProductService.*(..)) &quot;/&gt;    &lt;!--* 返回任意类型,   包名以 service.ProductService 开头的类的任意方法,   (..) 参数是任意数量和类型--&gt;</span><br><span class=\"line\">        &lt;!--指定辅助业务功能--&gt;</span><br><span class=\"line\">        &lt;aop:aspect id=&quot;logAspect&quot; ref=&quot;loggerAspect&quot;&gt;</span><br><span class=\"line\">            &lt;aop:around pointcut-ref=&quot;loggerCutpoint&quot; method=&quot;log&quot;/&gt;</span><br><span class=\"line\">        &lt;/aop:aspect&gt;</span><br><span class=\"line\">    &lt;/aop:config&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"6-注解方式AOP\"><a href=\"#6-注解方式AOP\" class=\"headerlink\" title=\"6.注解方式AOP\"></a>6.注解方式AOP</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1.注解注解配置业务类,使用@Component(&quot;s&quot;)</span><br><span class=\"line\">2.注解配置切面</span><br><span class=\"line\">    @Aspect 注解表示这是一个切面</span><br><span class=\"line\">    @Component 表示这是一个bean,由Spring进行管理</span><br><span class=\"line\">    @Around(value = &quot;execution(* service.ProductService.*(..))&quot;) 表示对service.ProductService 这个类中的所有方法进行切面操作</span><br><span class=\"line\">3.配置applicationContext.xml</span><br><span class=\"line\">    &lt;!--扫描这俩包，定位业务类和切面类--&gt;</span><br><span class=\"line\">    &lt;context:component-scan base-package=&quot;aspect&quot;/&gt;</span><br><span class=\"line\">    &lt;context:component-scan base-package=&quot;service&quot;/&gt;</span><br><span class=\"line\">    &lt;!--找到被注解了的切面类，进行切面配置--&gt;</span><br><span class=\"line\">    &lt;aop:aspectj-autoproxy/&gt;</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/olicity-wong/Spring\" target=\"_blank\" rel=\"noopener\">样例源码参照</a></p>\n<h2 id=\"四、总结说明\"><a href=\"#四、总结说明\" class=\"headerlink\" title=\"四、总结说明\"></a>四、总结说明</h2><p>&emsp;这个帖子主要就是简单的介绍了一下spring的IOC和AOP思想以及简单的例子。暂时写这么多，spring东西比较多，下一阶段应该是再看一下Spring各种注解的使用还有SpringMVC。</p>\n","site":{"data":{}},"excerpt":"<p>&emsp;最近在做后端的开发，用到了spring，借这个机会给spring捋一遍吧。东西有点多，需要点时间，抽空学。记录(一)主要是写一点spring最基础的IOC和AOP,至于spring的深入学习、各种注解和SpingMVC另开贴。<br>","more":"</p>\n<h2 id=\"一、Spring概述\"><a href=\"#一、Spring概述\" class=\"headerlink\" title=\"一、Spring概述\"></a>一、Spring概述</h2><h3 id=\"1-简介\"><a href=\"#1-简介\" class=\"headerlink\" title=\"1.简介\"></a>1.简介</h3><p>&emsp;Spring是一个基于<strong>控制反转（IOC）</strong>和<strong>面向切面（AOP）</strong>的结构J2EE系统的框架。从简单性、可测试性和松耦合的角度而言，任何Java应用都可以从Spring中受益。<br>&emsp;控制反转(IOC)是一个通用概念，Spring最认同的技术是控制反转的<strong>依赖注入（DI）</strong>模式。简单地说就是拿到的对象的属性，已经被注入好相关值了，直接使用即可。<br>&emsp;面向切面编程（AOP），一个程序中跨越多个点的功能被称为横切关注点，这些横切关注点在概念上独立于应用程序的业务逻辑。Spring 框架的 AOP 模块提供了面向方面的程序设计实现，可以定义诸如方法拦截器和切入点等，从而使实现功能的代码彻底的解耦出来。  </p>\n<h3 id=\"2-Spring关键策略\"><a href=\"#2-Spring关键策略\" class=\"headerlink\" title=\"2.Spring关键策略\"></a>2.Spring关键策略</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">* 基于POJO的轻量级和最小侵入性编程</span><br><span class=\"line\">* 通过依赖注入和面向接口实现松耦合</span><br><span class=\"line\">* 基于切面和惯例进行声明式编程</span><br><span class=\"line\">* 通过切面和模板减少样板式代码</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Spring优点\"><a href=\"#3-Spring优点\" class=\"headerlink\" title=\"3.Spring优点\"></a>3.Spring优点</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">*方便解耦，简化开发 （高内聚低耦合） </span><br><span class=\"line\">    Spring就是一个大工厂（容器），可以将所有对象创建和依赖关系维护，交给Spring管理 </span><br><span class=\"line\">    spring工厂是用于生成bean</span><br><span class=\"line\">*AOP编程的支持 </span><br><span class=\"line\">    Spring提供面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等功能</span><br><span class=\"line\">    声明式事务的支持 </span><br><span class=\"line\">    只需要通过配置就可以完成对事务的管理，而无需手动编程</span><br><span class=\"line\">*方便程序的测试 </span><br><span class=\"line\">    Spring对Junit4支持，可以通过注解方便的测试Spring程序</span><br><span class=\"line\">*方便集成各种优秀框架 </span><br><span class=\"line\">    Spring不排斥各种优秀的开源框架，其内部提供了对各种优秀框架（如：Struts、Hibernate、MyBatis、Quartz等）的直接支持</span><br><span class=\"line\">*降低JavaEE API的使用难度 </span><br><span class=\"line\">    Spring 对JavaEE开发中非常难用的一些API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低</span><br></pre></td></tr></table></figure>\n<h2 id=\"二、控制反转（IOC）\"><a href=\"#二、控制反转（IOC）\" class=\"headerlink\" title=\"二、控制反转（IOC）\"></a>二、控制反转（IOC）</h2><h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1.概述\"></a>1.概述</h3><p>&emsp;代码之间的<strong>耦合性</strong>具有两面性。耦合是必须的，但应小心谨慎的管理。<br>&emsp;DI会将所以来的关系自动交给目标对象，而不是让对象自己去获取依赖。–<strong>松耦合</strong></p>\n<h3 id=\"2-获取对象的方式进行比较\"><a href=\"#2-获取对象的方式进行比较\" class=\"headerlink\" title=\"2.获取对象的方式进行比较\"></a>2.获取对象的方式进行比较</h3><p>传统方式：<br>&emsp;通过new关键字主动创建一个对象。<br>IOC方式：<br>&emsp;对象的生命周期由Spring来管理，直接从Spring那里去获取一个对象。 IOC是反转控制 (Inversion Of Control)的缩写，就像控制权从本来在自己手里，交给了Spring。  </p>\n<h3 id=\"3-装配\"><a href=\"#3-装配\" class=\"headerlink\" title=\"3.装配\"></a>3.装配</h3><p>&emsp;创建应用组件之间协作的行为通常称为装配。常用XML装配方式。Spring配置文件。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">*使用构造器进行配置</span><br><span class=\"line\">    &lt;bean&gt; 定义JavaBean，配置需要创建的对象</span><br><span class=\"line\">    id/name ：用于之后从spring容器获得实例时使用的</span><br><span class=\"line\">    class ：需要创建实例的全限定类名</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;bean id=&quot;c&quot; class=&quot;pojo.Category&quot;&gt;</span><br><span class=\"line\">        &lt;property name=&quot;name&quot; value=&quot;category 1&quot; /&gt; &lt;!--category对象--&gt;</span><br><span class=\"line\">    &lt;/bean&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;!--创建product对象的时候注入了一个category对象，使用ref--&gt;</span><br><span class=\"line\">    &lt;bean name=&quot;p&quot; class=&quot;pojo.Product&quot;&gt; &lt;!--product类中添加category对象的set&amp;get方法--&gt;</span><br><span class=\"line\">        &lt;property name=&quot;name&quot; value=&quot;product1&quot; /&gt;   &lt;!--product对象--&gt;</span><br><span class=\"line\">        &lt;property name=&quot;category&quot; ref=&quot;c&quot; /&gt;    &lt;!--为product对象注入category对象--&gt;</span><br><span class=\"line\">    &lt;/bean&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">*使用注解的方式进行配置</span><br><span class=\"line\">    xml中添加  &lt;context:annotation-config/&gt; 注释掉注入category</span><br><span class=\"line\">        *@Autowired注解方法</span><br><span class=\"line\">            在Product.java的category属性前加上@Autowired注解</span><br><span class=\"line\">            @Autowired</span><br><span class=\"line\">            private Category category;</span><br><span class=\"line\">            或者</span><br><span class=\"line\">            @Autowired</span><br><span class=\"line\">            public void setCategory(Category category) </span><br><span class=\"line\">        *@Resource</span><br><span class=\"line\">            @Resource(name=&quot;c&quot;)</span><br><span class=\"line\">            private Category category;</span><br><span class=\"line\"></span><br><span class=\"line\">*对bean进行注解配置</span><br><span class=\"line\">    直接xml中添加  &lt;context:component-scan base-package=&quot;pojo&quot;/&gt;</span><br><span class=\"line\">    就是告知spring，所有的bean都在pojo包下</span><br><span class=\"line\">    通过@Component注解</span><br><span class=\"line\">        在类前加入  @Component(&quot;c&quot;)  表明此类是bean</span><br><span class=\"line\">        因为配置从applicationContext.xml中移出来了，所以属性初始化放在属性声明上进行了。</span><br><span class=\"line\">        private String name=&quot;product 1&quot;;</span><br><span class=\"line\">        private String name=&quot;category 1&quot;;</span><br><span class=\"line\">        同时@Autowired也需要在Product.java的category属性前加上</span><br><span class=\"line\"></span><br><span class=\"line\">*Spring javaConfig 后期补充</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/olicity-wong/Spring/tree/master/spring\" target=\"_blank\" rel=\"noopener\">样例源码参照</a>  </p>\n<h2 id=\"三、面向切面（AOP）\"><a href=\"#三、面向切面（AOP）\" class=\"headerlink\" title=\"三、面向切面（AOP）\"></a>三、面向切面（AOP）</h2><h3 id=\"1-概述-1\"><a href=\"#1-概述-1\" class=\"headerlink\" title=\"1.概述\"></a>1.概述</h3><p>&emsp;AOP 即 Aspect Oriented Program 面向切面编程。在面向切面编程的思想里面，把功能分为<strong>核心业务功能</strong>和<strong>周边功能</strong>。<br>&emsp;核心业务，比如登陆，增加数据，删除数据都叫核心业务。<br>&emsp;周边功能，比如性能统计，日志，事务管理等等。<br>&emsp;周边功能在Spring的面向切面编程AOP思想里，即被定义为<strong>切面</strong><br>&emsp;在面向切面编程AOP的思想里面，核心业务功能和切面功能分别独立进行开发，然后把切面功能和核心业务功能 “编织” 在一起，这种能够选择性的，低耦合的把切面和核心业务功能结合在一起的编程思想，就叫AOP。  </p>\n<h3 id=\"2-AOP核心概念\"><a href=\"#2-AOP核心概念\" class=\"headerlink\" title=\"2.AOP核心概念\"></a>2.AOP核心概念</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1、横切关注点</span><br><span class=\"line\">对哪些方法进行拦截，拦截后怎么处理，这些关注点称之为横切关注点</span><br><span class=\"line\">2、切面（aspect）</span><br><span class=\"line\">类是对物体特征的抽象，切面就是对横切关注点的抽象</span><br><span class=\"line\">3、连接点（joinpoint）</span><br><span class=\"line\">被拦截到的点，因为Spring只支持方法类型的连接点，所以在Spring中连接点指的就是被拦截到的方法，实际上连接点还可以是字段或者构造器</span><br><span class=\"line\">4、切入点（pointcut）</span><br><span class=\"line\">对连接点进行拦截的定义</span><br><span class=\"line\">5、通知（advice）</span><br><span class=\"line\">所谓通知指的就是指拦截到连接点之后要执行的代码，通知分为前置、后置、异常、最终、环绕通知五类</span><br><span class=\"line\">6、目标对象</span><br><span class=\"line\">代理的目标对象</span><br><span class=\"line\">7、织入（weave）</span><br><span class=\"line\">将切面应用到目标对象并导致代理对象创建的过程</span><br><span class=\"line\">8、引入（introduction）</span><br><span class=\"line\">在不修改代码的前提下，引入可以在运行期为类动态地添加一些方法或字段</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Spring对AOP的支持\"><a href=\"#3-Spring对AOP的支持\" class=\"headerlink\" title=\"3.Spring对AOP的支持\"></a>3.Spring对AOP的支持</h3><p>&emsp;Spring中AOP代理由Spring的IOC容器负责生成、管理，其依赖关系也由IOC容器负责管理。因此，AOP代理可以直接使用容器中的其它bean实例作为目标，这种关系可由IOC容器的依赖注入提供。Spring创建代理的规则为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1、默认使用Java动态代理来创建AOP代理，这样就可以为任何接口实例创建代理了</span><br><span class=\"line\">2、当需要代理的类不是代理接口的时候，Spring会切换为使用CGLIB代理，也可强制使用CGLIB</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-AOP编程\"><a href=\"#4-AOP编程\" class=\"headerlink\" title=\"4.AOP编程\"></a>4.AOP编程</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1、定义普通业务组件</span><br><span class=\"line\">2、定义切入点，一个切入点可能横切多个业务组件 </span><br><span class=\"line\">3、定义增强处理，增强处理就是在AOP框架为普通业务组件织入的处理动作</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-简单例子熟悉AOP\"><a href=\"#5-简单例子熟悉AOP\" class=\"headerlink\" title=\"5.简单例子熟悉AOP\"></a>5.简单例子熟悉AOP</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AOP配置文件</span><br><span class=\"line\">&lt;!--声明业务对象--&gt;</span><br><span class=\"line\">    &lt;bean name=&quot;s&quot; class=&quot;service.ProductService&quot;&gt;</span><br><span class=\"line\">    &lt;/bean&gt;</span><br><span class=\"line\">    &lt;!--声明日志切面--&gt;</span><br><span class=\"line\">    &lt;bean id=&quot;loggerAspect&quot; class=&quot;aspect.LoggerAspect&quot;/&gt;</span><br><span class=\"line\">    &lt;aop:config&gt;</span><br><span class=\"line\">        &lt;!--指定核心业务功能--&gt;</span><br><span class=\"line\">        &lt;aop:pointcut id=&quot;loggerCutpoint&quot;</span><br><span class=\"line\">                      expression=</span><br><span class=\"line\">                              &quot;execution(* service.ProductService.*(..)) &quot;/&gt;    &lt;!--* 返回任意类型,   包名以 service.ProductService 开头的类的任意方法,   (..) 参数是任意数量和类型--&gt;</span><br><span class=\"line\">        &lt;!--指定辅助业务功能--&gt;</span><br><span class=\"line\">        &lt;aop:aspect id=&quot;logAspect&quot; ref=&quot;loggerAspect&quot;&gt;</span><br><span class=\"line\">            &lt;aop:around pointcut-ref=&quot;loggerCutpoint&quot; method=&quot;log&quot;/&gt;</span><br><span class=\"line\">        &lt;/aop:aspect&gt;</span><br><span class=\"line\">    &lt;/aop:config&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"6-注解方式AOP\"><a href=\"#6-注解方式AOP\" class=\"headerlink\" title=\"6.注解方式AOP\"></a>6.注解方式AOP</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1.注解注解配置业务类,使用@Component(&quot;s&quot;)</span><br><span class=\"line\">2.注解配置切面</span><br><span class=\"line\">    @Aspect 注解表示这是一个切面</span><br><span class=\"line\">    @Component 表示这是一个bean,由Spring进行管理</span><br><span class=\"line\">    @Around(value = &quot;execution(* service.ProductService.*(..))&quot;) 表示对service.ProductService 这个类中的所有方法进行切面操作</span><br><span class=\"line\">3.配置applicationContext.xml</span><br><span class=\"line\">    &lt;!--扫描这俩包，定位业务类和切面类--&gt;</span><br><span class=\"line\">    &lt;context:component-scan base-package=&quot;aspect&quot;/&gt;</span><br><span class=\"line\">    &lt;context:component-scan base-package=&quot;service&quot;/&gt;</span><br><span class=\"line\">    &lt;!--找到被注解了的切面类，进行切面配置--&gt;</span><br><span class=\"line\">    &lt;aop:aspectj-autoproxy/&gt;</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/olicity-wong/Spring\" target=\"_blank\" rel=\"noopener\">样例源码参照</a></p>\n<h2 id=\"四、总结说明\"><a href=\"#四、总结说明\" class=\"headerlink\" title=\"四、总结说明\"></a>四、总结说明</h2><p>&emsp;这个帖子主要就是简单的介绍了一下spring的IOC和AOP思想以及简单的例子。暂时写这么多，spring东西比较多，下一阶段应该是再看一下Spring各种注解的使用还有SpringMVC。</p>"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"ck0q92czr00007guoam084or3","tag_id":"ck0q92d0400037guoe6709ohb","_id":"ck0q92d0e00087guo4mqrx7uy"},{"post_id":"ck0q92d0100027guoh24rlf7n","tag_id":"ck0q92d0d00077guorgs1fxy0","_id":"ck0q92d0w000f7guou9tehqaq"},{"post_id":"ck0q92d0100027guoh24rlf7n","tag_id":"ck0q92d0j000b7guo16tbqsor","_id":"ck0q92d0x000g7guotyoza5pt"},{"post_id":"ck0q92d0q000d7guocmjrmdy6","tag_id":"ck0q92d0d00077guorgs1fxy0","_id":"ck0q92d0y000i7guo75znrzf7"},{"post_id":"ck0q92d0q000d7guocmjrmdy6","tag_id":"ck0q92d0j000b7guo16tbqsor","_id":"ck0q92d0y000j7guosbky0kbo"},{"post_id":"ck0q92d0800047guon9cq8bqh","tag_id":"ck0q92d0v000e7guolygnrlc5","_id":"ck0q92d0z000l7guocj6ov4rd"},{"post_id":"ck0q92d0a00057guo3t6blyku","tag_id":"ck0q92d0x000h7guoyxuc3tge","_id":"ck0q92d12000n7guooc62p3jw"},{"post_id":"ck0q92d0a00057guo3t6blyku","tag_id":"ck0q92d0z000k7guol4h8wpmr","_id":"ck0q92d13000o7guof8fydh5l"},{"post_id":"ck0q92d0c00067guo0yqoh4jq","tag_id":"ck0q92d0x000h7guoyxuc3tge","_id":"ck0q92d15000r7guo8fyob89c"},{"post_id":"ck0q92d0c00067guo0yqoh4jq","tag_id":"ck0q92d0z000k7guol4h8wpmr","_id":"ck0q92d16000s7guot4jfa2mt"},{"post_id":"ck0q92d0e00097guoofbno2tr","tag_id":"ck0q92d0x000h7guoyxuc3tge","_id":"ck0q92d17000u7guotq6r36gh"},{"post_id":"ck0q92d0h000a7guo9aynuu28","tag_id":"ck0q92d16000t7guo1fbpr4zx","_id":"ck0q92d19000w7guosjlbj2mx"},{"post_id":"ck0q92d0j000c7guorghwah14","tag_id":"ck0q92d18000v7guodrxx87z0","_id":"ck0q92d1a000y7guotm035333"},{"post_id":"ck0q92d0j000c7guorghwah14","tag_id":"ck0q92d19000x7guom0nlkl0x","_id":"ck0q92d1a000z7guor2kmf6dk"},{"post_id":"ck0q92d1f00107guotky9r4l0","tag_id":"ck0q92d16000t7guo1fbpr4zx","_id":"ck0q92d1i00127guogmxpd4cn"},{"post_id":"ck0q92d1g00117guo2w2b6omj","tag_id":"ck0q92d16000t7guo1fbpr4zx","_id":"ck0q92d1j00147guofz0sxv43"},{"post_id":"ck0q92d1i00137guo8pb45roz","tag_id":"ck0q92d0400037guoe6709ohb","_id":"ck0q92d1m00167guoemf52vcg"},{"post_id":"ck0q92d1i00137guo8pb45roz","tag_id":"ck0q92d1l00157guouosgj16w","_id":"ck0q92d1n00177guouf6ie2h1"}],"Tag":[{"name":"java","_id":"ck0q92d0400037guoe6709ohb"},{"name":"linux","_id":"ck0q92d0d00077guorgs1fxy0"},{"name":"shell","_id":"ck0q92d0j000b7guo16tbqsor"},{"name":"hadoop","_id":"ck0q92d0v000e7guolygnrlc5"},{"name":"hive","_id":"ck0q92d0x000h7guoyxuc3tge"},{"name":"udf","_id":"ck0q92d0z000k7guol4h8wpmr"},{"name":"kafka","_id":"ck0q92d16000t7guo1fbpr4zx"},{"name":"日志处理","_id":"ck0q92d18000v7guodrxx87z0"},{"name":"log4j","_id":"ck0q92d19000x7guom0nlkl0x"},{"name":"spring","_id":"ck0q92d1l00157guouosgj16w"}]}}