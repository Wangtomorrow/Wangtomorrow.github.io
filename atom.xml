<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Olicity Forever</title>
  
  <subtitle>菜鸡与代码的日常</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wangtomorrow.github.io/"/>
  <updated>2018-09-13T10:28:57.804Z</updated>
  <id>https://wangtomorrow.github.io/</id>
  
  <author>
    <name>Olicity</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kafka及Spring&amp;Kafka整合</title>
    <link href="https://wangtomorrow.github.io/post/85675a3e.html"/>
    <id>https://wangtomorrow.github.io/post/85675a3e.html</id>
    <published>2018-09-13T08:05:01.000Z</published>
    <updated>2018-09-13T10:28:57.804Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;由于某项目的消息队列使用了Spring整合Kafka，开发中我需要使用kafka客户端模拟生产者和消费者。简单了解了一下Kafka，扫盲贴，先标记一下，日后再深入学习。<br><a id="more"></a></p><h2 id="一、Kafka简介"><a href="#一、Kafka简介" class="headerlink" title="一、Kafka简介"></a>一、Kafka简介</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p>&emsp; kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。<br>&emsp;在大数据系统中，常常会碰到一个问题，整个大数据是由各个子系统组成，数据需要在各个子系统中高性能，低延迟的不停流转。传统的企业消息系统并不是非常适合大规模的数据处理。为了已在同时搞定在线应用（消息）和离线应用（数据文件，日志）Kafka就出现了。<br>&emsp;简单点概括一下：Kafka是一个分布式的，可划分的，高性能，低延迟的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。</p><h3 id="1-2-特点"><a href="#1-2-特点" class="headerlink" title="1.2 特点"></a>1.2 特点</h3><pre><code>* 高吞吐量* 可进行持久化操作* 分布式</code></pre><h3 id="1-3-组件"><a href="#1-3-组件" class="headerlink" title="1.3 组件"></a>1.3 组件</h3><p>&emsp;Topic，Broker，Partition，Message，Producer，Consumer,Zookpeer</p><h4 id="1-3-1-名词解释"><a href="#1-3-1-名词解释" class="headerlink" title="1.3.1 名词解释"></a>1.3.1 名词解释</h4><pre><code>服务：Topic：主题，Kafka处理的消息的不同分类。Broker：消息代理，Kafka集群中的一个kafka服务节点称为一个broker，主要存储消息数据。存在硬盘中。每个topic都是有分区的。Partition：Topic物理上的分组，一个topic在broker中被分为1个或者多个partition，分区在创建topic的时候指定。Message：消息，是通信的基本单位，每个消息都属于一个partition服务相关：Producer：消息和数据的生产者，向Kafka的一个topic发布消息。Consumer：消息和数据的消费者，定于topic并处理其发布的消息。Zookeeper：协调kafka的正常运行。</code></pre><h3 id="1-4-应用场景"><a href="#1-4-应用场景" class="headerlink" title="1.4 应用场景"></a>1.4 应用场景</h3><p>构建实时的流数据管道，可靠地获取系统和应用程序之间的数据。<br>构建实时流的应用程序，对数据流进行转换或反应。  </p><h2 id="二、Kafka搭建"><a href="#二、Kafka搭建" class="headerlink" title="二、Kafka搭建"></a>二、Kafka搭建</h2><h3 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1 安装"></a>2.1 安装</h3><p>&emsp;教程很多，就不写了。 </p><h3 id="2-2-配置"><a href="#2-2-配置" class="headerlink" title="2.2 配置"></a>2.2 配置</h3><p>&emsp;配置文件放在kafka下config下</p><pre><code>* consumer.properites 消费者配置* producer.properties 生产者配置* server.properties kafka服务器的配置    broker.id 申明当前kafka服务器在集群中的唯一ID，需配置为integer,并且集群中的每一个kafka服务器的id都应是唯一的    listeners 申明此kafka服务器需要监听的端口号，如果是在本机上跑虚拟机运行可以不用配置本项，默认会使用localhost的地址，如果是在远程服务器上运行则必须配置，例如：              listeners=PLAINTEXT:// 192.168.180.128:9092。并确保服务器的9092端口能够访问    zookeeper.connect 申明kafka所连接的zookeeper的地址 ，需配置为zookeeper的地址</code></pre><p>&emsp;上面配置文件中listeners的配置尤其注意，刚开始整的时候，没注意自己编写producer和cusmer时报错，如下：</p><pre><code>Connection to node -1 could not be established. Broker may not be available.</code></pre><p>&emsp;就是因为配置文件中的PLAINTEXT跟我请求的内容不同。</p><p>&emsp;具体配置教程很多，也不写了。</p><h2 id="三、Kafka操作"><a href="#三、Kafka操作" class="headerlink" title="三、Kafka操作"></a>三、Kafka操作</h2><h3 id="3-1-Topic操作"><a href="#3-1-Topic操作" class="headerlink" title="3.1 Topic操作"></a>3.1 Topic操作</h3><h4 id="3-1-1-创建Topic"><a href="#3-1-1-创建Topic" class="headerlink" title="3.1.1 创建Topic"></a>3.1.1 创建Topic</h4><pre><code>kafka-topics.sh --create --topic hbase --zookeeper ip1:port --partitions 3 --replication-factor 1创建topic过程的问题，replication-factor个数不能超过broker的个数创建topic后，可以在../data/kafka目录查看到分区的目录</code></pre><h4 id="3-1-2-查看Topic列表"><a href="#3-1-2-查看Topic列表" class="headerlink" title="3.1.2 查看Topic列表"></a>3.1.2 查看Topic列表</h4><pre><code>kafka-topics.sh --list --zookeeper ip:port</code></pre><h4 id="3-1-3-查看某一个具体的Topic"><a href="#3-1-3-查看某一个具体的Topic" class="headerlink" title="3.1.3 查看某一个具体的Topic"></a>3.1.3 查看某一个具体的Topic</h4><pre><code>kafka-topics.sh --describe xxx --zookeeper ip:port</code></pre><h4 id="3-1-4-修改Topic"><a href="#3-1-4-修改Topic" class="headerlink" title="3.1.4 修改Topic"></a>3.1.4 修改Topic</h4><pre><code>kafka-topics.sh --alter --topic topic-test --zookeeper ip:port --partitions 3不能修改replication-factor，以及只能对partition个数进行增加，不能减少</code></pre><h4 id="3-1-5-删除Topic"><a href="#3-1-5-删除Topic" class="headerlink" title="3.1.5 删除Topic"></a>3.1.5 删除Topic</h4><pre><code>kafka-topics.sh --delete --topic topic-test --zookeeper ip:port彻底删除一个topic，需要在server.properties中配置delete.topic.enable=true，否则只是标记删除配置完成之后，需要重启kafka服务。</code></pre><h3 id="3-2-生产者操作"><a href="#3-2-生产者操作" class="headerlink" title="3.2 生产者操作"></a>3.2 生产者操作</h3><pre><code>sh kafka-console-producer.sh --broker-list ip1:port,ip2:port,ip3:port --sync --topic kafka-topic-test生产数据的时候需要指定：当前数据流向哪个broker，以及哪一个topic</code></pre><h3 id="3-3-消费者操作"><a href="#3-3-消费者操作" class="headerlink" title="3.3 消费者操作"></a>3.3 消费者操作</h3><pre><code>sh kafka-console-consumer.sh --zookeeper ip1:port,ip2:port,ip3:port --topic kafka-topic-test --from-beginning--from-begining 获取最新以及历史数据黑白名单（暂时未用到）--blacklist 后面跟需要过滤的topic的列表，使用&quot;,&quot;隔开，意思是除了列表中的topic之外，都能接收其它topic的数据--whitelist 后面跟需要过滤的topic的列表，使用&quot;,&quot;隔开，意思是除了列表中的topic之外，都不能接收其它topic的数据</code></pre><h2 id="四、Springboot整合Kafka"><a href="#四、Springboot整合Kafka" class="headerlink" title="四、Springboot整合Kafka"></a>四、Springboot整合Kafka</h2><p>这个只是个人使用的简单的测试环境搭建，可能有很多地方有问题，以后深入学习时再检查。</p><h3 id="4-1-整合"><a href="#4-1-整合" class="headerlink" title="4.1 整合"></a>4.1 整合</h3><p>&emsp;springboot集成kafka的默认配置都在org.springframework.boot.autoconfigure.kafka包里面。直接使用即可。flag=深入学习kafka。</p><h3 id="4-2-pom-xml配置"><a href="#4-2-pom-xml配置" class="headerlink" title="4.2 pom.xml配置"></a>4.2 pom.xml配置</h3><pre><code>&lt;dependency&gt;   &lt;!--引入spring和kafka整合的jar--&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-stream-kafka&lt;/artifactId&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;                    &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;                    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;                    &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;            &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;            &lt;version&gt;1.0.1&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-task-core&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;            &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;            &lt;version&gt;1.3.5.RELEASE&lt;/version&gt;&lt;!--$NO-MVN-MAN-VER$--&gt;            &lt;/dependency&gt;        &lt;dependency&gt;</code></pre><h3 id="4-3-Producer配置"><a href="#4-3-Producer配置" class="headerlink" title="4.3 Producer配置"></a>4.3 Producer配置</h3><pre><code>@Configuration@EnableKafkapublic class KafkaProducer {    public Map&lt;String, Object&gt; producerConfigs() {        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);        props.put(ProducerConfig.RETRIES_CONFIG, KafkaConfig.PRODUCER_RETRIES);        props.put(ProducerConfig.BATCH_SIZE_CONFIG, KafkaConfig.PRODUCER_BATCH_SIZE);        props.put(ProducerConfig.LINGER_MS_CONFIG, KafkaConfig.PRODUCER_LINGER_MS);        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, KafkaConfig.PRODUCER_BUFFER_MEMORY);        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);        props.put(&quot;advertised.host.name&quot;,KafkaConfig.BOOTSTRAP_SERVERS);        props.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;1&quot;);        System.out.println(&quot;KafkaConfig.BOOTSTRAP_SERVERS:&quot;+KafkaConfig.BOOTSTRAP_SERVERS);        return props;    }    /** 获取工厂 */    public ProducerFactory&lt;String, String&gt; producerFactory() {        return new DefaultKafkaProducerFactory&lt;&gt;(producerConfigs());    }    /** 注册实例 */    @Bean    public KafkaTemplate&lt;String, String&gt; kafkaTemplate() {        return new KafkaTemplate&lt;&gt;(producerFactory());    }}</code></pre><h3 id="4-4-使用生产者"><a href="#4-4-使用生产者" class="headerlink" title="4.4 使用生产者"></a>4.4 使用生产者</h3><pre><code>@Autowiredprivate KafkaTemplate&lt;String, String&gt; kafkaTemplate;kafkaTemplate.send(&quot;kafka-topic-test&quot;, &quot;helloWorld&quot;);</code></pre><h3 id="4-5-Consumer配置"><a href="#4-5-Consumer配置" class="headerlink" title="4.5 Consumer配置"></a>4.5 Consumer配置</h3><pre><code>@Configuration@EnableKafkapublic class KafkaConsumer {    private final static Logger log = LoggerFactory.getLogger(KafkaConsumer .class);    @KafkaListener(topics = {&quot;kafka-topic-test&quot;})    public void consume(ConsumerRecord&lt;?, ?&gt; record) {        String topic = record.topic();        String value = record.value().toString();        System.out.println(&quot;partitions:&quot;+record.partition()+&quot;,&quot;+&quot;offset:&quot;+record.offset()+&quot;,value=&quot;+value);        MqConsumerRunnable runnable = new MqConsumerRunnable(topic,value);        executor.execute(runnable);    }    public Map&lt;String, Object&gt; consumerConfigs() {        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, &quot;100&quot;);        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);        props.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;1&quot;);        props.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);// 一般配置earliest 或者latest 值        return props;    }    /** 获取工厂 */    public ConsumerFactory&lt;String, String&gt; consumerFactory() {        return new DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs());    }    /** 获取实例 */    @Bean    public KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;String, String&gt;&gt; kafkaListenerContainerFactory() {        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();        factory.setConsumerFactory(consumerFactory());        factory.setConcurrency(3);        factory.getContainerProperties().setPollTimeout(3000);        return factory;    }}</code></pre><h3 id="4-6-使用消费者"><a href="#4-6-使用消费者" class="headerlink" title="4.6 使用消费者"></a>4.6 使用消费者</h3><pre><code>public class KafkaMessageListener implements MessageListener&lt;String, String&gt; {    private static Logger LOG = LoggerFactory.getLogger(KafkaMessageListener.class);    @Autowired    private AppProperties appProperties;    @Override    public void onMessage(ConsumerRecord&lt;String, String&gt; data) {        LOG.info(&quot;消费消息topic：{} value {}&quot;, data.topic(), data.value());        String topic = data.topic();        String content = data.value();        //可同时监听多个topic，根据不同topic处理不同的业务        if (topic.equals(&quot;topica&quot;)) {                       LOG.info(&quot;###############topic:{} value:{}&quot; ,topic,content);        } else if (topic.equals(&quot;topicb&quot;)) {         LOG.info(&quot;###############topic:{} value:{}&quot; ,topic,content);        }     }}</code></pre><h3 id="4-7-注意"><a href="#4-7-注意" class="headerlink" title="4.7 注意"></a>4.7 注意</h3><pre><code>kafkaTemplate.send(&quot;kafka-topic-test&quot;, &quot;helloWorld&quot;);@KafkaListener(topics = {&quot;kafka-topic-test&quot;})topic需要对应</code></pre><h4 id="4-8-使用"><a href="#4-8-使用" class="headerlink" title="4.8 使用"></a>4.8 使用</h4><p>&emsp;本地运行以后，到kafka服务器上可以进行消费者和生产者的模拟发送与接收信息。</p><h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p>&emsp;上述方法进行模拟测试，可以测试，但是总感觉问题很大，却又找不出问题，这个后期再说吧，先凑合用。<br>&emsp;有关Kafka的具体学习，后期补上。</p><h2 id="六、相关链接"><a href="#六、相关链接" class="headerlink" title="六、相关链接"></a>六、相关链接</h2><p>感谢各位大佬：</p><ul><li><a href="https://www.cnblogs.com/yangxiaoyi/p/7359236.html" target="_blank" rel="noopener">kafka 基础知识梳理</a></li><li><a href="https://www.cnblogs.com/hei12138/p/7805475.html" target="_blank" rel="noopener">kafka实战</a></li><li><a href="http://blog.51cto.com/xpleaf/2090847" target="_blank" rel="noopener">kafka笔记整理</a></li><li><a href="https://www.cnblogs.com/yepei/p/6197236.html" target="_blank" rel="noopener">kafka介绍</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;由于某项目的消息队列使用了Spring整合Kafka，开发中我需要使用kafka客户端模拟生产者和消费者。简单了解了一下Kafka，扫盲贴，先标记一下，日后再深入学习。&lt;br&gt;
    
    </summary>
    
    
      <category term="Kafka" scheme="https://wangtomorrow.github.io/tags/Kafka/"/>
    
      <category term="Springboot Kafka" scheme="https://wangtomorrow.github.io/tags/Springboot-Kafka/"/>
    
      <category term="笔记" scheme="https://wangtomorrow.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>spring学习记录(一)</title>
    <link href="https://wangtomorrow.github.io/post/61f45dcf.html"/>
    <id>https://wangtomorrow.github.io/post/61f45dcf.html</id>
    <published>2018-09-11T06:45:18.000Z</published>
    <updated>2018-09-12T08:16:57.224Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;最近在做后端的开发，用到了spring，借这个机会给spring捋一遍吧。东西有点多，需要点时间，抽空学。记录(一)主要是写一点spring最基础的东西。<br><a id="more"></a></p><h2 id="一、Spring概述"><a href="#一、Spring概述" class="headerlink" title="一、Spring概述"></a>一、Spring概述</h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h3><p>&emsp;Spring是一个基于 <strong>控制反转（IOC）</strong> 和 <strong>面向切面（AOP）</strong> 的结构J2EE系统的框架。从简单性、可测试性和松耦合的角度而言，任何Java应用都可以从Spring中受益。<br>&emsp;控制反转(IOC)是一个通用概念，Spring最认同的技术是控制反转的 <strong>依赖注入（DI）</strong> 模式。简单地说就是拿到的对象的属性，已经被注入好相关值了，直接使用即可。<br>&emsp;面向切面编程（AOP），一个程序中跨越多个点的功能被称为横切关注点，这些横切关注点在概念上独立于应用程序的业务逻辑。Spring 框架的 AOP 模块提供了面向方面的程序设计实现，可以定义诸如方法拦截器和切入点等，从而使实现功能的代码彻底的解耦出来。  </p><h3 id="2-Spring关键策略"><a href="#2-Spring关键策略" class="headerlink" title="2.Spring关键策略"></a>2.Spring关键策略</h3><pre><code>* 基于POJO的轻量级和最小侵入性编程* 通过依赖注入和面向接口实现松耦合* 基于切面和惯例进行声明式编程* 通过切面和模板减少样板式代码</code></pre><h3 id="3-Spring优点"><a href="#3-Spring优点" class="headerlink" title="3.Spring优点"></a>3.Spring优点</h3><pre><code>*方便解耦，简化开发 （高内聚低耦合）     Spring就是一个大工厂（容器），可以将所有对象创建和依赖关系维护，交给Spring管理     spring工厂是用于生成bean*AOP编程的支持     Spring提供面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等功能    声明式事务的支持     只需要通过配置就可以完成对事务的管理，而无需手动编程*方便程序的测试     Spring对Junit4支持，可以通过注解方便的测试Spring程序*方便集成各种优秀框架     Spring不排斥各种优秀的开源框架，其内部提供了对各种优秀框架（如：Struts、Hibernate、MyBatis、Quartz等）的直接支持*降低JavaEE API的使用难度     Spring 对JavaEE开发中非常难用的一些API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低</code></pre><h2 id="二、控制反转（IOC）"><a href="#二、控制反转（IOC）" class="headerlink" title="二、控制反转（IOC）"></a>二、控制反转（IOC）</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h3><p>&emsp;代码之间的 <strong>耦合性</strong> 具有两面性。耦合是必须的，但应小心谨慎的管理。<br>&emsp;DI会将所以来的关系自动交给目标对象，而不是让对象自己去获取依赖。– <strong>松耦合</strong></p><h3 id="2-获取对象的方式进行比较"><a href="#2-获取对象的方式进行比较" class="headerlink" title="2.获取对象的方式进行比较"></a>2.获取对象的方式进行比较</h3><p>传统方式：<br>通过new关键字主动创建一个对象<br>IOC方式：<br>对象的生命周期由Spring来管理，直接从Spring那里去获取一个对象。 IOC是反转控制 (Inversion Of Control)的缩写，就像控制权从本来在自己手里，交给了Spring。</p><h3 id="3-装配"><a href="#3-装配" class="headerlink" title="3.装配"></a>3.装配</h3><p>&emsp;创建应用组件之间协作的行为通常称为装配。常用XML装配方式。Spring配置文件。</p><pre><code class="xml">*使用构造器进行配置    <span class="tag">&lt;<span class="name">bean</span>&gt;</span> 定义JavaBean，配置需要创建的对象    id/name ：用于之后从spring容器获得实例时使用的    class ：需要创建实例的全限定类名    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"c"</span> <span class="attr">class</span>=<span class="string">"pojo.Category"</span>&gt;</span>        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"name"</span> <span class="attr">value</span>=<span class="string">"category 1"</span> /&gt;</span> <span class="comment">&lt;!--category对象--&gt;</span>    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span>    <span class="comment">&lt;!--创建product对象的时候注入了一个category对象，使用ref--&gt;</span>    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">name</span>=<span class="string">"p"</span> <span class="attr">class</span>=<span class="string">"pojo.Product"</span>&gt;</span> <span class="comment">&lt;!--product类中添加category对象的set&amp;get方法--&gt;</span>        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"name"</span> <span class="attr">value</span>=<span class="string">"product1"</span> /&gt;</span>   <span class="comment">&lt;!--product对象--&gt;</span>        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"category"</span> <span class="attr">ref</span>=<span class="string">"c"</span> /&gt;</span>    <span class="comment">&lt;!--为product对象注入category对象--&gt;</span>    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span>*使用注解的方式进行配置    xml中添加  <span class="tag">&lt;<span class="name">context:annotation-config</span>/&gt;</span> 注释掉注入category        *@Autowired注解方法            在Product.java的category属性前加上@Autowired注解            @Autowired            private Category category;            或者            @Autowired            public void setCategory(Category category)         *@Resource            @Resource(name="c")            private Category category;*对bean进行注解配置    直接xml中添加  <span class="tag">&lt;<span class="name">context:component-scan</span> <span class="attr">base-package</span>=<span class="string">"pojo"</span>/&gt;</span>    就是告知spring，所有的bean都在pojo包下    通过@Component注解        在类前加入  @Component("c")  表明此类是bean        因为配置从applicationContext.xml中移出来了，所以属性初始化放在属性声明上进行了。        private String name="product 1";        private String name="category 1";        同时@Autowired也需要在Product.java的category属性前加上*Spring javaConfig</code></pre><p><a href="https://github.com/Wangtomorrow/Spring/tree/master/spring" target="_blank" rel="noopener">源码参照</a></p><h2 id="三、面向切面（AOP）"><a href="#三、面向切面（AOP）" class="headerlink" title="三、面向切面（AOP）"></a>三、面向切面（AOP）</h2><pre><code>未完待续。。。先整kafka。。。。</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;最近在做后端的开发，用到了spring，借这个机会给spring捋一遍吧。东西有点多，需要点时间，抽空学。记录(一)主要是写一点spring最基础的东西。&lt;br&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://wangtomorrow.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="spring" scheme="https://wangtomorrow.github.io/tags/spring/"/>
    
      <category term="IOC" scheme="https://wangtomorrow.github.io/tags/IOC/"/>
    
      <category term="DI" scheme="https://wangtomorrow.github.io/tags/DI/"/>
    
  </entry>
  
  <entry>
    <title>开始吧</title>
    <link href="https://wangtomorrow.github.io/post/9f79558f.html"/>
    <id>https://wangtomorrow.github.io/post/9f79558f.html</id>
    <published>2018-09-10T10:36:06.000Z</published>
    <updated>2018-09-11T06:08:54.936Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;入职一个月了，暑假的时候来公司做实习生，目前是在做hadoop的运维和别的项目的后端开发以及一些日常琐事的处理。<br><a id="more"></a><br>&emsp;刚来的时候先是学习了hadoop的搭建，当时是搭建的Hadoop2.4版本（有时间学习一下3.0版本的新特性，自己试着搭建一下），再后来搭建了HA，加入了zookeeper。在此基础上，又学习了HDFS的机制与shell操作，yarn资源调度，还有mapreduce的工作机制，包括shuffle，partitioner，combiner等，并写了最基础的wordcount，现在做的也是mapreduce的日常运维与开发。后来又学习了hive表的机制与操作（写了好长一段时间sql语句，发现自己基础真的薄弱）。相关笔记：<a href="https://github.com/Wangtomorrow/Hadoop" target="_blank" rel="noopener">hadoop学习记录</a><br>&emsp;现在也算简单的hadoop入门了，还有公司的环境也是一个比较好的机会吧。这一个月的时间里在学习，熟悉公司业务、框架的过程中，发现了自己存在很大的问题。感觉自己涉猎过很多东西，其实真正用起来的时候发现自己其实真的是啥也会，啥也不会。说白了，还是看的多了，做的少了。<br>&emsp;最近一段时间，在做某项目的后端开发，hadoop的学习先暂告一段落了。就标记一下，hadoop接下来一段时间要学的东西吧。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">*hadoop3.0新特新及搭建</span><br><span class="line">*linux的操作，各种操作指令</span><br><span class="line">*hive表相关，涉及到sql</span><br><span class="line">*hadoop及HDFS深度学习研究</span><br><span class="line">*pig</span><br><span class="line">*hbase</span><br><span class="line">*redis</span><br><span class="line">*MongoDB</span><br><span class="line">*yarn相关细节</span><br><span class="line">*zookeeper相关细节</span><br><span class="line">*sqoop</span><br><span class="line">*Flume对比DataX</span><br><span class="line">*Spark全家桶</span><br><span class="line">*Storm对比Spark Streaming</span><br><span class="line">*kafka</span><br><span class="line">*JVM虚拟机</span><br><span class="line"></span><br><span class="line">*调度监控管理系统、数据应用等很多东西</span><br><span class="line"></span><br><span class="line">*大数据中涉及到的机器学习，Mahout、Spark MLLib等。</span><br><span class="line">（中文分词、自然语言处理、推荐算法、分类算法、回归算法、聚类算法、神经网络与深度学习）</span><br><span class="line"> 关于机器学习，可以先放一放。数学基础很重要。</span><br><span class="line"></span><br><span class="line">*会涉及到并发、多线程、负载均衡、分布式、云计算等问题。</span><br></pre></td></tr></table></figure></p><p>&emsp;关于大数据目前就想到这么多，很多东西还是得边学边加，实际项目中缺啥补啥吧。<br>&emsp;关于最近所做项目的后端，想了想，最近应该会去多学习kafka以及spring相关，可以借此机会给spring全家桶捣鼓一遍。<br>&emsp;以后大部分代码更新会在<a href="https://github.com/Wangtomorrow" target="_blank" rel="noopener">github</a>上，日常博客更新会在这里，csdn看内容吧，没营养的就不往那边整了。这边估计很少会有人看见吧，哈哈哈~~~<br>&emsp;写这个blog最初的想法，一是就当一个记事本了，记录自己学习上遇到的坑和笔记。二是为了督促和记录自己学习的。开始吧</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;入职一个月了，暑假的时候来公司做实习生，目前是在做hadoop的运维和别的项目的后端开发以及一些日常琐事的处理。&lt;br&gt;
    
    </summary>
    
    
      <category term="start" scheme="https://wangtomorrow.github.io/tags/start/"/>
    
  </entry>
  
</feed>
