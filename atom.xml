<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Olicity Forever</title>
  
  <subtitle>菜鸡与代码的日常</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wangtomorrow.github.io/"/>
  <updated>2019-03-28T07:14:42.973Z</updated>
  <id>https://wangtomorrow.github.io/</id>
  
  <author>
    <name>Olicity</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>shell中获取hdfs文件路径参数</title>
    <link href="https://wangtomorrow.github.io/post/ad3ef435.html"/>
    <id>https://wangtomorrow.github.io/post/ad3ef435.html</id>
    <published>2019-03-28T07:08:24.000Z</published>
    <updated>2019-03-28T07:14:42.973Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;算是个简单的工具吧。需求是这样的，有套脚本是不定期跑的累积表，所以需要知道上次跑到了哪天。累积表有个day_id分区，所以直接看表分区是最后的day_id就行。<br><a id="more"></a><br>不多比比直接上代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">hdfs_path=$1</span><br><span class="line">#获取hdfs最后一个时间分区时间参数脚本</span><br><span class="line">#注意分区在第几层改第二个print的参数</span><br><span class="line">last_data_date=`hadoop fs -ls  $hdfs_path | awk &apos;&#123;print $8&#125;&apos; |  awk -F&apos;/&apos; &apos;&#123;print $8&#125;&apos; | tail -n 1`</span><br><span class="line">#echo $last_data_date </span><br><span class="line">last_date=$&#123;last_data_date##*=&#125;</span><br><span class="line">echo $last_date</span><br></pre></td></tr></table></figure></p><p>获取其他信息同理。需要加别的条件就加grep。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;算是个简单的工具吧。需求是这样的，有套脚本是不定期跑的累积表，所以需要知道上次跑到了哪天。累积表有个day_id分区，所以直接看表分区是最后的day_id就行。&lt;br&gt;
    
    </summary>
    
    
      <category term="linux" scheme="https://wangtomorrow.github.io/tags/linux/"/>
    
      <category term="hdfs" scheme="https://wangtomorrow.github.io/tags/hdfs/"/>
    
      <category term="脚本" scheme="https://wangtomorrow.github.io/tags/%E8%84%9A%E6%9C%AC/"/>
    
      <category term="shell" scheme="https://wangtomorrow.github.io/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>hive日常使用的几个小技巧</title>
    <link href="https://wangtomorrow.github.io/post/e36ec0a7.html"/>
    <id>https://wangtomorrow.github.io/post/e36ec0a7.html</id>
    <published>2019-03-27T03:06:24.000Z</published>
    <updated>2019-03-28T07:42:25.077Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;长期维护中。。。。主要记录日常使用hive中会用到的小技巧<br><a id="more"></a></p><h3 id="1-简单查询不跑MapReduce"><a href="#1-简单查询不跑MapReduce" class="headerlink" title="1.简单查询不跑MapReduce"></a>1.简单查询不跑MapReduce</h3><p>&emsp;如果你想直接查询（select * from table），却不想执行MapReduce，可以使用FetchTask，FetchTask不同于MapReduce任务，它不会启动mapreduce，而是直接读取文件，输出结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;minimal&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;</span><br><span class="line">    Some select queries can be converted to single FETCH task </span><br><span class="line">    minimizing latency.Currently the query should be single </span><br><span class="line">    sourced not having any subquery and should not have</span><br><span class="line">    any aggregations or distincts (which incurrs RS), </span><br><span class="line">    lateral views and joins.</span><br><span class="line">    1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only</span><br><span class="line">    2. more    : SELECT, FILTER, LIMIT only (+TABLESAMPLE, virtual columns)</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p><p>&emsp;该参数默认值为minimal，表示运行“select * ”并带有limit查询时候，会将其转换为FetchTask；如果参数值为more，则select某一些列并带有limit条件时，也会将其转换为FetchTask任务。<br>&emsp;使用前提:单一数据源，即输入来源一个表或者分区；没有子查询；没有聚合运算和distinct；不能用于视图和join<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">实现：</span><br><span class="line">set hive.fetch.task.conversion=more</span><br></pre></td></tr></table></figure></p><h3 id="2-小文件合并"><a href="#2-小文件合并" class="headerlink" title="2.小文件合并"></a>2.小文件合并</h3><p>&emsp;Hive中存在过多的小文件会给namecode带来巨大的性能压力,同时小文件过多会影响JOB的执行，hadoop会将一个job转换成多个task，即使对于每个小文件也需要一个task去单独处理，task作为一个独立的jvm实例，其开启和停止的开销可能会大大超过实际的任务处理时间。hive输出最终是mr的输出，即reducer（或mapper）的输出，有多少个reducer（mapper）输出就会生成多少个输出文件，根据shuffle/sort的原理，每个文件按照某个值进行shuffle后的结果。为了防止生成过多小文件，hive可以通过配置参数在mr过程中合并小文件。而且在执行sql之前将小文件都进行Merge，也会提高程序的性能。我们可以从两个方面进行优化，其一是map执行之前将小文件进行合并会提高性能，其二是输出的时候进行合并压缩，减少IO压力。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Map操作之前合并小文件：</span><br><span class="line">    set mapred.max.split.size=2048000000</span><br><span class="line">    #每个Map最大输入大小设置为2GB（单位：字节）</span><br><span class="line">    </span><br><span class="line">    set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</span><br><span class="line">    #执行Map前进行小文件合并</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">输出时进行合并：</span><br><span class="line">    sethive.merge.mapfiles = true</span><br><span class="line">    #在Map-only的任务结束时合并小文件</span><br><span class="line"></span><br><span class="line">    set hive.merge.mapredfiles= true</span><br><span class="line">    #在Map-Reduce的任务结束时合并小文件</span><br><span class="line"></span><br><span class="line">    set hive.merge.size.per.task = 1024000000</span><br><span class="line">    #合并后文件的大小为1GB左右</span><br><span class="line"></span><br><span class="line">    set hive.merge.smallfiles.avgsize=1024000000</span><br><span class="line">    #当输出文件的平均大小小于1GB时，启动一个独立的map-reduce任务进行文件merge</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">如果需要压缩输出文件，就需要增加一个压缩编解码器，同时还有两个压缩方式和多种压缩编码器，压缩方式一个是压缩输出结果，一个是压缩中间结果，按照自己的需求选择，我需要的是gzip就选择的GzipCodec，同时也可以选择使用BZip2Codec、SnappyCodec、LzopCodec进行压缩。</span><br><span class="line">    压缩文件：</span><br><span class="line">    set hive.exec.compress.output=true;</span><br><span class="line">    #默认false，是否对输出结果压缩</span><br><span class="line"></span><br><span class="line">    set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;</span><br><span class="line">    #压缩格式设置</span><br><span class="line"></span><br><span class="line">    set mapred.output.compression.type=BLOCK;</span><br><span class="line">    #一共三种压缩方式（NONE, RECORD,BLOCK），BLOCK压缩率最高，一般用BLOCK。</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">老哥关于map和reduce个数配置的说明：</span><br><span class="line">https://irwenqiang.iteye.com/blog/1535809</span><br></pre></td></tr></table></figure><h3 id="3-动态分区"><a href="#3-动态分区" class="headerlink" title="3.动态分区"></a>3.动态分区</h3><p>&emsp;有时候需要根据数据去动态生成分区，这时候就需要用到动态分区<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition=true; </span><br><span class="line">#开启动态分区功能</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;  </span><br><span class="line">#表示允许所有分区都是动态的，否则必须有静态分区字段</span><br><span class="line">set hive.exec.max.dynamic.partitions=100000;</span><br><span class="line">#表示一个动态分区语句可以创建的最大动态分区个数，超出报错</span><br><span class="line">set hive.exec.max.dynamic.partitions.pernode=100000;</span><br><span class="line">#表示每个maper或reducer可以允许创建的最大动态分区个数，默认是100，超出则会报错。</span><br><span class="line">set hive.exec.max.created.files =10000</span><br><span class="line">#全局可以创建的最大文件个数，超出报错。</span><br></pre></td></tr></table></figure></p><p>注意：动态分区不允许主分区采用动态列而副分区采用静态列，这样将导致所有的主分区都要创建副分区静态列所定义的分区</p><h3 id="4-容错"><a href="#4-容错" class="headerlink" title="4.容错"></a>4.容错</h3><p>&emsp;有时候因为各种原因难免会有hive执行时出错的问题，例如个别数据不规范等。这时候需要允许部分错误发生。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.map.failures.maxpercent=10; </span><br><span class="line">#设置map任务失败的比率，可以容许10%的任务失败</span><br><span class="line">set mapreduce.reduce.failures.maxpercent = 10; </span><br><span class="line">#设置reduce任务失败的比率，可以容许10%的任务失败</span><br></pre></td></tr></table></figure></p><h3 id="5-交集并集差集"><a href="#5-交集并集差集" class="headerlink" title="5.交集并集差集"></a>5.交集并集差集</h3><p>&emsp;交集（join），并集（union all）。这俩简单没啥说的。差集（left outer join、not in、not exists）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">具体参考：</span><br><span class="line">https://blog.csdn.net/u010003835/article/details/80928732</span><br></pre></td></tr></table></figure></p><p>这里涉及到left outer join和left semi join<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#left semi join解决的是IN/EXISTS的问题</span><br><span class="line">select a.id from a left semi join b on (a.id = b.id);</span><br><span class="line"></span><br><span class="line">#left outer join解决的是a差b的问题</span><br><span class="line">select a.id from a left outer join b on (a.id = b.id) where b.id is null;</span><br></pre></td></tr></table></figure></p><h3 id="6-reduce资源申请时间"><a href="#6-reduce资源申请时间" class="headerlink" title="6.reduce资源申请时间"></a>6.reduce资源申请时间</h3><p>&emsp;为了节省时间，map未执行完时就申请reduce资源。mapreduce.job.reduce.slowstart.completedmaps，这个参数可以控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源。<br>配置在mapred-site.xml，如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;</span><br><span class="line">         mapreduce.job.reduce.slowstart.completedmaps</span><br><span class="line">    &lt;/name&gt;</span><br><span class="line">    &lt;value&gt;</span><br><span class="line">        0.05</span><br><span class="line">    &lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">        Fraction of the number of maps in the job which should be complete before reduces are scheduled for the job.</span><br><span class="line">     &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p><p>&emsp;默认map5%时申请reduce资源，开始执行reduce操作，reduce可以开始进行拷贝map结果数据和做reduce shuffle操作。<br>&emsp;mapreduce.job.reduce.slowstart.completedmaps这个参数如果设置的过低，那么reduce就会过早地申请资源，造成资源浪费；如果这个参数设置的过高，比如为1，那么只有当map全部完成后，才为reduce申请资源，开始进行reduce操作，实际上是串行执行，不能采用并行方式充分利用资源。<br>&emsp;如果map数量比较多，一般建议提前开始为reduce申请资源。</p><h3 id="7-三种常用的判断空后赋值的方法"><a href="#7-三种常用的判断空后赋值的方法" class="headerlink" title="7.三种常用的判断空后赋值的方法"></a>7.三种常用的判断空后赋值的方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1)if(boolean testCondition, T valueTrue, T valueFalseOrNull)</span><br><span class="line">说明：当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNull</span><br><span class="line">应用：select if(a is null,0,1) from table;</span><br><span class="line"></span><br><span class="line">2)COALESCE(T v1, T v2, …) </span><br><span class="line">说明：返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL</span><br><span class="line">应用：select coalesce(a,0) from table;</span><br><span class="line"></span><br><span class="line">3）case a when b then c [else f] end</span><br><span class="line">说明：如果a等于b，那么返回c；如果a等于d，那么返回e；否则返回f</span><br><span class="line">应用：select case a</span><br><span class="line">            when a is null</span><br><span class="line">            then a=0</span><br><span class="line">            else a</span><br><span class="line">            end</span><br><span class="line">      from table;</span><br></pre></td></tr></table></figure><h3 id="8-hive-transform"><a href="#8-hive-transform" class="headerlink" title="8.hive transform"></a>8.hive transform</h3><p>&emsp;hive自定义函数除了支持udf还支持transform,可以引入脚本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">首先添加脚本文件</span><br><span class="line">add file /data/users/olicity/a.py;</span><br><span class="line">select transform(a, b, c, d, e) using &apos;python a.py&apos; as (f , g)</span><br><span class="line">from table;</span><br></pre></td></tr></table></figure></p><p>自己没有比较过速度，不过看大佬们说是要比udf慢很多。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;长期维护中。。。。主要记录日常使用hive中会用到的小技巧&lt;br&gt;
    
    </summary>
    
    
      <category term="hadoop" scheme="https://wangtomorrow.github.io/tags/hadoop/"/>
    
      <category term="hive" scheme="https://wangtomorrow.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>kafka一个服务配置到不同topic的group</title>
    <link href="https://wangtomorrow.github.io/post/cd564347.html"/>
    <id>https://wangtomorrow.github.io/post/cd564347.html</id>
    <published>2019-02-27T04:14:00.000Z</published>
    <updated>2019-02-28T07:47:03.041Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;标题比较长，实在想不出什么好的描述。大概要解决的问题就是，同一个服务同时监听多个topic，且在每个topic中的group都不相同，具体看问题描述吧。<br><a id="more"></a></p><h2 id="一、问题背景"><a href="#一、问题背景" class="headerlink" title="一、问题背景"></a>一、问题背景</h2><p>&emsp;前几天部署了一套系统，每个服务都搭建了多个节点，而且是没有主从关系的节点。每个服务中有很多东西是放到缓存中的，配置多节点之后，相同服务的不同节点出现了缓存不一致的问题。  </p><h2 id="二、问题描述"><a href="#二、问题描述" class="headerlink" title="二、问题描述"></a>二、问题描述</h2><p>&emsp;刚开始想出一种解决方案，监听同一个topic1，每个节点分到一个group中，这样每次生产者生产消息后，kafka会将消息分发到所有group中，消息中带一个消息类型字段（mq_type）。<br>各个节点由于处于不同group中都会消费此消息，然后根据mq_type判断是否该处理此消息。<br>&emsp;然而，pass。原因:由于此系统（系统B）中的服务1还与系统A有消费与生产消息的关系，都放到一个topic下数据不规范。而且如果多个服务1同时消费消息，会进行读表改表操作，还得做处理。</p><p>&emsp;emmm，又想出了一种解决方案，系统B中每个节点还是分到不同的group中，当某个服务1消费到系统A发送的消息，需要刷新缓存时，该节点对所有节点通过系统B内部的消息队列topic2进行广播，各个服务接收到消费消息后根据消息类型进行缓存的更新。<br>具体系统图如下：<br><img src="https://github.com/Wangtomorrow/Resource/blob/master/system.png?raw=true" alt="系统图"><br><a href="https://github.com/Wangtomorrow/Resource/blob/master/system.png?raw=true" target="_blank" rel="noopener">图片备用链接</a><br>&emsp;<strong>ps：以上区分两个topic是为了规范来自不同的渠道的数据走不同的topic，如果没有这种要求完全没有必要做如下这种操作，可以直接通过group和消息内容去做区分</strong><br>&emsp;如上图，系统A通过topic1向系统B中的服务1发送消息，系统B中服务1和服务2以及他们的其他节点在系统B中通过topic2发送消息。<br>&emsp;可以看出，系统B中的服务1扮演了三个角色：系统A发送消息的消费者，系统B内部消息的生产者和消费者。可以得出如下问题：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">对于服务1，需要将其配置为监听两个topic，分别监听topic1和topic2</span><br><span class="line">系统A向系统B发送消息时，服务1以及他的其他节点处于topic1的同一个group下，即只有一个服务1节点会去消费系统A发来的消息</span><br><span class="line">系统B内部之间发送消息时，每个服务和节点都处于topic2的不同group下</span><br></pre></td></tr></table></figure></p><p>&emsp;说到这里，其实就清楚很多了。其实就是想让服务1-1和他的其他节点在topic1中都处于group-A2B中，服务1-1在topic2中处于group-service1-1中，服务1-2在topic2中处于group-service1-2中。</p><h2 id="三、需求实现"><a href="#三、需求实现" class="headerlink" title="三、需求实现"></a>三、需求实现</h2><h3 id="3-1-代码基础"><a href="#3-1-代码基础" class="headerlink" title="3.1 代码基础"></a>3.1 代码基础</h3><p>&emsp;kafka的基础代码请参照我的以下两篇博客,本次修改都是基于这些代码的基础上改造的<br>&emsp;<a href="https://wangtomorrow.github.io/post/85675a3e.html">Kafka及Spring&amp;Kafka整合</a><br>&emsp;<a href="https://wangtomorrow.github.io/post/bf5e9970.html">kafka动态配置topic</a></p><h3 id="3-2-生产者"><a href="#3-2-生产者" class="headerlink" title="3.2 生产者"></a>3.2 生产者</h3><p>&emsp;kafka生产者发送消息时，会向该topic下的所有group发送消息，而每个group只会有一个消费者进行消费。所以生产者不用进行更改。</p><h3 id="3-3-消费者"><a href="#3-3-消费者" class="headerlink" title="3.3 消费者"></a>3.3 消费者</h3><h4 id="3-3-1-消费者的配置"><a href="#3-3-1-消费者的配置" class="headerlink" title="3.3.1 消费者的配置"></a>3.3.1 消费者的配置</h4><p>&emsp;以下消费者以服务1-1为例，其他节点服务同理。<br>&emsp;由于同一个服务要扮演两个消费者，所以我们需要不同的配置文件用来生成不同的消费者<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">//首先是获取公共配置方法</span><br><span class="line">    public Map&lt;String, Object&gt; getCommonPropertis()&#123;</span><br><span class="line">        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);</span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);</span><br><span class="line">        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, &quot;100&quot;);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class="line">        props.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);// 一般配置earliest 或者latest 值</span><br><span class="line">        return props;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">//然后不同的用来生成不同消费者的工厂</span><br><span class="line">    //topic1的消费者</span><br><span class="line">    public ConsumerFactory&lt;String, String&gt; consumerFactoryA2B() &#123;</span><br><span class="line">        Map&lt;String, Object&gt; properties = getCommonPropertis();</span><br><span class="line">        //所在group</span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;group-A2B&quot;);</span><br><span class="line">        return new DefaultKafkaConsumerFactory&lt;String, String&gt;(properties);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    //系统B内topic2的每个服务的group我这里用服务名+ip+端口命名</span><br><span class="line">    String GROUP_NAME = &quot;service1-1-&quot;+serviceInfoUtil.getIpAddress()+&quot;-&quot;+serviceInfoUtil.getLocalPort();</span><br><span class="line">    </span><br><span class="line">    //topic2的消费者</span><br><span class="line">    public ConsumerFactory&lt;String, String&gt; consumerFactoryB2B()&#123;</span><br><span class="line">            Map&lt;String, Object&gt; properties = getCommonPropertis();</span><br><span class="line">            //所在group</span><br><span class="line">            properties.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_NAME);</span><br><span class="line">            return new DefaultKafkaConsumerFactory&lt;String, String&gt;(properties);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">//再通过不同的配置工厂生成实例bean</span><br><span class="line">    //topic1的消费者bean</span><br><span class="line">    @Bean</span><br><span class="line">    public KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;String, String&gt;&gt; kafkaListenerContainerFactoryA2B() &#123;</span><br><span class="line">        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();</span><br><span class="line">        factory.setConsumerFactory(consumerFactoryA2B());//通过不同工厂获取实例</span><br><span class="line">        factory.setConcurrency(3);</span><br><span class="line">        factory.getContainerProperties().setPollTimeout(3000);</span><br><span class="line">        return factory;</span><br><span class="line">    &#125;  </span><br><span class="line">    </span><br><span class="line">    //topic2的消费者bean</span><br><span class="line">    @Bean</span><br><span class="line">    public KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;String, String&gt;&gt; kafkaListenerContainerFactoryB2B() &#123;</span><br><span class="line">        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();</span><br><span class="line">        factory.setConsumerFactory(consumerFactoryB2B());//通过不同工厂获取实例</span><br><span class="line">        factory.setConcurrency(3);</span><br><span class="line">        factory.getContainerProperties().setPollTimeout(3000);</span><br><span class="line">        return factory;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><h4 id="3-3-2-消费者的使用"><a href="#3-3-2-消费者的使用" class="headerlink" title="3.3.2 消费者的使用"></a>3.3.2 消费者的使用</h4><p>&emsp;以上消费者的配置就算完成了，接下来就可以直接使用了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">     /**</span><br><span class="line">     * 监听B2B所有消息</span><br><span class="line">     * @param record</span><br><span class="line">     */</span><br><span class="line">    @KafkaListener(topics = &quot;#&#123;&apos;$&#123;kafka.B2B.listener_topics&#125;&apos;.split(&apos;,&apos;)&#125;&quot;,containerFactory = &quot;kafkaListenerContainerFactoryB2B&quot;)</span><br><span class="line">    public void B2Bconsume(ConsumerRecord&lt;?, ?&gt; record)&#123;</span><br><span class="line">        recordDeal(record);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 监听A2B的所有消息</span><br><span class="line">     * @param record</span><br><span class="line">     */</span><br><span class="line">    @KafkaListener(topics = &quot;#&#123;&apos;$&#123;kafka.A2B.listener_topics&#125;&apos;.split(&apos;,&apos;)&#125;&quot;,containerFactory = &quot;kafkaListenerContainerFactoryA2B&quot;)</span><br><span class="line">    public void A2Bconsume(ConsumerRecord&lt;?, ?&gt; record) &#123;</span><br><span class="line">        recordDeal(record);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">//containerFactory = &quot;kafkaListenerContainerFactoryA2B&quot;  主要就是这个containerFactory参数，用它控制是哪个实例</span><br></pre></td></tr></table></figure></p><h4 id="3-3-3-获取服务启动的ip和端口类"><a href="#3-3-3-获取服务启动的ip和端口类" class="headerlink" title="3.3.3 获取服务启动的ip和端口类"></a>3.3.3 获取服务启动的ip和端口类</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">@Configuration</span><br><span class="line">public class ServiceInfoUtil &#123;</span><br><span class="line">    public static String getIpAddress() throws UnknownHostException &#123;</span><br><span class="line">        InetAddress address = InetAddress.getLocalHost();</span><br><span class="line">        return address.getHostAddress();</span><br><span class="line">    &#125;</span><br><span class="line">    public static String getLocalPort() throws MalformedObjectNameException &#123;</span><br><span class="line">        MBeanServer beanServer = ManagementFactory.getPlatformMBeanServer();</span><br><span class="line">        Set&lt;ObjectName&gt; objectNames = beanServer.queryNames(new ObjectName(&quot;*:type=Connector,*&quot;),</span><br><span class="line">                Query.match(Query.attr(&quot;protocol&quot;), Query.value(&quot;HTTP/1.1&quot;)));</span><br><span class="line">        String port = objectNames.iterator().next().getKeyProperty(&quot;port&quot;);</span><br><span class="line">        return port;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-3-4-最后"><a href="#3-3-4-最后" class="headerlink" title="3.3.4 最后"></a>3.3.4 最后</h4><p>&emsp;这样修改后启动时，通过配置文件中的kafka.A2B.listener_topics去判断这个消费者该监听哪个topic，通过containerFactory = “kafkaListenerContainerFactoryA2B”判断这个消费者在这个topic中属于哪个group。<br>然后发送消息测试，成了。</p><h2 id="四、感谢大佬"><a href="#四、感谢大佬" class="headerlink" title="四、感谢大佬"></a>四、感谢大佬</h2><p>这几个大佬的对于kafka的group的讲解比较好：<br><a href="http://www.mrslee.cn/archives/54" target="_blank" rel="noopener">KAFKA 多个消费者同一个GROUPID，只有一个能收到消息的原因</a><br><a href="http://www.cnblogs.com/huxi2b/p/6223228.html" target="_blank" rel="noopener">Kafka消费组(consumer group)</a><br><a href="https://blog.csdn.net/caijiapeng0102/article/details/80765923" target="_blank" rel="noopener">springboot 集成kafka 实现多个customer不同group</a>  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;标题比较长，实在想不出什么好的描述。大概要解决的问题就是，同一个服务同时监听多个topic，且在每个topic中的group都不相同，具体看问题描述吧。&lt;br&gt;
    
    </summary>
    
    
      <category term="kafka" scheme="https://wangtomorrow.github.io/tags/kafka/"/>
    
      <category term="kafka-group" scheme="https://wangtomorrow.github.io/tags/kafka-group/"/>
    
  </entry>
  
  <entry>
    <title>hive开发udaf和udtf</title>
    <link href="https://wangtomorrow.github.io/post/37d63f90.html"/>
    <id>https://wangtomorrow.github.io/post/37d63f90.html</id>
    <published>2019-02-13T02:26:34.000Z</published>
    <updated>2019-02-27T03:53:39.732Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;之前开发过udf，但是udf只能处理一对一的情况，也就是一个输入对应一个输出。而日常开发中却会遇到多种情况，普通的udf不能满足，这时候就需要引入udtf和udaf了。<br><a id="more"></a></p><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><h3 id="1-1-UDAF"><a href="#1-1-UDAF" class="headerlink" title="1.1 UDAF"></a>1.1 UDAF</h3><p>&emsp;UDAF(User- Defined Aggregation Funcation)用户定义聚合函数，可对多行数据产生作用；等同与SQL中常用的SUM()，AVG()，也是聚合函数；<br>简单说就是多行输入一行输出。</p><h3 id="1-2-UDTF"><a href="#1-2-UDTF" class="headerlink" title="1.2 UDTF"></a>1.2 UDTF</h3><p>&emsp;UDTF(User-Defined Table-Generating Functions)用户定义表生成函数，用来解决输入一行输出多行；<br>简单说就是一行输入多行输出。</p><h2 id="二、UDAF编写例子"><a href="#二、UDAF编写例子" class="headerlink" title="二、UDAF编写例子"></a>二、UDAF编写例子</h2><h3 id="2-1-说明"><a href="#2-1-说明" class="headerlink" title="2.1 说明"></a>2.1 说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1.引入如下两下类</span><br><span class="line">    import org.apache.hadoop.hive.ql.exec.UDAF  </span><br><span class="line">    import org.apache.hadoop.hive.ql.exec.UDAFEvaluator  </span><br><span class="line">2.函数类需要继承UDAF类，计算类Evaluator实现UDAFEvaluator接口</span><br><span class="line">3.Evaluator需要实现UDAFEvaluator的init、iterate、terminatePartial、merge、terminate这几个函数。</span><br><span class="line">    a）init函数实现接口UDAFEvaluator的init函数。</span><br><span class="line">    b）iterate接收传入的参数，并进行内部的迭代。其返回类型为boolean。</span><br><span class="line">    c）terminatePartial无参数，其为iterate函数遍历结束后，返回遍历得到的数据，terminatePartial类似于 hadoop的Combiner。</span><br><span class="line">    d）merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean。</span><br><span class="line">    e）terminate返回最终的聚集函数结果。</span><br></pre></td></tr></table></figure><h3 id="2-2-实例"><a href="#2-2-实例" class="headerlink" title="2.2 实例"></a>2.2 实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">package hive.udaf;  </span><br><span class="line">  </span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDAF;  </span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;  </span><br><span class="line">//create temporary function udaf_avg &apos;hive.udaf.Avg&apos;</span><br><span class="line">public class Avg extends UDAF &#123;  </span><br><span class="line">    public static class AvgState &#123;  </span><br><span class="line">        private long mCount;  </span><br><span class="line">        private double mSum;  </span><br><span class="line">  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    public static class AvgEvaluator implements UDAFEvaluator &#123;  </span><br><span class="line">        AvgState state;  </span><br><span class="line">  </span><br><span class="line">        public AvgEvaluator() &#123;  </span><br><span class="line">            super();  </span><br><span class="line">            state = new AvgState();  </span><br><span class="line">            init();  </span><br><span class="line">        &#125;  </span><br><span class="line">  </span><br><span class="line">        /** </span><br><span class="line">         * init函数类似于构造函数，用于UDAF的初始化 </span><br><span class="line">         */  </span><br><span class="line">        public void init() &#123;  </span><br><span class="line">            state.mSum = 0;  </span><br><span class="line">            state.mCount = 0;  </span><br><span class="line">        &#125;  </span><br><span class="line">  </span><br><span class="line">        /** </span><br><span class="line">         * iterate接收传入的参数，并进行内部的轮转。其返回类型为boolean * * @param o * @return </span><br><span class="line">         */  </span><br><span class="line">  </span><br><span class="line">        public boolean iterate(Double o) &#123;  </span><br><span class="line">            if (o != null) &#123;  </span><br><span class="line">                state.mSum += o;  </span><br><span class="line">                state.mCount++;  </span><br><span class="line">            &#125;  </span><br><span class="line">            return true;  </span><br><span class="line">        &#125;  </span><br><span class="line">  </span><br><span class="line">        /** </span><br><span class="line">         * terminatePartial无参数，其为iterate函数遍历结束后，返回轮转数据， * terminatePartial类似于hadoop的Combiner * * @return </span><br><span class="line">         */  </span><br><span class="line">  </span><br><span class="line">        public AvgState terminatePartial() &#123;  </span><br><span class="line">            // combiner  </span><br><span class="line">            return state.mCount == 0 ? null : state;  </span><br><span class="line">        &#125;  </span><br><span class="line">  </span><br><span class="line">        /** </span><br><span class="line">         * merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean * * @param o * @return </span><br><span class="line">         */  </span><br><span class="line">  </span><br><span class="line">        public boolean merge(AvgState avgState) &#123;  </span><br><span class="line">            if (avgState != null) &#123;  </span><br><span class="line">                state.mCount += avgState.mCount;  </span><br><span class="line">                state.mSum += avgState.mSum;  </span><br><span class="line">            &#125;  </span><br><span class="line">            return true;  </span><br><span class="line">        &#125;  </span><br><span class="line">  </span><br><span class="line">        /** </span><br><span class="line">         * terminate返回最终的聚集函数结果 * * @return </span><br><span class="line">         */  </span><br><span class="line">        public Double terminate() &#123;  </span><br><span class="line">            return state.mCount == 0 ? null : Double.valueOf(state.mSum / state.mCount);  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="三、UDTF编写例子"><a href="#三、UDTF编写例子" class="headerlink" title="三、UDTF编写例子"></a>三、UDTF编写例子</h2><h3 id="3-1-说明"><a href="#3-1-说明" class="headerlink" title="3.1 说明"></a>3.1 说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.引入import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span><br><span class="line">2.继承GenericUDTF类</span><br><span class="line">3.重写initialize（返回输出行信息：列个数，类型）, process, close三方法</span><br></pre></td></tr></table></figure><h3 id="3-2-实例"><a href="#3-2-实例" class="headerlink" title="3.2 实例"></a>3.2 实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">//create temporary function DaylistUDTF as &apos;com.ai.hive.udf.uc.DaylistUDTF&apos;;</span><br><span class="line">public class DaylistUDTF extends GenericUDTF&#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void close() throws HiveException &#123;</span><br><span class="line">        // TODO Auto-generated method stub</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public void process(Object[] args) throws HiveException &#123;</span><br><span class="line">        int argsLength = args.length;</span><br><span class="line">        int resultLength = argsLength+1;</span><br><span class="line">        String daylist = args[4].toString();</span><br><span class="line">        String dayId = args[11].toString();</span><br><span class="line">        String[] result = new String[resultLength];</span><br><span class="line">        for(int j = 0; j &lt; argsLength; j++)&#123;</span><br><span class="line">            result[j] = args[j].toString();</span><br><span class="line">        &#125;</span><br><span class="line">        try&#123;</span><br><span class="line">            for (int i = daylist.length() - 1; i &gt;= 0; i--) &#123;</span><br><span class="line">                char c = daylist.charAt(i);</span><br><span class="line">                if (c == 49) &#123;</span><br><span class="line">                    int diff = 0-(daylist.length() - i);</span><br><span class="line">                    SimpleDateFormat sdf=new SimpleDateFormat(&quot;yyyyMMdd&quot;);</span><br><span class="line">                    Date dt=sdf.parse(dayId);</span><br><span class="line">                    Calendar rightNow = Calendar.getInstance();</span><br><span class="line">                    rightNow.setTime(dt);</span><br><span class="line">                    rightNow.add(Calendar.DAY_OF_YEAR,diff);</span><br><span class="line">                    Date dt1=rightNow.getTime();</span><br><span class="line">                    String reStr = sdf.format(dt1);</span><br><span class="line">                    result[resultLength-1] = reStr;</span><br><span class="line">                    forward(result);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;catch (Exception e)&#123;</span><br><span class="line">            System.out.println(&quot;error:&quot;+e.toString());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public StructObjectInspector initialize(ObjectInspector[] args)</span><br><span class="line">            throws UDFArgumentException &#123;</span><br><span class="line">        ArrayList&lt;String&gt; fieldNames = new ArrayList&lt;String&gt;();</span><br><span class="line">        ArrayList&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;ObjectInspector&gt;();</span><br><span class="line">        for (int i = 1; i &lt;= 14; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            fieldNames.add(&quot;col&quot; + i);</span><br><span class="line">            fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line">        &#125;</span><br><span class="line">        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,fieldOIs);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="四、感谢大佬"><a href="#四、感谢大佬" class="headerlink" title="四、感谢大佬"></a>四、感谢大佬</h2><p>&emsp;<a href="https://www.cnblogs.com/mzzcy/p/7119423.html" target="_blank" rel="noopener">Hive 自定义函数 UDF UDAF UDTF</a><br>&emsp;<a href="http://www.cnblogs.com/ggjucheng/archive/2013/02/01/2888051.html" target="_blank" rel="noopener">hive udaf开发入门和运行过程详解</a><br>&emsp;<a href="https://blog.csdn.net/u012485099/article/details/80790908" target="_blank" rel="noopener">hive中udtf编写及使用</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;之前开发过udf，但是udf只能处理一对一的情况，也就是一个输入对应一个输出。而日常开发中却会遇到多种情况，普通的udf不能满足，这时候就需要引入udtf和udaf了。&lt;br&gt;
    
    </summary>
    
    
      <category term="hadoop" scheme="https://wangtomorrow.github.io/tags/hadoop/"/>
    
      <category term="hive" scheme="https://wangtomorrow.github.io/tags/hive/"/>
    
      <category term="udtf" scheme="https://wangtomorrow.github.io/tags/udtf/"/>
    
      <category term="udaf" scheme="https://wangtomorrow.github.io/tags/udaf/"/>
    
  </entry>
  
  <entry>
    <title>kafka动态配置topic</title>
    <link href="https://wangtomorrow.github.io/post/bf5e9970.html"/>
    <id>https://wangtomorrow.github.io/post/bf5e9970.html</id>
    <published>2019-01-11T03:01:38.000Z</published>
    <updated>2019-03-27T02:46:49.559Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;之前使用@org.springframework.kafka.annotation.KafkaListener这个注解的时候，是在yml文件中配置，然后使用@KafkaListener(topics = {“${kafka.topic.a2b.name}”})，这样去单独监听某一个topic，生产者也固定在代码里定义变量读取配置文件。昨天改了个需求，希望以后通过配置文件去动态配置生产者和消费者的topic（不知道个数和topic名字），而不需要改代码。<br><a id="more"></a></p><h2 id="一、踩坑"><a href="#一、踩坑" class="headerlink" title="一、踩坑"></a>一、踩坑</h2><p>&emsp;刚开始的时候，由于考虑不充分（没有考虑到topic个数未知），想到@KafkaListener注解中的topics本身就是个字符串数组，于是想通过传入变量的形式。产生了以下两种方法：  </p><h3 id="1-传入变量方法一"><a href="#1-传入变量方法一" class="headerlink" title="1.传入变量方法一"></a>1.传入变量方法一</h3><p>&emsp; 使用@Value注解提取配置文件中相关配置，@KafkaListener中传入变量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public static String[] topicArr;</span><br><span class="line">@Value(&quot;$&#123;kafka.bootstrap.servers&#125;&quot;)</span><br><span class="line">public void setTopicArr(String[] value)&#123;</span><br><span class="line">    String topicArr = value;</span><br><span class="line">&#125;</span><br><span class="line">@KafkaListener(topics= topicArr)</span><br></pre></td></tr></table></figure></p><p>emmmm。。。结果可想而知，不行。</p><h3 id="2-传入变量方法二"><a href="#2-传入变量方法二" class="headerlink" title="2.传入变量方法二"></a>2.传入变量方法二</h3><p>&emsp;还是传入变量，不过这次写了个动态配置的代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">注解里这么写</span><br><span class="line">@KafkaListener(topics = &quot;$&#123;topicName1&#125;&quot;,&quot;$&#123;topicName2&#125;&quot;,&quot;$&#123;topicName3&#125;&quot;)</span><br><span class="line">提前将yml文件里添加</span><br><span class="line">topics: topicName1,topicName2,topicName3</span><br><span class="line">然后加载进来</span><br><span class="line">@Value(&quot;$&#123;kafka.topics&#125;&quot;)</span><br><span class="line">public void setTopics(String value)&#123;</span><br><span class="line">    topics = value;</span><br><span class="line">&#125;</span><br><span class="line">动态配置代码：</span><br><span class="line">@Configuration</span><br><span class="line">public class KafkaTopicConfiguration implements InitializingBean &#123;</span><br><span class="line">    @Autowired</span><br><span class="line">    private KafkaConfig kafkaconfig;</span><br><span class="line">    @Override</span><br><span class="line">    public void afterPropertiesSet() throws Exception &#123;</span><br><span class="line">        String[] topicArr = kafkaconfig.split(&quot;,&quot;);</span><br><span class="line">        int i = 1;</span><br><span class="line">        for(String topic : topicArr)&#123;</span><br><span class="line">            String topicName = &quot;topicName&quot;+i;</span><br><span class="line">            System.setProperty(topicName, topic);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>相比方法一，可行。但是未知topic数量呢。GG。</p><h3 id="3-不用注解"><a href="#3-不用注解" class="headerlink" title="3.不用注解"></a>3.不用注解</h3><p>&emsp;百度找到几个老哥的动态获取并创建topic的方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://www.cnblogs.com/gaoyawei/p/7723974.html</span><br><span class="line">https://www.cnblogs.com/huxi2b/p/7040617.html</span><br><span class="line">https://blog.csdn.net/qq_27232757/article/details/78970830</span><br></pre></td></tr></table></figure></p><p>写了几版，各种各样的问题，还是我太菜。就想再看看有没有别的简单点的解决办法，没有了再回来搞这个。</p><h3 id="4-正则匹配topic"><a href="#4-正则匹配topic" class="headerlink" title="4.正则匹配topic"></a>4.正则匹配topic</h3><p>&emsp;这期间又找到一个使用正则匹配topic的。直接贴<a href="https://www.jianshu.com/p/4c422a6a6c7a" target="_blank" rel="noopener">链接</a>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">@KafkaListener(topicPattern = &quot;showcase.*&quot;)</span><br><span class="line">这里使用正则匹配topic，其中【*】之前得加上【.】才能匹配到。</span><br></pre></td></tr></table></figure></p><p>中间模仿写了一版使用正则匹配的，其实也可以糊弄实现需求，除了topic取名的时候一定得规范以外，还得考虑到如果不想用某个topic了又得想怎么去避免他。<br>这种方法不太严谨，继续踩坑吧。</p><h2 id="二、问题解决"><a href="#二、问题解决" class="headerlink" title="二、问题解决"></a>二、问题解决</h2><p>&emsp;用蹩脚的英语google了一下，嗯?好多老哥们也是用的以上差不多的方法。然而最后在某个老哥github的<a href="https://github.com/spring-projects/spring-kafka/issues/361" target="_blank" rel="noopener">issues</a>中看到了解决办法。老哥的需求跟我差不多，感谢大佬,贴上最终问题解决方案。  </p><h3 id="1-kafka消费者监听配置"><a href="#1-kafka消费者监听配置" class="headerlink" title="1.kafka消费者监听配置"></a>1.kafka消费者监听配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">还是注解的形式</span><br><span class="line">@KafkaListener(topics = &quot;#&#123;&apos;$&#123;kafka.listener_topics&#125;&apos;.split(&apos;,&apos;)&#125;&quot;)</span><br></pre></td></tr></table></figure><p>读取yml文件中kafka.listener_topics的参数，然后根据“,”去split,得到一个topics数组。<br>这么做就可以根据配置文件动态的去监听topic。</p><h3 id="2-yml配置文件"><a href="#2-yml配置文件" class="headerlink" title="2.yml配置文件"></a>2.yml配置文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">只列出topic相关部分（mqTypes是我用来判断使用哪个topic发送的）</span><br><span class="line">    kafka:</span><br><span class="line">      listener_topics: kafka-topic-a2b,kafka-topic-c2b</span><br><span class="line">      consume:</span><br><span class="line">        topic:</span><br><span class="line">          - name: kafka-topic-a2b</span><br><span class="line">            partitions: 12</span><br><span class="line">            replication_factor: 2</span><br><span class="line">          - name: kafka-topic-c2b</span><br><span class="line">            partitions: 12</span><br><span class="line">            replication_factor: 2</span><br><span class="line">      product:</span><br><span class="line">        topic:</span><br><span class="line">          - name: kafka-topic-b2a</span><br><span class="line">            partitions: 12</span><br><span class="line">            replication_factor: 2</span><br><span class="line">            mqTypes: type1</span><br><span class="line">          - name: kafka-topic-b2c</span><br><span class="line">            partitions: 12</span><br><span class="line">            replication_factor: 2</span><br><span class="line">            mqTypes: type1</span><br></pre></td></tr></table></figure><h3 id="3-yml参数解析"><a href="#3-yml参数解析" class="headerlink" title="3.yml参数解析"></a>3.yml参数解析</h3><p>这里我将kafka的topic相关加载到bean中处理。<br>创建KafkaConsumerBean和KafkaProducerBean分别用来存储yml中生产者和消费者的topic相关参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">//KafkaConsumerBean</span><br><span class="line">@Component</span><br><span class="line">@ConfigurationProperties(prefix = &quot;kafka.consume&quot;)</span><br><span class="line">public class KafkaConsumerBean &#123;</span><br><span class="line">    private List&lt;Map&lt;String,String&gt;&gt; topic;</span><br><span class="line">    public void setTopic(List&lt;Map&lt;String, String&gt;&gt; topic) &#123;</span><br><span class="line">        this.topic = topic;</span><br><span class="line">    &#125;</span><br><span class="line">    public List&lt;Map&lt;String, String&gt;&gt; getTopic() &#123;</span><br><span class="line">        return topic;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//KafkaProducerBean</span><br><span class="line">@Component</span><br><span class="line">@ConfigurationProperties(prefix = &quot;kafka.product&quot;)</span><br><span class="line">public class KafkaProducerBean &#123;</span><br><span class="line">    private List&lt;Map&lt;String,String&gt;&gt; topic;</span><br><span class="line">    public void setTopic(List&lt;Map&lt;String, String&gt;&gt; topic) &#123;</span><br><span class="line">        this.topic = topic;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private Map&lt;String,String&gt; mqType2NameMap = new HashMap&lt;String,String&gt;();</span><br><span class="line">    public List&lt;Map&lt;String, String&gt;&gt; getTopic() &#123;</span><br><span class="line">        return topic;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getTopic(String mqType)&#123;</span><br><span class="line">        String name = mqType2NameMap.get(mqType);</span><br><span class="line">        if(name != null)&#123;</span><br><span class="line">            return name;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            for(Map&lt;String,String&gt; topicProperty : topic)&#123;</span><br><span class="line">                if (topicProperty.get(&quot;mqTypes&quot;).indexOf(mqType) &gt;= 0)&#123;</span><br><span class="line">                    name = topicProperty.get(&quot;name&quot;);</span><br><span class="line">                    mqType2NameMap.put(mqType,name);</span><br><span class="line">                    return name;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="4-创建topic"><a href="#4-创建topic" class="headerlink" title="4.创建topic"></a>4.创建topic</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Map&lt;String,String&gt;&gt; producerTopicList = kafkaProducerBean.getTopic();</span><br><span class="line">for (Map&lt;String,String&gt; topicProperty : producerTopicList)&#123;</span><br><span class="line">    KafkaClient.createTopic(topicProperty.get(&quot;name&quot;),Integer.parseInt(topicProperty.get(&quot;partitions&quot;)),Integer.parseInt(topicProperty.get(&quot;replication_factor&quot;)));</span><br><span class="line">&#125;</span><br><span class="line">List&lt;Map&lt;String,String&gt;&gt; consumerTopicList = kafkaConsumerBean.getTopic();</span><br><span class="line">for (Map&lt;String,String&gt; topicProperty : consumerTopicList)&#123;</span><br><span class="line">    KafkaClient.createTopic(topicProperty.get(&quot;name&quot;),Integer.parseInt(topicProperty.get(&quot;partitions&quot;)),Integer.parseInt(topicProperty.get(&quot;replication_factor&quot;)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>&emsp;上面解决问题的方法关键在于<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@KafkaListener(topics = &quot;#&#123;&apos;$&#123;kafka.listener_topics&#125;&apos;.split(&apos;,&apos;)&#125;&quot;)</span><br></pre></td></tr></table></figure></p><p>@KafkaListener这个注解会去读取spring的yml配置文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kafka:</span><br><span class="line">      listener_topics: kafka-topic-a2b,kafka-topic-c2b</span><br></pre></td></tr></table></figure></p><p>这块listener_topics配置信息，然后通过’,’分割成topic数组，KafkaListener注解中的 topics 参数，本身就是个数组，如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">//</span><br><span class="line">// Source code recreated from a .class file by IntelliJ IDEA</span><br><span class="line">// (powered by Fernflower decompiler)</span><br><span class="line">//</span><br><span class="line"></span><br><span class="line">package org.springframework.kafka.annotation;</span><br><span class="line"></span><br><span class="line">import java.lang.annotation.Documented;</span><br><span class="line">import java.lang.annotation.ElementType;</span><br><span class="line">import java.lang.annotation.Repeatable;</span><br><span class="line">import java.lang.annotation.Retention;</span><br><span class="line">import java.lang.annotation.RetentionPolicy;</span><br><span class="line">import java.lang.annotation.Target;</span><br><span class="line">import org.springframework.messaging.handler.annotation.MessageMapping;</span><br><span class="line"></span><br><span class="line">@Target(&#123;ElementType.TYPE, ElementType.METHOD, ElementType.ANNOTATION_TYPE&#125;)</span><br><span class="line">@Retention(RetentionPolicy.RUNTIME)</span><br><span class="line">@MessageMapping</span><br><span class="line">@Documented</span><br><span class="line">@Repeatable(KafkaListeners.class)</span><br><span class="line">public @interface KafkaListener &#123;</span><br><span class="line">    String id() default &quot;&quot;;</span><br><span class="line"></span><br><span class="line">    String containerFactory() default &quot;&quot;;</span><br><span class="line"></span><br><span class="line">    String[] topics() default &#123;&#125;;</span><br><span class="line"></span><br><span class="line">    String topicPattern() default &quot;&quot;;</span><br><span class="line"></span><br><span class="line">    TopicPartition[] topicPartitions() default &#123;&#125;;</span><br><span class="line"></span><br><span class="line">    String group() default &quot;&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>&emsp;结合我之前的kafka文章，应该是可以拼出一套成型的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;之前使用@org.springframework.kafka.annotation.KafkaListener这个注解的时候，是在yml文件中配置，然后使用@KafkaListener(topics = {“${kafka.topic.a2b.name}”})，这样去单独监听某一个topic，生产者也固定在代码里定义变量读取配置文件。昨天改了个需求，希望以后通过配置文件去动态配置生产者和消费者的topic（不知道个数和topic名字），而不需要改代码。&lt;br&gt;
    
    </summary>
    
    
      <category term="Kafka" scheme="https://wangtomorrow.github.io/tags/Kafka/"/>
    
      <category term="KafkaListener" scheme="https://wangtomorrow.github.io/tags/KafkaListener/"/>
    
      <category term="yml" scheme="https://wangtomorrow.github.io/tags/yml/"/>
    
  </entry>
  
  <entry>
    <title>hadoop删库跑路？</title>
    <link href="https://wangtomorrow.github.io/post/956da96e.html"/>
    <id>https://wangtomorrow.github.io/post/956da96e.html</id>
    <published>2019-01-04T07:58:59.000Z</published>
    <updated>2019-01-04T08:47:28.302Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;emmmm…昨天hadoop删库了。当时很慌，想半天好像记起来hadoop有个类似于回收站的东西，找了一下果然有，记录下，下次删库不要急着跑路。<br><a id="more"></a></p><h2 id="一、“回收站”"><a href="#一、“回收站”" class="headerlink" title="一、“回收站”"></a>一、“回收站”</h2><p>hadoop有个类似于回收站的机制，通常我们删除hdfs文件时<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm -r olicity/tableName</span><br></pre></td></tr></table></figure></p><p>执行命令后，并非将文件直接删除，而是将文件移动到设置的”.Trash”目录下。  </p><h2 id="二、配置“回收站”"><a href="#二、配置“回收站”" class="headerlink" title="二、配置“回收站”"></a>二、配置“回收站”</h2><p>默认情况下，.Trash为关闭状态，如果需要恢复误删文件，需要进行配置core-site.xml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;100&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p><p>说明：fs.trash.interval代表删除的文件保留的时间，时间单位为分钟，默认为0代表不保存删除的文件。我们只需要设置该时间即可打开.Trash。</p><h2 id="三、使用“回收站”"><a href="#三、使用“回收站”" class="headerlink" title="三、使用“回收站”"></a>三、使用“回收站”</h2><p>配置完了，就试试吧。先删库。（一定要确保配置完了，要不然还是随便删点不重要的东西吧）<br>删除之后，会提示<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">19/01/04 16:24:10 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://路径&apos; to trash at: hdfs://路径/.Trash/Current/路径</span><br></pre></td></tr></table></figure></p><p>恢复文件时，只需要<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mv</span><br></pre></td></tr></table></figure></p><p>嗯，移动出来就可以了。</p><h2 id="四、彻底删除"><a href="#四、彻底删除" class="headerlink" title="四、彻底删除"></a>四、彻底删除</h2><p>如果说这东西，你真的确定不想要了&amp;&amp;还觉得他占空间，可以彻底删除。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm -r 路径/.Trash/路径</span><br></pre></td></tr></table></figure></p><p>不过轻易不要这么干，万一出事就真的得跑路了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;emmmm…昨天hadoop删库了。当时很慌，想半天好像记起来hadoop有个类似于回收站的东西，找了一下果然有，记录下，下次删库不要急着跑路。&lt;br&gt;
    
    </summary>
    
    
      <category term="hadoop" scheme="https://wangtomorrow.github.io/tags/hadoop/"/>
    
      <category term="hdfs" scheme="https://wangtomorrow.github.io/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>log4j与hadoop的简单结合</title>
    <link href="https://wangtomorrow.github.io/post/b3d37a88.html"/>
    <id>https://wangtomorrow.github.io/post/b3d37a88.html</id>
    <published>2018-12-25T09:11:47.000Z</published>
    <updated>2018-12-25T10:31:26.134Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;最近使用了一种数据存储的方法，就是使用log4j的logback将数据进行保存，然后将数据上传到hive表中，进行相关的数据分析操作。<br><a id="more"></a></p><h2 id="一、配置说明"><a href="#一、配置说明" class="headerlink" title="一、配置说明"></a>一、配置说明</h2><p>&emsp;不多比比，感谢大佬。<a href="http://www.cnblogs.com/warking/p/5710303.html" target="_blank" rel="noopener">logback的使用和logback.xml详解</a>。这篇博客写的比较详细，关于logbak的相关配置文件说明。</p><h2 id="二、提取需要的信息"><a href="#二、提取需要的信息" class="headerlink" title="二、提取需要的信息"></a>二、提取需要的信息</h2><p>先在业务逻辑层中提取关键信息。<br>这里我是简单定义一个字符串数组，将信息保存。如果有别的需求，可以自行更改提取方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public static String[] getLogMessage(String a,String b,String c,String d)</span><br><span class="line">    &#123;</span><br><span class="line">        return new String[]&#123;a,b,c,d&#125;;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><h2 id="三、编写单独的日志打印类"><a href="#三、编写单独的日志打印类" class="headerlink" title="三、编写单独的日志打印类"></a>三、编写单独的日志打印类</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public class TestBhLogger</span><br><span class="line">&#123;</span><br><span class="line">    private static final Logger log = LoggerFactory.getLogger(TestBhLogger.class);</span><br><span class="line">    public static void log(String[] array)</span><br><span class="line">    &#123;</span><br><span class="line">        if ((array == null) || (array.length == 0)) &#123;</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line">        StringBuilder sb = new StringBuilder();</span><br><span class="line">        for (String str : array) &#123;</span><br><span class="line">            sb.append(str).append(&quot;\t&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        if (sb.length() &gt;= 1)</span><br><span class="line">        &#123;</span><br><span class="line">            String l = sb.substring(0, sb.length() - 1);</span><br><span class="line">            System.out.println(l);</span><br><span class="line">            log.info(l);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="四、logback-spring-xml文件配置"><a href="#四、logback-spring-xml文件配置" class="headerlink" title="四、logback-spring.xml文件配置"></a>四、logback-spring.xml文件配置</h2><h3 id="1-将日志信息输出到控制台标准输出流"><a href="#1-将日志信息输出到控制台标准输出流" class="headerlink" title="1.将日志信息输出到控制台标准输出流"></a>1.将日志信息输出到控制台标准输出流</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;</span><br><span class="line">    &lt;encoder&gt;</span><br><span class="line">        &lt;pattern&gt;%d&#123;MM/dd HH:mm:ss.SSS&#125; %-5level %logger&#123;5&#125; [%thread] - %msg%n&lt;/pattern&gt;</span><br><span class="line">    &lt;/encoder&gt;</span><br><span class="line">&lt;/appender&gt;</span><br></pre></td></tr></table></figure><h3 id="2-配置编写打印日志类的日志回滚策略"><a href="#2-配置编写打印日志类的日志回滚策略" class="headerlink" title="2.配置编写打印日志类的日志回滚策略"></a>2.配置编写打印日志类的日志回滚策略</h3><p>1)指定要打印日志的类及日志级别<br>2)将日志输出到定义目录的日志文件中<br>3)定义日志回滚策略及日志文件格式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;appender name=&quot;test-log&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;</span><br><span class="line">    &lt;File&gt;目录/logs/test/event.txt&lt;/File&gt;</span><br><span class="line">    &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;</span><br><span class="line">        &lt;fileNamePattern&gt;目录/logs/test/event-%d&#123;yyyyMMddHH&#125;.txt&lt;/fileNamePattern&gt;</span><br><span class="line">    &lt;/rollingPolicy&gt;</span><br><span class="line">    &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt;</span><br><span class="line">        &lt;Pattern&gt;%msg%n&lt;/Pattern&gt;</span><br><span class="line">    &lt;/layout&gt;</span><br><span class="line">    &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;</span><br><span class="line">        &lt;level&gt;INFO&lt;/level&gt;</span><br><span class="line">        &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;</span><br><span class="line">        &lt;onMismatch&gt;DENY&lt;/onMismatch&gt;</span><br><span class="line">    &lt;/filter&gt;</span><br><span class="line">&lt;/appender&gt;</span><br><span class="line">&lt;logger name=&quot;com.test.log.TestBhLogger&quot; level=&quot;INFO&quot; &gt;</span><br><span class="line">    &lt;appender-ref ref=&quot;test-log&quot;/&gt;</span><br><span class="line">&lt;/logger&gt;</span><br></pre></td></tr></table></figure></p><h3 id="3-完整的配置文件"><a href="#3-完整的配置文件" class="headerlink" title="3.完整的配置文件"></a>3.完整的配置文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt;</span><br><span class="line">&lt;configuration scan=&quot;false&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;false&quot;&gt;</span><br><span class="line">    &lt;timestamp key=&quot;day&quot; datePattern=&quot;yyyyMMdd&quot;/&gt;</span><br><span class="line">    &lt;timestamp key=&quot;hour&quot; datePattern=&quot;yyyyMMddHH&quot;/&gt;</span><br><span class="line"></span><br><span class="line">    &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;</span><br><span class="line">        &lt;encoder&gt;</span><br><span class="line">            &lt;pattern&gt;%d&#123;MM/dd HH:mm:ss.SSS&#125; %-5level %logger&#123;5&#125; [%thread] - %msg%n&lt;/pattern&gt;</span><br><span class="line">        &lt;/encoder&gt;</span><br><span class="line">    &lt;/appender&gt;</span><br><span class="line"></span><br><span class="line">    &lt;appender name=&quot;operatorLog&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;</span><br><span class="line">        &lt;File&gt;目录/logs/operator.log&lt;/File&gt;</span><br><span class="line">        &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;</span><br><span class="line">            &lt;fileNamePattern&gt;目录/logs/operator-%d&#123;yyyyMMddHH&#125;.log&lt;/fileNamePattern&gt;</span><br><span class="line">        &lt;/rollingPolicy&gt;</span><br><span class="line">        &lt;encoder&gt;</span><br><span class="line">            &lt;pattern&gt;%d&#123;MM/dd HH:mm:ss.SSS&#125; %-5level %logger&#123;5&#125; [%thread] - %msg%n&lt;/pattern&gt;</span><br><span class="line">        &lt;/encoder&gt;</span><br><span class="line">        &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;</span><br><span class="line">            &lt;level&gt;INFO&lt;/level&gt;</span><br><span class="line">            &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;</span><br><span class="line">            &lt;onMismatch&gt;DENY&lt;/onMismatch&gt;</span><br><span class="line">        &lt;/filter&gt;</span><br><span class="line">    &lt;/appender&gt;</span><br><span class="line"></span><br><span class="line">&lt;appender name=&quot;test-log&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;</span><br><span class="line">&lt;File&gt;目录/logs/test/event.txt&lt;/File&gt;</span><br><span class="line">&lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;</span><br><span class="line">&lt;fileNamePattern&gt;目录/logs/test/event-%d&#123;yyyyMMddHH&#125;.txt&lt;/fileNamePattern&gt;</span><br><span class="line">&lt;/rollingPolicy&gt;</span><br><span class="line">&lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt;</span><br><span class="line">&lt;Pattern&gt;%msg%n&lt;/Pattern&gt;</span><br><span class="line">&lt;/layout&gt;</span><br><span class="line">&lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;</span><br><span class="line">&lt;level&gt;INFO&lt;/level&gt;</span><br><span class="line">&lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;</span><br><span class="line">&lt;onMismatch&gt;DENY&lt;/onMismatch&gt;</span><br><span class="line">&lt;/filter&gt;</span><br><span class="line">&lt;/appender&gt;</span><br><span class="line">&lt;logger name=&quot;com.test.log.TestBhLogger&quot; level=&quot;INFO&quot; &gt;</span><br><span class="line">&lt;appender-ref ref=&quot;test-log&quot;/&gt;</span><br><span class="line">&lt;/logger&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;root level=&quot;info&quot;&gt;</span><br><span class="line">        &lt;appender-ref ref=&quot;STDOUT&quot;/&gt;</span><br><span class="line">        &lt;appender-ref ref=&quot;operatorLog&quot;/&gt;</span><br><span class="line">    &lt;/root&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h2 id="五、编写激活程序"><a href="#五、编写激活程序" class="headerlink" title="五、编写激活程序"></a>五、编写激活程序</h2><p>由于日志回滚需要打印日志去激活，故编写一个根据需要日志回滚的时间间隔的定时激活程序。<br>这里我直接采用了Sping自带的定时任务注解@EnableScheduling<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">@Configuration</span><br><span class="line">@EnableScheduling</span><br><span class="line">public class AliveTask</span><br><span class="line">&#123;</span><br><span class="line">    private static final Logger log = LoggerFactory.getLogger(AliveTask.class);</span><br><span class="line">    SimpleDateFormat dataFormat_yyyyMMddHH = new SimpleDateFormat(&quot;yyyyMMddHH&quot;);</span><br><span class="line"></span><br><span class="line">    @Scheduled(cron=&quot;0 10 0-23 * * ?&quot;)</span><br><span class="line">    public void scheduler()</span><br><span class="line">    &#123;</span><br><span class="line">        //大概就是这么个意思，具体代码根据不同需求与逻辑更改</span><br><span class="line">        String[] arr = getLogMessage(a,b,c,d);</span><br><span class="line">TestBhLogger.log(arr);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="六、加载日志文件到hive表"><a href="#六、加载日志文件到hive表" class="headerlink" title="六、加载日志文件到hive表"></a>六、加载日志文件到hive表</h2><p>根据不同需求获取需要的时间格式，加载到相应的hive表的相应分区中<br>HiveUtil工具类网上一堆，就不细写了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public void loadData()&#123;</span><br><span class="line">    try</span><br><span class="line">    &#123;</span><br><span class="line">        Calendar cal = Calendar.getInstance();</span><br><span class="line">        cal.add(11, -1);</span><br><span class="line">        String hourId = this.dataFormat_yyyyMMddHH.format(cal.getTime());</span><br><span class="line">        String dayId = hourId.substring(0, 8);</span><br><span class="line">        String hivesql = &quot;LOAD DATA LOCAL INPATH &apos;&quot; + Main.BASE_PATH + &quot;/logs/test/*&quot; + hourId + &quot;*&apos; INTO TABLE &quot; + Config.getString(&quot;hive.database&quot;) + &quot;.test_bh PARTITION(day_id=&apos;&quot; + dayId + &quot;&apos;,hour_id=&apos;&quot; + hourId + &quot;&apos;)&quot;;</span><br><span class="line">        HiveUtil.exec(hivesql);</span><br><span class="line">    &#125;</span><br><span class="line">    catch (Exception e)</span><br><span class="line">    &#123;</span><br><span class="line">        log.error(&quot;上传数据失败&quot; + e.getMessage(), e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><hr><p>&emsp;至此就将需要的数据存储到hive表中了，接下来就是根据需求进行数据分析了。log4j真的强大。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;最近使用了一种数据存储的方法，就是使用log4j的logback将数据进行保存，然后将数据上传到hive表中，进行相关的数据分析操作。&lt;br&gt;
    
    </summary>
    
    
      <category term="hadoop" scheme="https://wangtomorrow.github.io/tags/hadoop/"/>
    
      <category term="hive" scheme="https://wangtomorrow.github.io/tags/hive/"/>
    
      <category term="log4j" scheme="https://wangtomorrow.github.io/tags/log4j/"/>
    
      <category term="spring" scheme="https://wangtomorrow.github.io/tags/spring/"/>
    
  </entry>
  
  <entry>
    <title>crontab使用时间参数</title>
    <link href="https://wangtomorrow.github.io/post/78cb4de.html"/>
    <id>https://wangtomorrow.github.io/post/78cb4de.html</id>
    <published>2018-12-25T08:06:14.000Z</published>
    <updated>2018-12-25T08:53:33.596Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;最近写了一个脚本，需要定时执行，决定使用Crontab。<br><a id="more"></a></p><h2 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h2><p>&emsp;由于脚本需要传入时间参数，传入时间时发生了不能执行的问题，如下。<br>刚开始写的调用脚本为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">20 0 * * * source ~/.bash_profile;cd */shell;sh count.sh $(date +%Y%m%d)&gt; count.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure></p><p>定时执行不能执行，查看系统日志后，发现错误位置在$(date。</p><h2 id="二、问题解决"><a href="#二、问题解决" class="headerlink" title="二、问题解决"></a>二、问题解决</h2><p>&emsp;看了网上发现问题所在。原因应该是，任务调度中，%是个特殊字符，表示特殊含义，有换行的意思。所以不能直接使用%，而需要添加反斜杠来进行转义。<br>修改后的调用脚本为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">20 0 * * * source ~/.bash_profile;cd */shell;sh count.sh $(date +&quot;\%Y\%m\%d&quot;)&gt; count.log 2&gt;&amp;1^C</span><br></pre></td></tr></table></figure></p><p>然后问题解决了。</p><h2 id="三、拓展"><a href="#三、拓展" class="headerlink" title="三、拓展"></a>三、拓展</h2><p>&emsp;既然用到了crontab，就简单的学习一下吧。</p><h3 id="1-时间参数说明"><a href="#1-时间参数说明" class="headerlink" title="1.时间参数说明"></a>1.时间参数说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">20 0 * * *</span><br><span class="line">从左到右依次为：</span><br><span class="line">[分钟] [小时] [每月的某一天] [每年的某一月] [每周的某一天] [执行的命令]</span><br><span class="line">该参数的意义为：每天的0点20分执行脚本</span><br><span class="line">星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作</span><br><span class="line">逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”</span><br><span class="line">中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”</span><br><span class="line">正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10</span><br><span class="line">@reboot 系统重启时执行</span><br></pre></td></tr></table></figure><h3 id="2-添加-编辑-Crontab"><a href="#2-添加-编辑-Crontab" class="headerlink" title="2.添加/编辑 Crontab"></a>2.添加/编辑 Crontab</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">crontab [-u username] -e</span><br><span class="line">默认情况下，系统会编辑当前用户的crontab命令集合</span><br></pre></td></tr></table></figure><h3 id="3-查看Crontab"><a href="#3-查看Crontab" class="headerlink" title="3.查看Crontab"></a>3.查看Crontab</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crontab [-u username] -l</span><br></pre></td></tr></table></figure><h3 id="4-删除Crontab"><a href="#4-删除Crontab" class="headerlink" title="4.删除Crontab"></a>4.删除Crontab</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">crontab [-u username] -r</span><br><span class="line">慎用。可以直接crontab -e 进行编辑</span><br></pre></td></tr></table></figure><h3 id="5-载入"><a href="#5-载入" class="headerlink" title="5.载入"></a>5.载入</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">crontab [-u user] file</span><br><span class="line">将file做为crontab的任务列表文件并载入crontab</span><br><span class="line">如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。</span><br></pre></td></tr></table></figure><h3 id="6-Crontab服务"><a href="#6-Crontab服务" class="headerlink" title="6.Crontab服务"></a>6.Crontab服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">service crond start    //启动服务</span><br><span class="line">service crond stop     //关闭服务</span><br><span class="line">service crond restart  //重启服务</span><br><span class="line">service crond reload   //重新载入配置</span><br><span class="line">service crond status   //查看服务状态</span><br></pre></td></tr></table></figure><h3 id="7-目录"><a href="#7-目录" class="headerlink" title="7.目录"></a>7.目录</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/etc/cron.d/</span><br><span class="line">这个目录用来存放任何要执行的crontab文件或脚本。</span><br></pre></td></tr></table></figure><h3 id="8-注意事项"><a href="#8-注意事项" class="headerlink" title="8.注意事项"></a>8.注意事项</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1)脚本中涉及文件路径时写全局路径</span><br><span class="line">2)脚本执行要用到java或其他环境变量时，通过source命令引入环境变量</span><br><span class="line">3)新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;最近写了一个脚本，需要定时执行，决定使用Crontab。&lt;br&gt;
    
    </summary>
    
    
      <category term="linux" scheme="https://wangtomorrow.github.io/tags/linux/"/>
    
      <category term="crontab" scheme="https://wangtomorrow.github.io/tags/crontab/"/>
    
  </entry>
  
  <entry>
    <title>hive优化之count distinct</title>
    <link href="https://wangtomorrow.github.io/post/e146891d.html"/>
    <id>https://wangtomorrow.github.io/post/e146891d.html</id>
    <published>2018-11-06T02:53:47.000Z</published>
    <updated>2018-11-06T03:09:30.115Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;日常使用hive sql进行数据的分析，当需要统计某个字段去重后的数量时，会用到如下语句：<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT COUNT( DISTINCT id ) FROM TABLE_NAME WHERE ...;</span><br></pre></td></tr></table></figure></p><p>&emsp;这条语句的mapreduce执行图如下：<br><img src="https://github.com/Wangtomorrow/Resource/blob/master/hive_count_job1.jpg?raw=true" alt="mapreduce执行图"></p><p>&emsp;由于引入了DISTINCT，因此在Map阶段无法利用combine对输出结果消重，必须将id作为Key输出，在Reduce阶段再对来自于不同Map Task、相同Key的结果进行消重，计入最终统计值。<br>&emsp;我们看到作业运行时的Reduce Task个数为1，对于统计大数据量时，这会导致最终Map的全部输出由单个的ReduceTask处理。这唯一的Reduce Task需要Shuffle大量的数据，并且进行排序聚合等处理，这使得它成为整个作业的IO和运算瓶颈。<br>&emsp;经过上述分析后，我们尝试显式地增大Reduce Task个数来提高Reduce阶段的并发，使每一个Reduce Task的数据处理量控制在2G左右。具体设置如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapred.reduce.tasks=100</span><br></pre></td></tr></table></figure></p><p>&emsp;调整后我们发现这一参数并没有影响实际Reduce Task个数，Hive运行时输出“Number of reduce tasks determined at compile time: 1”。原来Hive在处理COUNT这种“全聚合(full aggregates)”计算时，它会忽略用户指定的Reduce Task数，而强制使用1。我们只能采用变通的方法来绕过这一限制。我们利用Hive对嵌套语句的支持，将原来一个MapReduce作业转换为两个作业，在第一阶段选出全部的非重复id，在第二阶段再对这些已消重的id进行计数。这样在第一阶段我们可以通过增大Reduce的并发数，并发处理Map输出。在第二阶段，由于id已经消重，因此COUNT(*)操作在Map阶段不需要输出原id数据，只输出一个合并后的计数即可。这样即使第二阶段Hive强制指定一个Reduce Task，极少量的Map输出数据也不会使单一的Reduce Task成为瓶颈。改进后的SQL语句如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT COUNT(*) FROM (SELECT DISTINCT id FROM TABLE_NAME WHERE … ) t;</span><br></pre></td></tr></table></figure></p><p>&emsp;在实际运行时，我们发现Hive还对这两阶段的作业做了额外的优化。它将第二个MapReduce作业Map中的Count过程移到了第一个作业的Reduce阶段。这样在第一阶Reduce就可以输出计数值，而不是消重的全部id。这一优化大幅地减少了第一个作业的Reduce输出IO以及第二个作业Map的输入数据量。最终在同样的运行环境下优化后的语句执行只需要原语句20%左右的时间。优化后的MapReduce作业流如下：<br><img src="https://github.com/Wangtomorrow/Resource/blob/master/hive_count_job2.jpg?raw=true" alt="mapreduce执行图"><br>从上述优化过程我们可以看出，一个简单的统计需求，如果不理解Hive和MapReduce的工作原理，它可能会比优化后的执行过程多四、五倍的时间。我们在利用Hive简化开发的同时，也要尽可能优化SQL语句，提升计算作业的执行效率。</p><p>此文转自<a href="http://bigdata-blog.net/2013/11/08/hive-sql%E4%BC%98%E5%8C%96%E4%B9%8B-count-distinct/" target="_blank" rel="noopener">Bigdata Blog</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;日常使用hive sql进行数据的分析，当需要统计某个字段去重后的数量时，会用到如下语句：&lt;br&gt;
    
    </summary>
    
    
      <category term="hive" scheme="https://wangtomorrow.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>hive开发UDF及使用</title>
    <link href="https://wangtomorrow.github.io/post/96910ad9.html"/>
    <id>https://wangtomorrow.github.io/post/96910ad9.html</id>
    <published>2018-10-26T02:35:40.000Z</published>
    <updated>2018-10-26T03:47:18.446Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;最近有个数据挖掘的需求，要求统计所给经纬度附近n公里某些事物的数量。涉及到地球两点间的距离计算，需要写UDF进行计算。<br><a id="more"></a></p><h2 id="一、UDF编写"><a href="#一、UDF编写" class="headerlink" title="一、UDF编写"></a>一、UDF编写</h2><p>&emsp;根据经纬度计算两点间的距离，网上有很多计算方法，试了几个，发现这篇<a href="https://blog.csdn.net/u011001084/article/details/52980834" target="_blank" rel="noopener">博客</a>的方法计算的精度差比较小，他的分析方法也很详细，最终采用此方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">import com.ai.hive.udf.topdomain.StringUtil;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.log4j.Logger;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 功能：根据两地经纬度计算两点之间的距离</span><br><span class="line"> * create temporary function LocDistanceCalUDF as &apos;com.ai.hive.udf.util.LocDistanceCalUDF&apos;;</span><br><span class="line"> * @author olicity</span><br><span class="line"> */</span><br><span class="line">public class LocDistanceCalUDF extends UDF&#123;</span><br><span class="line">    private static Logger log = Logger.getLogger(LocDistanceCalUDF.class);</span><br><span class="line"></span><br><span class="line">    private Text nullText = new Text(&quot;&quot;);</span><br><span class="line">    /**</span><br><span class="line">    *根据经纬度计算地球两点间的距离</span><br><span class="line">    */</span><br><span class="line">    private static double distanceCal(double lng1, double lat1,double lng2,double lat2)&#123;</span><br><span class="line">        double dx = lng1 - lng2;// 经度差值</span><br><span class="line">        double dy= lat1 - lat2;// 纬度差值</span><br><span class="line">        double b = (lat1 + lat2) / 2.0;// 平均纬度</span><br><span class="line">        double Lx = Math.toRadians(dx)*6367000.0*Math.cos(Math.toRadians(b));// 东西距离</span><br><span class="line">        double Ly = 6367000.0*Math.toRadians(dy);// 南北距离</span><br><span class="line">        return Math.sqrt(Lx*Lx+Ly*Ly);// 用平面的矩形对角距离公式计算总距离(米)</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">    *重写evaluate方法</span><br><span class="line">    */</span><br><span class="line">    public Text evaluate(Text longitudeText1, Text latitudeText1,Text longitudeText2, Text latitudeText2)&#123;</span><br><span class="line">        try&#123;</span><br><span class="line">            if(longitudeText1==null || latitudeText1==null || longitudeText2==null || latitudeText2==null)&#123;</span><br><span class="line">                return nullText;</span><br><span class="line">            &#125;</span><br><span class="line">            if(StringUtil.isEmpty(longitudeText1.toString()) || StringUtil.isEmpty(latitudeText1.toString()) || StringUtil.isEmpty(longitudeText2.toString()) || StringUtil.isEmpty(latitudeText2.toString()))&#123;</span><br><span class="line">                return nullText;</span><br><span class="line">            &#125;</span><br><span class="line">            double lng1 = Double.valueOf(longitudeText1.toString());</span><br><span class="line">            double lat1 = Double.valueOf(latitudeText1.toString());</span><br><span class="line">            double lng2 = Double.valueOf(longitudeText2.toString());</span><br><span class="line">            double lat2 = Double.valueOf(latitudeText2.toString());</span><br><span class="line"></span><br><span class="line">            double dis = distanceCal(lng1,lat1,lng2,lat2);</span><br><span class="line">            return new Text(String.valueOf(dis));</span><br><span class="line">        &#125;catch (Exception e)&#123;</span><br><span class="line">            return nullText;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">    *重写evaluate方法</span><br><span class="line">    */</span><br><span class="line">    public Text evaluate(Text locationA,Text locationB)&#123;</span><br><span class="line">        try&#123;</span><br><span class="line">            if (locationA==null||locationB==null)&#123;</span><br><span class="line">                return nullText;</span><br><span class="line">            &#125;</span><br><span class="line">            if(StringUtil.isEmpty(locationA.toString()) || StringUtil.isEmpty(locationB.toString()))&#123;</span><br><span class="line">                return nullText;</span><br><span class="line">            &#125;</span><br><span class="line">            String locationA2String  = locationA.toString();</span><br><span class="line">            String locationB2String  = locationB.toString();</span><br><span class="line">            double lng1 = Double.valueOf(locationA2String.split(&quot;,&quot;)[0]);</span><br><span class="line">            double lat1 = Double.valueOf(locationA2String.split(&quot;,&quot;)[1]);</span><br><span class="line">            double lng2 = Double.valueOf(locationB2String.split(&quot;,&quot;)[0]);</span><br><span class="line">            double lat2 = Double.valueOf(locationB2String.split(&quot;,&quot;)[1]);</span><br><span class="line"></span><br><span class="line">            double dis = distanceCal(lng1,lat1,lng2,lat2);</span><br><span class="line">            return new Text(String.valueOf(dis));</span><br><span class="line">        &#125;catch(Exception e)&#123;</span><br><span class="line">            return nullText;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>&emsp;UDF类要继承org.apache.hadoop.hive.ql.exec.UDF类，类中要实现evaluate。 当我们在hive中使用自定义的UDF的时候，hive会调用类中的evaluate方法来实现特定的功能。  </p><h2 id="二、UDF导入"><a href="#二、UDF导入" class="headerlink" title="二、UDF导入"></a>二、UDF导入</h2><h3 id="1-jar包上传"><a href="#1-jar包上传" class="headerlink" title="1.jar包上传"></a>1.jar包上传</h3><p>&emsp;右键类名，Copy reference，复制此类全路径得到：com.ai.hive.udf.util.LocDistanceCalUDF。将写完的类打成jar包上传到服务器。路径如：/user/olicity/hive/UDF</p><h3 id="2-jar包引入classpath变量中"><a href="#2-jar包引入classpath变量中" class="headerlink" title="2.jar包引入classpath变量中"></a>2.jar包引入classpath变量中</h3><p>进入hive，引入jar包，执行命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar /user/olicity/hive/UDF/udf.jar;</span><br></pre></td></tr></table></figure></p><p>查看导入的jar包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list jars;</span><br></pre></td></tr></table></figure></p><h3 id="3-创建函数"><a href="#3-创建函数" class="headerlink" title="3.创建函数"></a>3.创建函数</h3><p>创建一个名为LocDistanceCalUDF的临时函数，关联该jar包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create temporary function LocDistanceCalUDF as &apos;com.ai.hive.udf.util.LocDistanceCalUDF&apos;;</span><br></pre></td></tr></table></figure></p><p>查看创建的函数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show functions;</span><br></pre></td></tr></table></figure></p><h3 id="4-注意"><a href="#4-注意" class="headerlink" title="4.注意"></a>4.注意</h3><p>&emsp;上述方法仅限于当前会话生效，如需要添加一个永久的函数对应的永久的路径，则<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create function locUDF.LocDistanceCalUDF </span><br><span class="line">  as &apos;com.ai.hive.udf.util.LocDistanceCalUDF&apos; </span><br><span class="line">  using jar &apos;hdfs://hdfs路径/udf.jar&apos;;</span><br><span class="line">use LocDistanceCalUDF;</span><br></pre></td></tr></table></figure></p><p>需要将jar包放到hdfs上，然后创建函数关联路径即可。<br>另外还看到过另一种方法，配置hive-site.xml文件中的hive.aux.jars.path<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">配置参考如下：</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;hive.aux.jars.path&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;file:///home/hdfs/fangjs/DefTextInputFormat.jar,file:///jarpath/test.jar&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br></pre></td></tr></table></figure></p><h2 id="三、UDF使用"><a href="#三、UDF使用" class="headerlink" title="三、UDF使用"></a>三、UDF使用</h2><p>&emsp;准备工作已经就绪，可以开始查表了。emmmmm，就简单的俩表查吧，表结构和表数据就不展示了，示例表也就不建了，所给经纬度表叫A表，需要查询的表叫B表，临时中间表叫c表，经纬度的表中字段定义是loc，距离就算2公里吧。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create table C</span><br><span class="line">as</span><br><span class="line">select B.* from B join A where (LocDistanceCalUDF(A.loc,B.loc)&lt;=2000);</span><br></pre></td></tr></table></figure></p><p>OK.</p><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><p>&emsp;关于sql语句还是需要再多加练习，尤其是多表联查。Hadoop之路任重而道远。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;最近有个数据挖掘的需求，要求统计所给经纬度附近n公里某些事物的数量。涉及到地球两点间的距离计算，需要写UDF进行计算。&lt;br&gt;
    
    </summary>
    
    
      <category term="hadoop" scheme="https://wangtomorrow.github.io/tags/hadoop/"/>
    
      <category term="hive" scheme="https://wangtomorrow.github.io/tags/hive/"/>
    
      <category term="UDF" scheme="https://wangtomorrow.github.io/tags/UDF/"/>
    
  </entry>
  
  <entry>
    <title>URLEncode和URLDecoder</title>
    <link href="https://wangtomorrow.github.io/post/686dbd19.html"/>
    <id>https://wangtomorrow.github.io/post/686dbd19.html</id>
    <published>2018-10-17T08:35:19.000Z</published>
    <updated>2018-10-17T10:02:12.221Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>&emsp;使用http请求的在服务之间传递消息时，会出现字符串乱码现象。<br>&emsp;使用POST方法提交时，会对其中的有些字符进行编码,数据内容的类型是 application/x-www-form-urlencoded<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.字符&quot;a&quot;-&quot;z&quot;，&quot;A&quot;-&quot;Z&quot;，&quot;0&quot;-&quot;9&quot;，&quot;.&quot;，&quot;-&quot;，&quot;*&quot;，和&quot;_&quot; 都不会被编码;</span><br><span class="line">2.将空格转换为加号 (+) ;</span><br><span class="line">3.将非文本内容转换成&quot;%xy&quot;的形式,xy是两位16进制的数值;</span><br><span class="line">4.在每个 name=value 对之间放置 &amp; 符号。</span><br></pre></td></tr></table></figure></p><p>&emsp;URLDecoder 和 URLEncoder 用于完成普通字符串 和 application/x-www-form-urlencoded MIME 字符串之间的相互转换。</p><h2 id="二、使用"><a href="#二、使用" class="headerlink" title="二、使用"></a>二、使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">正常的字符串：破晓</span><br><span class="line">URL中的字符串：%e7%a0%b4%e6%99%93</span><br></pre></td></tr></table></figure><h3 id="1-URLEncode"><a href="#1-URLEncode" class="headerlink" title="1.URLEncode"></a>1.URLEncode</h3><p>&emsp;URLEncode是对URL中的特殊字符部分进行编码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">String url = &quot;破晓&quot;;</span><br><span class="line">try&#123;</span><br><span class="line">    url = URLEncoder.encode(url,&quot;utf-8&quot;);</span><br><span class="line">    System.out.println(&quot;-----&quot;+url);</span><br><span class="line">&#125;catch (Exception e)&#123;</span><br><span class="line">    System.out.println(&quot;---&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="2-URLDecode"><a href="#2-URLDecode" class="headerlink" title="2.URLDecode"></a>2.URLDecode</h3><p>&emsp;URLEncode是对URL中的特殊字符部分进行解码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">String url = &quot;%e7%a0%b4%e6%99%93&quot;;</span><br><span class="line">try&#123;</span><br><span class="line">    url = URLDecoder.decode(url);</span><br><span class="line">    System.out.println(&quot;-----&quot;+url);</span><br><span class="line">&#125;catch (Exception e)&#123;</span><br><span class="line">    System.out.println(&quot;---&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="三、应用"><a href="#三、应用" class="headerlink" title="三、应用"></a>三、应用</h2><p>&emsp;上次发现个问题，经过我们使用BASE64加密，通过http请求后，发现原来的“+”全部变成了“ ”。最后发现问题出在这里。<br>&emsp;上述方法即可解决http请求后出现乱码现象。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、背景&quot;&gt;&lt;a href=&quot;#一、背景&quot; class=&quot;headerlink&quot; title=&quot;一、背景&quot;&gt;&lt;/a&gt;一、背景&lt;/h2&gt;&lt;p&gt;&amp;emsp;使用http请求的在服务之间传递消息时，会出现字符串乱码现象。&lt;br&gt;&amp;emsp;使用POST方法提交时，会对其中的有些字符进行编码,数据内容的类型是 application/x-www-form-urlencoded&lt;br&gt;
    
    </summary>
    
    
      <category term="Java" scheme="https://wangtomorrow.github.io/tags/Java/"/>
    
      <category term="Http" scheme="https://wangtomorrow.github.io/tags/Http/"/>
    
  </entry>
  
  <entry>
    <title>关于hadoop3.0的使用</title>
    <link href="https://wangtomorrow.github.io/post/f9a72d3.html"/>
    <id>https://wangtomorrow.github.io/post/f9a72d3.html</id>
    <published>2018-10-10T11:00:07.000Z</published>
    <updated>2018-10-11T06:18:59.799Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;总算是有点时间了，捣鼓一下hadoop3.0的一些东西，听说3.0比spark快十倍？<br><a id="more"></a></p><h2 id="一、安装配置"><a href="#一、安装配置" class="headerlink" title="一、安装配置"></a>一、安装配置</h2><p>&emsp;前面的环境配置与解压安装大体一致。配置文件的异同如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">1.集群节点配置文件</span><br><span class="line">    3.0以前都是通过 安装目录/etc/hadoop/slaves 进行配置，3.0则是在同一目录下的workers配置，改个名?具体的配置方式与之前一样，每行一个节点名。</span><br><span class="line">2.hadoop-env.sh</span><br><span class="line">    之前只需配置此文件下的JAVA_HOME,现在除了必要的JAVA_HOME外还需修改如下：</span><br><span class="line">    export JAVA_HOME=/usr/local/java/jdk1.8.0_65</span><br><span class="line">    export HADOOP_HOME=/usr/local/hadoop/hadoop-3.1.1</span><br><span class="line">    export HDFS_NAMENODE_USER=root</span><br><span class="line">    export HDFS_DATANODE_USER=root</span><br><span class="line">    export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">    export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">    export YARN_NODEMANAGER_USER=root</span><br><span class="line">    就是配置你的hadoop用户。我是配置的root，如果配置个别用户的话，自行更改。</span><br><span class="line">3.core-site.xml</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">     &lt;!-- 指定hdfs的namenode的通信地址 --&gt;</span><br><span class="line">      &lt;value&gt;hdfs://192.168.2.100:9000&lt;/value&gt;</span><br><span class="line">     &lt;/property&gt;</span><br><span class="line">     &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;!-- 以下为存放临时文件的路径 --&gt;</span><br><span class="line">      &lt;value&gt;/home/olicity/hadoop/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    与之前版本一致</span><br><span class="line">4.hdfs-site.xml</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">       &lt;name&gt;dfs.name.dir&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;/home/olicity/hadoop/dfs/name&lt;/value&gt;</span><br><span class="line">       &lt;description&gt;Path on the local filesystem where theNameNode stores the namespace and transactions logs persistently.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">       &lt;name&gt;dfs.data.dir&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;/home/olicity/hadoop/dfs/data&lt;/value&gt;</span><br><span class="line">       &lt;description&gt;Comma separated list of paths on the localfilesystem of a DataNode where it should store its blocks.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;centos0:50070&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;centos0:50090&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">       &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">          &lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">          &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">          &lt;description&gt;need not permissions&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    无差。</span><br><span class="line">5.yarn-site.xml</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;centos0&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line">     &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">    &lt;!-- cpu个数 需要根据当前机器cpu设置 --&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    关于cpu的配置，记得之前看过2.2版本就提出过一个初步的实现方式，之前一直没配置过，3.0试一下。</span><br><span class="line">6.mapred-site.xml</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapred.job.tracker&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;centos0:49001&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">          &lt;name&gt;mapred.local.dir&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;/home/olicity/hadoop/var&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">           &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    大同小异。其实只要配置mapreduce.framework.name也行。</span><br></pre></td></tr></table></figure></p><p>&emsp;总体来看配置方面除了slave改名为worker和hadoop-env.xml配置用户外，别的地方都大同小异。<br>&emsp;接下来就是初始化同样的命令 hdfs namenode -format 。注意要是配置的root用户需要使用root用户进行初始化。同样，不报错返回“Exiting with status 0”即为成功。然后就是在namenode上start-all.sh或者dfs与yarn分开启动也行。最后在集群每台机器上分别jps查看进程启动情况。<br>&emsp;还有之前看过别人的配置，他在hadoop-env.xml下并没有配置用户，而是写在每个start-*.sh脚本中了。</p><h2 id="二、文档和大佬们总结的变化"><a href="#二、文档和大佬们总结的变化" class="headerlink" title="二、文档和大佬们总结的变化"></a>二、文档和大佬们总结的变化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">1.最低Java版本要求从Java7变为Java8  </span><br><span class="line">2.HDFS支持纠删码（erasure coding）</span><br><span class="line">    用来解决存储空间文件。EC技术既可以防止数据丢失，又能解决HDFS存储空间翻倍的问题。使得HDFS在不降低可靠性的前提下，节省一半存储空间。</span><br><span class="line">    另外，在使用这个新特性时，用户还需要考虑网络和CPU开销。</span><br><span class="line">3.YARN的时间线V.2服务</span><br><span class="line">    提高时间线服务的可伸缩性和可靠性，通过引入流和聚合来增强可用性</span><br><span class="line">    具体的一些原理和操作，还是看官方文档和大佬博客吧。</span><br><span class="line">4.优化Hadoop Shell脚本，除了修复一些bug，还加入一些新特性</span><br><span class="line">5.精简了内核，剔除了过期的API和实现，废弃hftp转由webhdfs替代。重构Hadoop Client Jar包，解决jar包冲突问题</span><br><span class="line">6.支持等待容器和分布式调度</span><br><span class="line">    在Hadoop3 中引入了一种新型执行类型，即等待容器，即使在调度时集群没有可用的资源，它也可以在NodeManager中被调度执行。在这种情况下，这些容器将在NM中排队等待资源启动，等待荣容器比默认容器优先级低，因此，如果需要，可以抢占默认容器的空间，这样可以提供机器的利用率。</span><br><span class="line">7.MapReduce任务级别本地化优化</span><br><span class="line">    MapReduce添加了映射输出收集器的本地化实现的支持。对于密集型的洗牌操作（shuffle-intensive）jobs，可以带来30%的性能提升。</span><br><span class="line">8.支持多个NameNode节点</span><br><span class="line">    namenode的高可用性</span><br><span class="line">9.修改了多重服务的默认端口</span><br><span class="line">    防止端口冲突</span><br><span class="line">10.提供文件系统连接器（filesystem connnector）,支持Microsoft Azure Data Lake和Aliyun对象存储系统</span><br><span class="line">11.数据节点内置平衡器</span><br><span class="line">    通过hdfs diskbalancer ClI来调用，解决单一DataNode管理多个磁盘情况下，添加或删除磁盘导致磁盘负载不均衡问题</span><br><span class="line">12.重写了守护进程和任务的堆管理机制</span><br><span class="line">    现在可以根据主机的内存大小进行自动调整，并且已经禁止HADOOP_HEAPSIZE变量</span><br><span class="line">    Map和Reduce的堆大小的配置被简化了，所以不再需要任务配置作为一个Java选项指定。已经指定的两个现有配置不受此更改的影响</span><br><span class="line">13.S3Gurad:为S3A文件系统客户端提供一致性和元数据缓存</span><br><span class="line">14.HDFS的基于路由器互联</span><br><span class="line">    HDFS Router-Based Federation添加了一个RPC路由层，为多个HDFS命名空间提供了一个联合视图。简化了现存HDFS客户端接入federated cluster的操作。</span><br><span class="line">15.基于API配置的Capacity Scheduler queue configuration</span><br><span class="line">16.YARN资源类型</span><br><span class="line">    Yarn资源模型已经被一般化，可以支持用户自定义的可计算资源类型，而不仅仅是CPU和内存。比如，集群管理员可以定义像GPU数量，软件序列号、本地连接的存储的资源。然后，Yarn任务能够在这些可用资源上进行调度。</span><br></pre></td></tr></table></figure><p>&emsp;差不多就这些，很多东西我还没细入研究过，主要看的是几处大的变动和效率的优化。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">以下是hadoop-3.0的最新参数</span><br><span class="line">Hadoop-3.0</span><br><span class="line">· HADOOP</span><br><span class="line">    · Move to JDK8+</span><br><span class="line">    · Classpath isolation on by default HADOOP-11656</span><br><span class="line">    · Shell script rewrite HADOOP-9902</span><br><span class="line">    · Move default ports out of ephemeral range HDFS-9427</span><br><span class="line">· HDFS</span><br><span class="line">    · Removal of hftp in favor of webhdfs HDFS-5570</span><br><span class="line">    · Support for more than two standby NameNodes HDFS-6440</span><br><span class="line">    · Support for Erasure Codes in HDFS HDFS-7285</span><br><span class="line">· YARN</span><br><span class="line">· MAPREDUCE</span><br><span class="line">    · Derive heap size or mapreduce.*.memory.mb automatically MAPREDUCE-5785</span><br></pre></td></tr></table></figure></p><h2 id="三、使用"><a href="#三、使用" class="headerlink" title="三、使用"></a>三、使用</h2><p>&emsp;暂时还没在项目上使用，就简单写了个单词划分测试，具体效率的提升还是参照<a href="https://blog.csdn.net/u011610826/article/details/79026022" target="_blank" rel="noopener">Hadoop3.0版本安装、性能研究</a></p><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><p>&emsp;目前已经自己模拟搭建并跑起来了一个四台机器组成的集群，下一步准备捣鼓捣鼓源码那块吧，使用hadoop跑点实际应用的东西，还在筹划中。</p><h2 id="五、相关链接"><a href="#五、相关链接" class="headerlink" title="五、相关链接"></a>五、相关链接</h2><p>感谢一下几位大佬的博客：</p><ul><li><a href="https://blog.csdn.net/yongge1981/article/details/80504935" target="_blank" rel="noopener">Hadoop 3相对于hadoop2的 新特性</a></li><li><a href="https://www.cnblogs.com/smartloli/p/8827623.html" target="_blank" rel="noopener">Hadoop 3.x 新特性剖析系列</a></li><li><a href="https://blog.csdn.net/u011610826/article/details/79026022" target="_blank" rel="noopener">Hadoop3.0版本安装、性能研究</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;总算是有点时间了，捣鼓一下hadoop3.0的一些东西，听说3.0比spark快十倍？&lt;br&gt;
    
    </summary>
    
    
      <category term="hadoop" scheme="https://wangtomorrow.github.io/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>通过配置application.yml实现开发测试生产环境的搭建</title>
    <link href="https://wangtomorrow.github.io/post/5c4ace42.html"/>
    <id>https://wangtomorrow.github.io/post/5c4ace42.html</id>
    <published>2018-09-29T08:47:18.000Z</published>
    <updated>2018-10-11T06:27:22.233Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;前几天由于开发与测试用的是同一个系统，开发的时候测试不能使用，通过application.yml来控制不同环境。<br><a id="more"></a></p><h2 id="一、系统设计"><a href="#一、系统设计" class="headerlink" title="一、系统设计"></a>一、系统设计</h2><p><img src="https://github.com/Wangtomorrow/Resource/blob/master/application.png?raw=true" alt="系统设计图"><br>系统设计如图，公共服务可以放到一起，容易产生冲突的服务单独搭建为私有服务。图片未加载出来，<a href="https://github.com/Wangtomorrow/Resource/blob/master/application.png?raw=true" target="_blank" rel="noopener">点击</a></p><h2 id="二、application-yml"><a href="#二、application-yml" class="headerlink" title="二、application.yml"></a>二、application.yml</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spring:</span><br><span class="line">  profiles:</span><br><span class="line">    active: dev //在此控制不同环境的入口（开发：dev， 测试：test， 生产：prod）</span><br></pre></td></tr></table></figure><h2 id="三、application-dev-yml"><a href="#三、application-dev-yml" class="headerlink" title="三、application-dev.yml"></a>三、application-dev.yml</h2><p>大部分配置都是相同的就行。<br>涉及到数据库或者别的公用的东西，产生冲突的分别单独搭建。<br>通过配置profile的active属性即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spring:</span><br><span class="line">  profiles:</span><br><span class="line">    active: dev</span><br></pre></td></tr></table></figure></p><p>&emsp;这样就可以一套代码部署到不同环境上，开发测试生产互不干扰。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;前几天由于开发与测试用的是同一个系统，开发的时候测试不能使用，通过application.yml来控制不同环境。&lt;br&gt;
    
    </summary>
    
    
      <category term="application.yml" scheme="https://wangtomorrow.github.io/tags/application-yml/"/>
    
      <category term="环境搭建" scheme="https://wangtomorrow.github.io/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>Kafka及Spring&amp;Kafka整合</title>
    <link href="https://wangtomorrow.github.io/post/85675a3e.html"/>
    <id>https://wangtomorrow.github.io/post/85675a3e.html</id>
    <published>2018-09-13T08:05:01.000Z</published>
    <updated>2019-02-28T06:53:06.083Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;由于某项目的消息队列使用了Spring整合Kafka，开发中我需要使用kafka客户端模拟生产者和消费者。简单了解了一下Kafka，扫盲贴，先标记一下，日后再深入学习。<br><a id="more"></a></p><h2 id="一、Kafka简介"><a href="#一、Kafka简介" class="headerlink" title="一、Kafka简介"></a>一、Kafka简介</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p>&emsp; kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。<br>&emsp;在大数据系统中，常常会碰到一个问题，整个大数据是由各个子系统组成，数据需要在各个子系统中高性能，低延迟的不停流转。传统的企业消息系统并不是非常适合大规模的数据处理。为了已在同时搞定在线应用（消息）和离线应用（数据文件，日志）Kafka就出现了。<br>&emsp;简单点概括一下：Kafka是一个分布式的，可划分的，高性能，低延迟的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。</p><h3 id="1-2-特点"><a href="#1-2-特点" class="headerlink" title="1.2 特点"></a>1.2 特点</h3><pre><code>* 高吞吐量* 可进行持久化操作* 分布式</code></pre><h3 id="1-3-组件"><a href="#1-3-组件" class="headerlink" title="1.3 组件"></a>1.3 组件</h3><p>&emsp;Topic，Broker，Partition，Message，Producer，Consumer,Zookpeer</p><h4 id="1-3-1-名词解释"><a href="#1-3-1-名词解释" class="headerlink" title="1.3.1 名词解释"></a>1.3.1 名词解释</h4><pre><code>服务：Topic：主题，Kafka处理的消息的不同分类。Broker：消息代理，Kafka集群中的一个kafka服务节点称为一个broker，主要存储消息数据。存在硬盘中。每个topic都是有分区的。Partition：Topic物理上的分组，一个topic在broker中被分为1个或者多个partition，分区在创建topic的时候指定。Message：消息，是通信的基本单位，每个消息都属于一个partition服务相关：Producer：消息和数据的生产者，向Kafka的一个topic发布消息。Consumer：消息和数据的消费者，定于topic并处理其发布的消息。Zookeeper：协调kafka的正常运行。</code></pre><h3 id="1-4-应用场景"><a href="#1-4-应用场景" class="headerlink" title="1.4 应用场景"></a>1.4 应用场景</h3><p>构建实时的流数据管道，可靠地获取系统和应用程序之间的数据。<br>构建实时流的应用程序，对数据流进行转换或反应。  </p><h2 id="二、Kafka搭建"><a href="#二、Kafka搭建" class="headerlink" title="二、Kafka搭建"></a>二、Kafka搭建</h2><h3 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1 安装"></a>2.1 安装</h3><p>&emsp;教程很多，就不写了。 </p><h3 id="2-2-配置"><a href="#2-2-配置" class="headerlink" title="2.2 配置"></a>2.2 配置</h3><p>&emsp;配置文件放在kafka下config下</p><pre><code>* consumer.properites 消费者配置* producer.properties 生产者配置* server.properties kafka服务器的配置    broker.id 申明当前kafka服务器在集群中的唯一ID，需配置为integer,并且集群中的每一个kafka服务器的id都应是唯一的    listeners 申明此kafka服务器需要监听的端口号，如果是在本机上跑虚拟机运行可以不用配置本项，默认会使用localhost的地址，如果是在远程服务器上运行则必须配置，例如：              listeners=PLAINTEXT:// 192.168.180.128:9092。并确保服务器的9092端口能够访问    zookeeper.connect 申明kafka所连接的zookeeper的地址 ，需配置为zookeeper的地址</code></pre><p>&emsp;上面配置文件中listeners的配置尤其注意，刚开始整的时候，没注意自己编写producer和cusmer时报错，如下：</p><pre><code>Connection to node -1 could not be established. Broker may not be available.</code></pre><p>&emsp;就是因为配置文件中的PLAINTEXT跟我请求的内容不同。</p><p>&emsp;具体配置教程很多，也不写了。</p><h2 id="三、Kafka操作"><a href="#三、Kafka操作" class="headerlink" title="三、Kafka操作"></a>三、Kafka操作</h2><h3 id="3-1-Topic操作"><a href="#3-1-Topic操作" class="headerlink" title="3.1 Topic操作"></a>3.1 Topic操作</h3><h4 id="3-1-1-创建Topic"><a href="#3-1-1-创建Topic" class="headerlink" title="3.1.1 创建Topic"></a>3.1.1 创建Topic</h4><pre><code>kafka-topics.sh --create --topic hbase --zookeeper ip1:port --partitions 3 --replication-factor 1创建topic过程的问题，replication-factor个数不能超过broker的个数创建topic后，可以在../data/kafka目录查看到分区的目录</code></pre><h4 id="3-1-2-查看Topic列表"><a href="#3-1-2-查看Topic列表" class="headerlink" title="3.1.2 查看Topic列表"></a>3.1.2 查看Topic列表</h4><pre><code>kafka-topics.sh --list --zookeeper ip:port</code></pre><h4 id="3-1-3-查看某一个具体的Topic"><a href="#3-1-3-查看某一个具体的Topic" class="headerlink" title="3.1.3 查看某一个具体的Topic"></a>3.1.3 查看某一个具体的Topic</h4><pre><code>kafka-topics.sh --describe xxx --zookeeper ip:port</code></pre><h4 id="3-1-4-修改Topic"><a href="#3-1-4-修改Topic" class="headerlink" title="3.1.4 修改Topic"></a>3.1.4 修改Topic</h4><pre><code>kafka-topics.sh --alter --topic topic-test --zookeeper ip:port --partitions 3不能修改replication-factor，以及只能对partition个数进行增加，不能减少</code></pre><h4 id="3-1-5-删除Topic"><a href="#3-1-5-删除Topic" class="headerlink" title="3.1.5 删除Topic"></a>3.1.5 删除Topic</h4><pre><code>kafka-topics.sh --delete --topic topic-test --zookeeper ip:port彻底删除一个topic，需要在server.properties中配置delete.topic.enable=true，否则只是标记删除配置完成之后，需要重启kafka服务。</code></pre><h3 id="3-2-生产者操作"><a href="#3-2-生产者操作" class="headerlink" title="3.2 生产者操作"></a>3.2 生产者操作</h3><pre><code>sh kafka-console-producer.sh --broker-list ip1:port,ip2:port,ip3:port --sync --topic kafka-topic-test生产数据的时候需要指定：当前数据流向哪个broker，以及哪一个topic</code></pre><h3 id="3-3-消费者操作"><a href="#3-3-消费者操作" class="headerlink" title="3.3 消费者操作"></a>3.3 消费者操作</h3><pre><code>sh kafka-console-consumer.sh --zookeeper ip1:port,ip2:port,ip3:port --topic kafka-topic-test --from-beginning--from-begining 获取最新以及历史数据黑白名单（暂时未用到）--blacklist 后面跟需要过滤的topic的列表，使用&quot;,&quot;隔开，意思是除了列表中的topic之外，都能接收其它topic的数据--whitelist 后面跟需要过滤的topic的列表，使用&quot;,&quot;隔开，意思是除了列表中的topic之外，都不能接收其它topic的数据</code></pre><h2 id="四、Springboot整合Kafka"><a href="#四、Springboot整合Kafka" class="headerlink" title="四、Springboot整合Kafka"></a>四、Springboot整合Kafka</h2><p>这个只是个人使用的简单的测试环境搭建，可能有很多地方有问题，以后深入学习时再检查。</p><h3 id="4-1-整合"><a href="#4-1-整合" class="headerlink" title="4.1 整合"></a>4.1 整合</h3><p>&emsp;springboot集成kafka的默认配置都在org.springframework.boot.autoconfigure.kafka包里面。直接使用即可。flag=深入学习kafka。</p><h3 id="4-2-pom-xml配置"><a href="#4-2-pom-xml配置" class="headerlink" title="4.2 pom.xml配置"></a>4.2 pom.xml配置</h3><pre><code>&lt;dependency&gt;   &lt;!--引入spring和kafka整合的jar--&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-stream-kafka&lt;/artifactId&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;                    &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;                    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;                    &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;            &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;            &lt;version&gt;1.0.1&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-task-core&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;            &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;            &lt;version&gt;1.3.5.RELEASE&lt;/version&gt;&lt;!--$NO-MVN-MAN-VER$--&gt;            &lt;/dependency&gt;        &lt;dependency&gt;</code></pre><h3 id="4-3-Producer配置"><a href="#4-3-Producer配置" class="headerlink" title="4.3 Producer配置"></a>4.3 Producer配置</h3><pre><code>@Configuration@EnableKafkapublic class KafkaProducer {    public Map&lt;String, Object&gt; producerConfigs() {        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);        props.put(ProducerConfig.RETRIES_CONFIG, KafkaConfig.PRODUCER_RETRIES);        props.put(ProducerConfig.BATCH_SIZE_CONFIG, KafkaConfig.PRODUCER_BATCH_SIZE);        props.put(ProducerConfig.LINGER_MS_CONFIG, KafkaConfig.PRODUCER_LINGER_MS);        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, KafkaConfig.PRODUCER_BUFFER_MEMORY);        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);        props.put(&quot;advertised.host.name&quot;,KafkaConfig.BOOTSTRAP_SERVERS);        props.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;1&quot;);        System.out.println(&quot;KafkaConfig.BOOTSTRAP_SERVERS:&quot;+KafkaConfig.BOOTSTRAP_SERVERS);        return props;    }    /** 获取工厂 */    public ProducerFactory&lt;String, String&gt; producerFactory() {        return new DefaultKafkaProducerFactory&lt;&gt;(producerConfigs());    }    /** 注册实例 */    @Bean    public KafkaTemplate&lt;String, String&gt; kafkaTemplate() {        return new KafkaTemplate&lt;&gt;(producerFactory());    }}</code></pre><h3 id="4-4-使用生产者"><a href="#4-4-使用生产者" class="headerlink" title="4.4 使用生产者"></a>4.4 使用生产者</h3><pre><code>@Autowiredprivate KafkaTemplate&lt;String, String&gt; kafkaTemplate;kafkaTemplate.send(&quot;kafka-topic-test&quot;, &quot;helloWorld&quot;);</code></pre><h3 id="4-5-Consumer配置"><a href="#4-5-Consumer配置" class="headerlink" title="4.5 Consumer配置"></a>4.5 Consumer配置</h3><pre><code>@Configuration@EnableKafkapublic class KafkaConsumer {    private final static Logger log = LoggerFactory.getLogger(KafkaConsumer .class);    @KafkaListener(topics = {&quot;kafka-topic-test&quot;})    public void consume(ConsumerRecord&lt;?, ?&gt; record) {        String topic = record.topic();        String value = record.value().toString();        System.out.println(&quot;partitions:&quot;+record.partition()+&quot;,&quot;+&quot;offset:&quot;+record.offset()+&quot;,value=&quot;+value);        MqConsumerRunnable runnable = new MqConsumerRunnable(topic,value);        executor.execute(runnable);    }    public Map&lt;String, Object&gt; consumerConfigs() {        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS);        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, &quot;100&quot;);        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);        props.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;1&quot;);        props.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);// 一般配置earliest 或者latest 值        return props;    }    /** 获取工厂 */    public ConsumerFactory&lt;String, String&gt; consumerFactory() {        return new DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs());    }    /** 获取实例 */    @Bean    public KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;String, String&gt;&gt; kafkaListenerContainerFactory() {        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();        factory.setConsumerFactory(consumerFactory());        factory.setConcurrency(3);        factory.getContainerProperties().setPollTimeout(3000);        return factory;    }}</code></pre><h3 id="4-6-使用消费者"><a href="#4-6-使用消费者" class="headerlink" title="4.6 使用消费者"></a>4.6 使用消费者</h3><pre><code>public class KafkaMessageListener implements MessageListener&lt;String, String&gt; {    private static Logger LOG = LoggerFactory.getLogger(KafkaMessageListener.class);    @Autowired    private AppProperties appProperties;    @Override    public void onMessage(ConsumerRecord&lt;String, String&gt; data) {        LOG.info(&quot;消费消息topic：{} value {}&quot;, data.topic(), data.value());        String topic = data.topic();        String content = data.value();        //可同时监听多个topic，根据不同topic处理不同的业务        if (topic.equals(&quot;topica&quot;)) {                       LOG.info(&quot;###############topic:{} value:{}&quot; ,topic,content);        } else if (topic.equals(&quot;topicb&quot;)) {         LOG.info(&quot;###############topic:{} value:{}&quot; ,topic,content);        }     }}</code></pre><h3 id="4-7-注意"><a href="#4-7-注意" class="headerlink" title="4.7 注意"></a>4.7 注意</h3><pre><code>kafkaTemplate.send(&quot;kafka-topic-test&quot;, &quot;helloWorld&quot;);@KafkaListener(topics = {&quot;kafka-topic-test&quot;})topic需要对应</code></pre><h3 id="4-8-使用"><a href="#4-8-使用" class="headerlink" title="4.8 使用"></a>4.8 使用</h3><p>&emsp;本地运行以后，到kafka服务器上可以进行消费者和生产者的模拟发送与接收信息。</p><h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p>&emsp;上述方法进行模拟测试，可以测试，但是总感觉问题很大，却又找不出问题，这个后期再说吧，先凑合用。<br>&emsp;有关Kafka的具体学习，后期补上。</p><h2 id="六、相关链接"><a href="#六、相关链接" class="headerlink" title="六、相关链接"></a>六、相关链接</h2><p>感谢各位大佬：</p><ul><li><a href="https://www.cnblogs.com/yangxiaoyi/p/7359236.html" target="_blank" rel="noopener">kafka 基础知识梳理</a></li><li><a href="https://www.cnblogs.com/hei12138/p/7805475.html" target="_blank" rel="noopener">kafka实战</a></li><li><a href="http://blog.51cto.com/xpleaf/2090847" target="_blank" rel="noopener">kafka笔记整理</a></li><li><a href="https://www.cnblogs.com/yepei/p/6197236.html" target="_blank" rel="noopener">kafka介绍</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;由于某项目的消息队列使用了Spring整合Kafka，开发中我需要使用kafka客户端模拟生产者和消费者。简单了解了一下Kafka，扫盲贴，先标记一下，日后再深入学习。&lt;br&gt;
    
    </summary>
    
    
      <category term="Kafka" scheme="https://wangtomorrow.github.io/tags/Kafka/"/>
    
      <category term="Springboot Kafka" scheme="https://wangtomorrow.github.io/tags/Springboot-Kafka/"/>
    
      <category term="笔记" scheme="https://wangtomorrow.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>spring学习记录(一)</title>
    <link href="https://wangtomorrow.github.io/post/61f45dcf.html"/>
    <id>https://wangtomorrow.github.io/post/61f45dcf.html</id>
    <published>2018-09-11T06:45:18.000Z</published>
    <updated>2018-09-14T09:12:56.086Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;最近在做后端的开发，用到了spring，借这个机会给spring捋一遍吧。东西有点多，需要点时间，抽空学。记录(一)主要是写一点spring最基础的IOC和AOP,至于spring的各种注解和SpingMVC另开贴。<br><a id="more"></a></p><h2 id="一、Spring概述"><a href="#一、Spring概述" class="headerlink" title="一、Spring概述"></a>一、Spring概述</h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h3><p>&emsp;Spring是一个基于 <strong>控制反转（IOC）</strong> 和 <strong>面向切面（AOP）</strong> 的结构J2EE系统的框架。从简单性、可测试性和松耦合的角度而言，任何Java应用都可以从Spring中受益。<br>&emsp;控制反转(IOC)是一个通用概念，Spring最认同的技术是控制反转的 <strong>依赖注入（DI）</strong> 模式。简单地说就是拿到的对象的属性，已经被注入好相关值了，直接使用即可。<br>&emsp;面向切面编程（AOP），一个程序中跨越多个点的功能被称为横切关注点，这些横切关注点在概念上独立于应用程序的业务逻辑。Spring 框架的 AOP 模块提供了面向方面的程序设计实现，可以定义诸如方法拦截器和切入点等，从而使实现功能的代码彻底的解耦出来。  </p><h3 id="2-Spring关键策略"><a href="#2-Spring关键策略" class="headerlink" title="2.Spring关键策略"></a>2.Spring关键策略</h3><pre><code>* 基于POJO的轻量级和最小侵入性编程* 通过依赖注入和面向接口实现松耦合* 基于切面和惯例进行声明式编程* 通过切面和模板减少样板式代码</code></pre><h3 id="3-Spring优点"><a href="#3-Spring优点" class="headerlink" title="3.Spring优点"></a>3.Spring优点</h3><pre><code>*方便解耦，简化开发 （高内聚低耦合）     Spring就是一个大工厂（容器），可以将所有对象创建和依赖关系维护，交给Spring管理     spring工厂是用于生成bean*AOP编程的支持     Spring提供面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等功能    声明式事务的支持     只需要通过配置就可以完成对事务的管理，而无需手动编程*方便程序的测试     Spring对Junit4支持，可以通过注解方便的测试Spring程序*方便集成各种优秀框架     Spring不排斥各种优秀的开源框架，其内部提供了对各种优秀框架（如：Struts、Hibernate、MyBatis、Quartz等）的直接支持*降低JavaEE API的使用难度     Spring 对JavaEE开发中非常难用的一些API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低</code></pre><h2 id="二、控制反转（IOC）"><a href="#二、控制反转（IOC）" class="headerlink" title="二、控制反转（IOC）"></a>二、控制反转（IOC）</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h3><p>&emsp;代码之间的 <strong>耦合性</strong> 具有两面性。耦合是必须的，但应小心谨慎的管理。<br>&emsp;DI会将所以来的关系自动交给目标对象，而不是让对象自己去获取依赖。– <strong>松耦合</strong></p><h3 id="2-获取对象的方式进行比较"><a href="#2-获取对象的方式进行比较" class="headerlink" title="2.获取对象的方式进行比较"></a>2.获取对象的方式进行比较</h3><p>传统方式：<br>通过new关键字主动创建一个对象<br>IOC方式：<br>对象的生命周期由Spring来管理，直接从Spring那里去获取一个对象。 IOC是反转控制 (Inversion Of Control)的缩写，就像控制权从本来在自己手里，交给了Spring。</p><h3 id="3-装配"><a href="#3-装配" class="headerlink" title="3.装配"></a>3.装配</h3><p>&emsp;创建应用组件之间协作的行为通常称为装配。常用XML装配方式。Spring配置文件。</p><pre><code class="xml">*使用构造器进行配置    <span class="tag">&lt;<span class="name">bean</span>&gt;</span> 定义JavaBean，配置需要创建的对象    id/name ：用于之后从spring容器获得实例时使用的    class ：需要创建实例的全限定类名    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"c"</span> <span class="attr">class</span>=<span class="string">"pojo.Category"</span>&gt;</span>        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"name"</span> <span class="attr">value</span>=<span class="string">"category 1"</span> /&gt;</span> <span class="comment">&lt;!--category对象--&gt;</span>    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span>    <span class="comment">&lt;!--创建product对象的时候注入了一个category对象，使用ref--&gt;</span>    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">name</span>=<span class="string">"p"</span> <span class="attr">class</span>=<span class="string">"pojo.Product"</span>&gt;</span> <span class="comment">&lt;!--product类中添加category对象的set&amp;get方法--&gt;</span>        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"name"</span> <span class="attr">value</span>=<span class="string">"product1"</span> /&gt;</span>   <span class="comment">&lt;!--product对象--&gt;</span>        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"category"</span> <span class="attr">ref</span>=<span class="string">"c"</span> /&gt;</span>    <span class="comment">&lt;!--为product对象注入category对象--&gt;</span>    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span>*使用注解的方式进行配置    xml中添加  <span class="tag">&lt;<span class="name">context:annotation-config</span>/&gt;</span> 注释掉注入category        *@Autowired注解方法            在Product.java的category属性前加上@Autowired注解            @Autowired            private Category category;            或者            @Autowired            public void setCategory(Category category)         *@Resource            @Resource(name="c")            private Category category;*对bean进行注解配置    直接xml中添加  <span class="tag">&lt;<span class="name">context:component-scan</span> <span class="attr">base-package</span>=<span class="string">"pojo"</span>/&gt;</span>    就是告知spring，所有的bean都在pojo包下    通过@Component注解        在类前加入  @Component("c")  表明此类是bean        因为配置从applicationContext.xml中移出来了，所以属性初始化放在属性声明上进行了。        private String name="product 1";        private String name="category 1";        同时@Autowired也需要在Product.java的category属性前加上*Spring javaConfig 后期补充</code></pre><p><a href="https://github.com/Wangtomorrow/Spring/tree/master/spring" target="_blank" rel="noopener">源码参照</a></p><h2 id="三、面向切面（AOP）"><a href="#三、面向切面（AOP）" class="headerlink" title="三、面向切面（AOP）"></a>三、面向切面（AOP）</h2><h3 id="1-概述-1"><a href="#1-概述-1" class="headerlink" title="1.概述"></a>1.概述</h3><p>&emsp;AOP 即 Aspect Oriented Program 面向切面编程。在面向切面编程的思想里面，把功能分为 <strong>核心业务功能</strong> 和 <strong>周边功能</strong>。<br>&emsp;核心业务，比如登陆，增加数据，删除数据都叫核心业务。<br>&emsp;周边功能，比如性能统计，日志，事务管理等等。<br>&emsp;周边功能在Spring的面向切面编程AOP思想里，即被定义为 <strong>切面</strong><br>&emsp;在面向切面编程AOP的思想里面，核心业务功能和切面功能分别独立进行开发，然后把切面功能和核心业务功能 “编织” 在一起，这种能够选择性的，低耦合的把切面和核心业务功能结合在一起的编程思想，就叫AOP。  </p><h3 id="2-AOP核心概念"><a href="#2-AOP核心概念" class="headerlink" title="2.AOP核心概念"></a>2.AOP核心概念</h3><pre><code>1、横切关注点对哪些方法进行拦截，拦截后怎么处理，这些关注点称之为横切关注点2、切面（aspect）类是对物体特征的抽象，切面就是对横切关注点的抽象3、连接点（joinpoint）被拦截到的点，因为Spring只支持方法类型的连接点，所以在Spring中连接点指的就是被拦截到的方法，实际上连接点还可以是字段或者构造器4、切入点（pointcut）对连接点进行拦截的定义5、通知（advice）所谓通知指的就是指拦截到连接点之后要执行的代码，通知分为前置、后置、异常、最终、环绕通知五类6、目标对象代理的目标对象7、织入（weave）将切面应用到目标对象并导致代理对象创建的过程8、引入（introduction）在不修改代码的前提下，引入可以在运行期为类动态地添加一些方法或字段</code></pre><h3 id="3-Spring对AOP的支持"><a href="#3-Spring对AOP的支持" class="headerlink" title="3.Spring对AOP的支持"></a>3.Spring对AOP的支持</h3><p>&emsp;Spring中AOP代理由Spring的IOC容器负责生成、管理，其依赖关系也由IOC容器负责管理。因此，AOP代理可以直接使用容器中的其它bean实例作为目标，这种关系可由IOC容器的依赖注入提供。Spring创建代理的规则为：  </p><pre><code>1、默认使用Java动态代理来创建AOP代理，这样就可以为任何接口实例创建代理了2、当需要代理的类不是代理接口的时候，Spring会切换为使用CGLIB代理，也可强制使用CGLIB</code></pre><h3 id="4-AOP编程"><a href="#4-AOP编程" class="headerlink" title="4.AOP编程"></a>4.AOP编程</h3><pre><code>1、定义普通业务组件2、定义切入点，一个切入点可能横切多个业务组件 3、定义增强处理，增强处理就是在AOP框架为普通业务组件织入的处理动作</code></pre><h3 id="5-简单例子熟悉AOP"><a href="#5-简单例子熟悉AOP" class="headerlink" title="5.简单例子熟悉AOP"></a>5.简单例子熟悉AOP</h3><pre><code>AOP配置文件&lt;!--声明业务对象--&gt;    &lt;bean name=&quot;s&quot; class=&quot;service.ProductService&quot;&gt;    &lt;/bean&gt;    &lt;!--声明日志切面--&gt;    &lt;bean id=&quot;loggerAspect&quot; class=&quot;aspect.LoggerAspect&quot;/&gt;    &lt;aop:config&gt;        &lt;!--指定核心业务功能--&gt;        &lt;aop:pointcut id=&quot;loggerCutpoint&quot;                      expression=                              &quot;execution(* service.ProductService.*(..)) &quot;/&gt;    &lt;!--* 返回任意类型,   包名以 service.ProductService 开头的类的任意方法,   (..) 参数是任意数量和类型--&gt;        &lt;!--指定辅助业务功能--&gt;        &lt;aop:aspect id=&quot;logAspect&quot; ref=&quot;loggerAspect&quot;&gt;            &lt;aop:around pointcut-ref=&quot;loggerCutpoint&quot; method=&quot;log&quot;/&gt;        &lt;/aop:aspect&gt;    &lt;/aop:config&gt;</code></pre><h3 id="6-注解方式AOP"><a href="#6-注解方式AOP" class="headerlink" title="6.注解方式AOP"></a>6.注解方式AOP</h3><pre><code>1.注解注解配置业务类,使用@Component(&quot;s&quot;)2.注解配置切面    @Aspect 注解表示这是一个切面    @Component 表示这是一个bean,由Spring进行管理    @Around(value = &quot;execution(* service.ProductService.*(..))&quot;) 表示对service.ProductService 这个类中的所有方法进行切面操作3.配置applicationContext.xml    &lt;!--扫描这俩包，定位业务类和切面类--&gt;    &lt;context:component-scan base-package=&quot;aspect&quot;/&gt;    &lt;context:component-scan base-package=&quot;service&quot;/&gt;    &lt;!--找到被注解了的切面类，进行切面配置--&gt;    &lt;aop:aspectj-autoproxy/&gt;</code></pre><p><a href="https://github.com/Wangtomorrow/Spring" target="_blank" rel="noopener">源码参照</a></p><h2 id="四、总结说明"><a href="#四、总结说明" class="headerlink" title="四、总结说明"></a>四、总结说明</h2><p>&emsp;这个帖子主要就是简单的介绍了一下spring的IOC和AOP思想以及简单的例子。暂时写这么多，spring东西比较多，下一阶段应该是再看一下Spring各种注解的使用还有SpringMVC。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;最近在做后端的开发，用到了spring，借这个机会给spring捋一遍吧。东西有点多，需要点时间，抽空学。记录(一)主要是写一点spring最基础的IOC和AOP,至于spring的各种注解和SpingMVC另开贴。&lt;br&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://wangtomorrow.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="spring" scheme="https://wangtomorrow.github.io/tags/spring/"/>
    
      <category term="IOC" scheme="https://wangtomorrow.github.io/tags/IOC/"/>
    
      <category term="DI" scheme="https://wangtomorrow.github.io/tags/DI/"/>
    
      <category term="AOP" scheme="https://wangtomorrow.github.io/tags/AOP/"/>
    
  </entry>
  
  <entry>
    <title>开始吧</title>
    <link href="https://wangtomorrow.github.io/post/9f79558f.html"/>
    <id>https://wangtomorrow.github.io/post/9f79558f.html</id>
    <published>2018-09-10T10:36:06.000Z</published>
    <updated>2018-09-11T06:08:54.936Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;入职一个月了，暑假的时候来公司做实习生，目前是在做hadoop的运维和别的项目的后端开发以及一些日常琐事的处理。<br><a id="more"></a><br>&emsp;刚来的时候先是学习了hadoop的搭建，当时是搭建的Hadoop2.4版本（有时间学习一下3.0版本的新特性，自己试着搭建一下），再后来搭建了HA，加入了zookeeper。在此基础上，又学习了HDFS的机制与shell操作，yarn资源调度，还有mapreduce的工作机制，包括shuffle，partitioner，combiner等，并写了最基础的wordcount，现在做的也是mapreduce的日常运维与开发。后来又学习了hive表的机制与操作（写了好长一段时间sql语句，发现自己基础真的薄弱）。相关笔记：<a href="https://github.com/Wangtomorrow/Hadoop" target="_blank" rel="noopener">hadoop学习记录</a><br>&emsp;现在也算简单的hadoop入门了，还有公司的环境也是一个比较好的机会吧。这一个月的时间里在学习，熟悉公司业务、框架的过程中，发现了自己存在很大的问题。感觉自己涉猎过很多东西，其实真正用起来的时候发现自己其实真的是啥也会，啥也不会。说白了，还是看的多了，做的少了。<br>&emsp;最近一段时间，在做某项目的后端开发，hadoop的学习先暂告一段落了。就标记一下，hadoop接下来一段时间要学的东西吧。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">*hadoop3.0新特新及搭建</span><br><span class="line">*linux的操作，各种操作指令</span><br><span class="line">*hive表相关，涉及到sql</span><br><span class="line">*hadoop及HDFS深度学习研究</span><br><span class="line">*pig</span><br><span class="line">*hbase</span><br><span class="line">*redis</span><br><span class="line">*MongoDB</span><br><span class="line">*yarn相关细节</span><br><span class="line">*zookeeper相关细节</span><br><span class="line">*sqoop</span><br><span class="line">*Flume对比DataX</span><br><span class="line">*Spark全家桶</span><br><span class="line">*Storm对比Spark Streaming</span><br><span class="line">*kafka</span><br><span class="line">*JVM虚拟机</span><br><span class="line"></span><br><span class="line">*调度监控管理系统、数据应用等很多东西</span><br><span class="line"></span><br><span class="line">*大数据中涉及到的机器学习，Mahout、Spark MLLib等。</span><br><span class="line">（中文分词、自然语言处理、推荐算法、分类算法、回归算法、聚类算法、神经网络与深度学习）</span><br><span class="line"> 关于机器学习，可以先放一放。数学基础很重要。</span><br><span class="line"></span><br><span class="line">*会涉及到并发、多线程、负载均衡、分布式、云计算等问题。</span><br></pre></td></tr></table></figure></p><p>&emsp;关于大数据目前就想到这么多，很多东西还是得边学边加，实际项目中缺啥补啥吧。<br>&emsp;关于最近所做项目的后端，想了想，最近应该会去多学习kafka以及spring相关，可以借此机会给spring全家桶捣鼓一遍。<br>&emsp;以后大部分代码更新会在<a href="https://github.com/Wangtomorrow" target="_blank" rel="noopener">github</a>上，日常博客更新会在这里，csdn看内容吧，没营养的就不往那边整了。这边估计很少会有人看见吧，哈哈哈~~~<br>&emsp;写这个blog最初的想法，一是就当一个记事本了，记录自己学习上遇到的坑和笔记。二是为了督促和记录自己学习的。开始吧</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;入职一个月了，暑假的时候来公司做实习生，目前是在做hadoop的运维和别的项目的后端开发以及一些日常琐事的处理。&lt;br&gt;
    
    </summary>
    
    
      <category term="start" scheme="https://wangtomorrow.github.io/tags/start/"/>
    
  </entry>
  
</feed>
